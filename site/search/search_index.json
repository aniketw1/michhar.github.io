{
    "docs": [
        {
            "location": "/",
            "text": "A blog on data science and software dev\n\n\nThe posts are in reverse order by date (the latest is at the top).  My posts tend to be more like tutorials around exciting projects I've come across in my career.\n\n\nMy name is Micheleen Harris (\n@rheartpython\n) and I'm interested in data science, have taught it some and am still learning much.  I try not to have post titles soley for \"click-through\" value, but sometimes I do get excited.\n\n\nI'm currently fascinated by:\n\n\n\n\nDeep learning for computer vision and NLP (mostly with CNTK, TensorFlow/Keras next)\n\n\nThe Jupyter project\n\n\nOpenCV-Python\n\n\nBuilding simple algorithms for deep learning\n\n\nChatbots some of the time\n\n\nSpeech to text\n\n\nCreating websites\n\n\nML on the Edge (IoT)",
            "title": "Home"
        },
        {
            "location": "/#a-blog-on-data-science-and-software-dev",
            "text": "The posts are in reverse order by date (the latest is at the top).  My posts tend to be more like tutorials around exciting projects I've come across in my career.  My name is Micheleen Harris ( @rheartpython ) and I'm interested in data science, have taught it some and am still learning much.  I try not to have post titles soley for \"click-through\" value, but sometimes I do get excited.  I'm currently fascinated by:   Deep learning for computer vision and NLP (mostly with CNTK, TensorFlow/Keras next)  The Jupyter project  OpenCV-Python  Building simple algorithms for deep learning  Chatbots some of the time  Speech to text  Creating websites  ML on the Edge (IoT)",
            "title": "A blog on data science and software dev"
        },
        {
            "location": "/cntk-has-feelings-too/",
            "text": "Posted:\n  2017-12-12\n\n\nSome Background\n\n\nCNTK\n\n\nThe original name for Microsoft's CNTK was the \nComputational Network Toolkit\n, now known today simply as the \nCognitive Toolkit\n, still abbreviated CNTK for short.  It was orignally written and offered up as a C++ package and now has Python bindings, making it much more widely adoptable.\n\n\n\n\nIn its original words: [CNTK is] a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph\n\n\n\n\nIt was first open-sourced in April of 2015 with intended use for researchers and protoypers using GPUs for accelerated matrix calculations, much of what deep learning is built upon these days.  Interestingly, TensorFlow has its initial public release in November of 2015.  Of note, 2015 was also a good year for Microsoft Research in the computer vision space as they won the \nImageNet challenge\n that December using this toolkit and a 152-layer deep neural network.\n\n\nSince the beginning CNTK has been available for Linux and Windows.  We will be using a Linux Docker image in a minute.\n\n\nFER Faces Data\n\n\nThis data comes via a Kaggle competition on facial expressions found \nhere\n.\n\n\nThe data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of two categories, Happy or Sad.  That being said this dataset has images (all but two of interest were dropped in this analysis) for a total of 6 emotions:  Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.\n\n\nHow I set things up\n\n\nSince I'm on a Mac, I chose to use the Docker image of CNTK (instructions found \nhere\n).  This pulls down an image of a pre-created system and I run it in my own Docker container basically recreating an Ubuntu setup with CNTK locally.  It's pretty neat!  And then I can run Jupyter notebooks on my system and pull in local files and write out data as needed.  Let me show you how and then after we'll talk about the CNNs.\n\n\nBy following this \nDoc\n I got a Jupyter notebook up and running with CNTK with all of the Tutorial notebooks at the ready and the ability to upload or create new ones as needed.\n\n\nI ran these commands to get a Jupyter notebook set up with CNTK (v2.1 used here).  \n\n\nAn important note:  in the \nrun\n command for Docker I mounted a volume with \n\"$PWD/data:/data\"\n.  This \"/data\" folder can be accessed from the Jupyter notebook, as you can see if you check them out (link below), but also used to add data or pull data from the docker container just like any file folder on your system.  A very handy trick!\n\n\ndocker pull microsoft/cntk:2.1-cpu-python3.5\n\ndocker run -d --volume \"$PWD/data:/data\" -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk:2.1-cpu-python3.5\n\ndocker exec -it cntk-jupyter-notebooks bash -c \"source /cntk/activate-cntk && jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root\"\n\n\n\nWhat I Did In a Nutshell\n\n\nSo, I tried many different CNN network architectures (simple three-layer CNN, ones with pooling and dropout layers, etc.) and several hyperparameter combinations (minibatch sizes for training, learning rate, etc.).  Here are just a few basic improvement tips for image classification based on what I found and a bit on how to do this with CNTK, plus my results - my hope is that you can do this on your own with some tips to start or maybe add to your workflow.\n\n\nThe Jupyter notebooks associated with this post can be found on GitHub \nhere\n\n\nPositive Results\n\n\nImprovement #1:  Use sufficient data\n\n\nI began this journey with about 1000 images that I sorted and curated into a \"sad\" or \"happy\" bucket - using the \nPICS 2D face sets\n.  I even got up to 72% accuracy with this dataset and a CNN with pooling.  Then I discovered a Kaggle competition from about 5 years ago dealing with recognizing facial expressions - \nChallenges in Representation Learning: Facial Expression Recognition Challenge\n and this provided me with about 13500 training images and 2300 test images of the \"happy\" or \"sad\" kind (there were also images for categories: angry, disgust, fear, surprise and neutral in case you wish to try these out).\n\n\nImprovement #2:  Intensity normalization\n\n\nI read about the sensitivity of CNNs to certain normalizations.  So, I decided to try a recommended pixel intensity normalization (\nRef to nice StackOverflow hint\n).  This update resulted in about a 5% increase in accuracy on the held-out test set.\n\n\nMy code looked like:\n\n\ndef\n \nnormalize\n(\narr\n):\n\n    \n\"\"\"\n\n\n    Linear normalization\n\n\n    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n\n\n    \"\"\"\n\n    \narr\n \n=\n \narr\n.\nastype\n(\n'float'\n)\n\n    \nminval\n \n=\n \narr\n.\nmin\n()\n\n    \nmaxval\n \n=\n \narr\n.\nmax\n()\n\n    \nif\n \nminval\n \n!=\n \nmaxval\n:\n\n        \narr\n \n-=\n \nminval\n\n        \narr\n \n*=\n \n(\n255.0\n/\n(\nmaxval\n-\nminval\n))\n\n    \nreturn\n \narr\n\n\n\n\n\nImprovement #3:  Adjust or even search hyperparameter space\n\n\nPlay with the hyperparameters.  I had a code cell full of my hyperparameters, e.g. learning rate and minibatch sizes.  I varied one or two at a time.  Just as an example, I took the learning rate down 10x (from 0.2 to 0.02) and my resulting accuracy increased approximately 5%.  Using the \nCNTK train package\n one can design a \"trainer\" object encapsulating all training tasks and include parameter ranges inside it.  I also played with the strides and filter sizes or receptive fields in my CNN for the convolutional and pooling layers (see the Stanford CV course \nnotes\n for great explanations of pretty much anything CNN and much more).  My big tip if you are starting out is simply to tune these adjustable parameters yourself and see what happens.\n\n\nImprovement #4:  Add more layers (but be careful of overfitting)\n\n\nInterestingly, layering in three pooling layers in between the convolutional layers (last one before the dense output layer) resulted in about another 5% jump in accuracy on the test set.  (more on CNNs in the Stanford course \nnotes\n)\n\n\nHere's what the relatively simple model looked like in the CNTK Python API:\n\n\ndef\n \ncreate_model\n(\nfeatures\n):\n\n    \nwith\n \nC\n.\nlayers\n.\ndefault_options\n(\ninit\n=\nC\n.\nglorot_uniform\n(),\n \nactivation\n=\nC\n.\nrelu\n):\n\n        \nmodel\n \n=\n \nC\n.\nlayers\n.\nSequential\n([\n\n            \nC\n.\nlayers\n.\nFor\n(\nrange\n(\n3\n),\n \nlambda\n \ni\n:\n \n[\n\n                \nC\n.\nlayers\n.\nConvolution2D\n(\nfilter_shape\n=\n(\n5\n,\n5\n),\n \n                                     \nnum_filters\n=\n[\n8\n,\n \n16\n,\n \n16\n][\ni\n],\n \n                                     \npad\n=\nTrue\n,\n \n                                     \nstrides\n=\n[(\n1\n,\n1\n),\n \n(\n1\n,\n1\n),\n \n(\n2\n,\n2\n)][\ni\n]),\n\n                \nC\n.\nlayers\n.\nAveragePooling\n(\nfilter_shape\n=\n(\n2\n,\n2\n),\n\n                                    \nstrides\n=\n1\n,\n \n                                    \npad\n=\nTrue\n)\n\n                \n]),\n\n            \nC\n.\nlayers\n.\nDense\n(\nnum_output_classes\n,\n \nactivation\n=\nNone\n)\n\n        \n])\n\n    \nreturn\n \nmodel\n(\nfeatures\n)\n\n\n\n\n\nIf you have seen some networks in CNTK, note I'm using the Sequential notation \"shortcut\" for repeating some layers, but maybe with different params (more in this \nDoc\n).\n\n\nThe above \"simple\" CNN resulted in an average test error of 28.53%.\n\n\nI decided I'd like to try a more complex network and so began reading some papers on emotion recognition.  I came across one that appeared to be using the same size images (likely the same dataset).  Their proposed CNN architecture looked like this (\nRef\n):\n\n\n\n\nI implemented the paper's architecture in the CNTK Python API as follows (it had 5935330 parameters in 12 parameter tensors):\n\n\ndef\n \ncreate_model\n(\nfeatures\n):\n\n    \nwith\n \nC\n.\nlayers\n.\ndefault_options\n(\ninit\n=\nC\n.\nglorot_uniform\n(),\n \nactivation\n=\nC\n.\nrelu\n):\n\n        \nmodel\n \n=\n \nC\n.\nlayers\n.\nSequential\n([\n\n            \nC\n.\nlayers\n.\nFor\n(\nrange\n(\n3\n),\n \nlambda\n \ni\n:\n \n[\n\n                \nC\n.\nlayers\n.\nConvolution2D\n(\nfilter_shape\n=\n\n                    \n[(\n5\n,\n5\n),\n \n(\n5\n,\n5\n),\n \n(\n3\n,\n3\n)][\ni\n],\n \n                                     \nnum_filters\n=\n10\n,\n \n                                     \npad\n=\nTrue\n,\n \n                                     \nstrides\n=\n(\n1\n,\n1\n)),\n\n                    \n[\nC\n.\nlayers\n.\nAveragePooling\n(\nfilter_shape\n=\n(\n2\n,\n2\n),\n\n                                    \nstrides\n=\n1\n,\n \n                                    \npad\n=\nTrue\n),\n\n                    \nC\n.\nlayers\n.\nMaxPooling\n(\nfilter_shape\n=\n(\n2\n,\n2\n),\n\n                                    \nstrides\n=\n1\n,\n \n                                    \npad\n=\nTrue\n),\n\n                    \nC\n.\nlayers\n.\nMaxPooling\n(\nfilter_shape\n=\n(\n2\n,\n2\n),\n\n                                    \nstrides\n=\n1\n,\n \n                                    \npad\n=\nTrue\n)][\ni\n]]),\n\n            \nC\n.\nlayers\n.\nDense\n(\n256\n),\n\n            \nC\n.\nlayers\n.\nDropout\n(\n0.5\n),\n\n            \nC\n.\nlayers\n.\nDense\n(\n128\n),\n\n            \nC\n.\nlayers\n.\nDropout\n(\n0.5\n),\n\n            \nC\n.\nlayers\n.\nDense\n(\nnum_output_classes\n,\n \nactivation\n=\nNone\n)\n\n        \n])\n\n    \nreturn\n \nmodel\n(\nfeatures\n)\n\n\n\n\n\nThe above CNN architecture which matched the paper, resulted in a 25.61% average test error.  Let's see if we can do even better.\n\n\nA slightly modified, even more complex version of this architecture looks like this in the CNTK Python API (my update was to increase the number of filters for each conv layer):\n\n\ndef\n \ncreate_model\n(\nfeatures\n):\n\n    \nwith\n \nC\n.\nlayers\n.\ndefault_options\n(\ninit\n=\nC\n.\nglorot_uniform\n(),\n \nactivation\n=\nC\n.\nrelu\n):\n\n        \nmodel\n \n=\n \nC\n.\nlayers\n.\nSequential\n([\n\n            \nC\n.\nlayers\n.\nFor\n(\nrange\n(\n3\n),\n \nlambda\n \ni\n:\n \n[\n\n                \nC\n.\nlayers\n.\nConvolution2D\n(\nfilter_shape\n=\n(\n5\n,\n5\n),\n \n                                     \nnum_filters\n=\n[\n48\n,\n \n48\n,\n \n64\n][\ni\n],\n \n                                     \npad\n=\nTrue\n,\n \n                                     \nstrides\n=\n(\n1\n,\n1\n),\n\n                \nC\n.\nlayers\n.\nMaxPooling\n(\nfilter_shape\n=\n(\n2\n,\n2\n),\n\n                                    \nstrides\n=\n1\n,\n \n                                    \npad\n=\nTrue\n)\n\n                \n]),\n\n            \nC\n.\nlayers\n.\nDense\n(\n256\n),\n\n            \nC\n.\nlayers\n.\nDropout\n(\n0.5\n),\n\n            \nC\n.\nlayers\n.\nDense\n(\n128\n),\n\n            \nC\n.\nlayers\n.\nDropout\n(\n0.5\n),\n\n            \nC\n.\nlayers\n.\nDense\n(\nnum_output_classes\n,\n \nactivation\n=\nNone\n)\n\n        \n])\n\n    \nreturn\n \nmodel\n(\nfeatures\n)\n\n\n\n\n\nIn the last architecture I had 37917906 parameters in 12 parameter tensors with an average test error of 22.35% (which, to be honest, could cause an issue of over-fitting due to the very large number of parameters - just something to consider).  A validation step would be highly recommended in this case given sufficient computational power!\n\n\nConclusion\n\n\nI picked a few random pictures from an online image search of happy and sad faces.  With my error rate of 22% it did pretty well, even on pictures of my own face (not shown).  These, for instance, were correctly identified as happy and sad.\n\n\n\n\n(Note, the images may not show correctly as their 48x48 square size in this browser).\n\n\nThe improvements are not an exhaustive list of everything one can try to gain better results of course, but do represent some common ways to deal with image data using CNNs.  If you have GPU-acceleration options, give some more complex network architectures a try - you could spin up an Azure Deep Learning VM which is what I usually do at my day job.  Happy deep learning!\n\n\nThe Jupyter notebooks associated with this post can be found on GitHub \nhere\n\n\nReferences\n\n\n\n\nHistorical (Jan. 2016) Readme on CNTK Repo\n\n\nMicrosoft researchers win ImageNet computer vision challenge\n\n\nFacial Emotion Detection Using Convolutional Neural\nNetworks and Representational Autoencoder Units by Prudhvi Raj Dachapally",
            "title": "The Cognitive Toolkit (CNTK) Understands How You Feel"
        },
        {
            "location": "/cntk-has-feelings-too/#some-background",
            "text": "",
            "title": "Some Background"
        },
        {
            "location": "/cntk-has-feelings-too/#cntk",
            "text": "The original name for Microsoft's CNTK was the  Computational Network Toolkit , now known today simply as the  Cognitive Toolkit , still abbreviated CNTK for short.  It was orignally written and offered up as a C++ package and now has Python bindings, making it much more widely adoptable.   In its original words: [CNTK is] a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph   It was first open-sourced in April of 2015 with intended use for researchers and protoypers using GPUs for accelerated matrix calculations, much of what deep learning is built upon these days.  Interestingly, TensorFlow has its initial public release in November of 2015.  Of note, 2015 was also a good year for Microsoft Research in the computer vision space as they won the  ImageNet challenge  that December using this toolkit and a 152-layer deep neural network.  Since the beginning CNTK has been available for Linux and Windows.  We will be using a Linux Docker image in a minute.",
            "title": "CNTK"
        },
        {
            "location": "/cntk-has-feelings-too/#fer-faces-data",
            "text": "This data comes via a Kaggle competition on facial expressions found  here .  The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of two categories, Happy or Sad.  That being said this dataset has images (all but two of interest were dropped in this analysis) for a total of 6 emotions:  Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.",
            "title": "FER Faces Data"
        },
        {
            "location": "/cntk-has-feelings-too/#how-i-set-things-up",
            "text": "Since I'm on a Mac, I chose to use the Docker image of CNTK (instructions found  here ).  This pulls down an image of a pre-created system and I run it in my own Docker container basically recreating an Ubuntu setup with CNTK locally.  It's pretty neat!  And then I can run Jupyter notebooks on my system and pull in local files and write out data as needed.  Let me show you how and then after we'll talk about the CNNs.  By following this  Doc  I got a Jupyter notebook up and running with CNTK with all of the Tutorial notebooks at the ready and the ability to upload or create new ones as needed.  I ran these commands to get a Jupyter notebook set up with CNTK (v2.1 used here).    An important note:  in the  run  command for Docker I mounted a volume with  \"$PWD/data:/data\" .  This \"/data\" folder can be accessed from the Jupyter notebook, as you can see if you check them out (link below), but also used to add data or pull data from the docker container just like any file folder on your system.  A very handy trick!  docker pull microsoft/cntk:2.1-cpu-python3.5\n\ndocker run -d --volume \"$PWD/data:/data\" -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk:2.1-cpu-python3.5\n\ndocker exec -it cntk-jupyter-notebooks bash -c \"source /cntk/activate-cntk && jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root\"",
            "title": "How I set things up"
        },
        {
            "location": "/cntk-has-feelings-too/#what-i-did-in-a-nutshell",
            "text": "So, I tried many different CNN network architectures (simple three-layer CNN, ones with pooling and dropout layers, etc.) and several hyperparameter combinations (minibatch sizes for training, learning rate, etc.).  Here are just a few basic improvement tips for image classification based on what I found and a bit on how to do this with CNTK, plus my results - my hope is that you can do this on your own with some tips to start or maybe add to your workflow.  The Jupyter notebooks associated with this post can be found on GitHub  here",
            "title": "What I Did In a Nutshell"
        },
        {
            "location": "/cntk-has-feelings-too/#positive-results",
            "text": "",
            "title": "Positive Results"
        },
        {
            "location": "/cntk-has-feelings-too/#improvement-1-use-sufficient-data",
            "text": "I began this journey with about 1000 images that I sorted and curated into a \"sad\" or \"happy\" bucket - using the  PICS 2D face sets .  I even got up to 72% accuracy with this dataset and a CNN with pooling.  Then I discovered a Kaggle competition from about 5 years ago dealing with recognizing facial expressions -  Challenges in Representation Learning: Facial Expression Recognition Challenge  and this provided me with about 13500 training images and 2300 test images of the \"happy\" or \"sad\" kind (there were also images for categories: angry, disgust, fear, surprise and neutral in case you wish to try these out).",
            "title": "Improvement #1:  Use sufficient data"
        },
        {
            "location": "/cntk-has-feelings-too/#improvement-2-intensity-normalization",
            "text": "I read about the sensitivity of CNNs to certain normalizations.  So, I decided to try a recommended pixel intensity normalization ( Ref to nice StackOverflow hint ).  This update resulted in about a 5% increase in accuracy on the held-out test set.  My code looked like:  def   normalize ( arr ): \n     \"\"\"      Linear normalization      http://en.wikipedia.org/wiki/Normalization_%28image_processing%29      \"\"\" \n     arr   =   arr . astype ( 'float' ) \n     minval   =   arr . min () \n     maxval   =   arr . max () \n     if   minval   !=   maxval : \n         arr   -=   minval \n         arr   *=   ( 255.0 / ( maxval - minval )) \n     return   arr",
            "title": "Improvement #2:  Intensity normalization"
        },
        {
            "location": "/cntk-has-feelings-too/#improvement-3-adjust-or-even-search-hyperparameter-space",
            "text": "Play with the hyperparameters.  I had a code cell full of my hyperparameters, e.g. learning rate and minibatch sizes.  I varied one or two at a time.  Just as an example, I took the learning rate down 10x (from 0.2 to 0.02) and my resulting accuracy increased approximately 5%.  Using the  CNTK train package  one can design a \"trainer\" object encapsulating all training tasks and include parameter ranges inside it.  I also played with the strides and filter sizes or receptive fields in my CNN for the convolutional and pooling layers (see the Stanford CV course  notes  for great explanations of pretty much anything CNN and much more).  My big tip if you are starting out is simply to tune these adjustable parameters yourself and see what happens.",
            "title": "Improvement #3:  Adjust or even search hyperparameter space"
        },
        {
            "location": "/cntk-has-feelings-too/#improvement-4-add-more-layers-but-be-careful-of-overfitting",
            "text": "Interestingly, layering in three pooling layers in between the convolutional layers (last one before the dense output layer) resulted in about another 5% jump in accuracy on the test set.  (more on CNNs in the Stanford course  notes )  Here's what the relatively simple model looked like in the CNTK Python API:  def   create_model ( features ): \n     with   C . layers . default_options ( init = C . glorot_uniform (),   activation = C . relu ): \n         model   =   C . layers . Sequential ([ \n             C . layers . For ( range ( 3 ),   lambda   i :   [ \n                 C . layers . Convolution2D ( filter_shape = ( 5 , 5 ),  \n                                      num_filters = [ 8 ,   16 ,   16 ][ i ],  \n                                      pad = True ,  \n                                      strides = [( 1 , 1 ),   ( 1 , 1 ),   ( 2 , 2 )][ i ]), \n                 C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), \n                                     strides = 1 ,  \n                                     pad = True ) \n                 ]), \n             C . layers . Dense ( num_output_classes ,   activation = None ) \n         ]) \n     return   model ( features )   If you have seen some networks in CNTK, note I'm using the Sequential notation \"shortcut\" for repeating some layers, but maybe with different params (more in this  Doc ).  The above \"simple\" CNN resulted in an average test error of 28.53%.  I decided I'd like to try a more complex network and so began reading some papers on emotion recognition.  I came across one that appeared to be using the same size images (likely the same dataset).  Their proposed CNN architecture looked like this ( Ref ):   I implemented the paper's architecture in the CNTK Python API as follows (it had 5935330 parameters in 12 parameter tensors):  def   create_model ( features ): \n     with   C . layers . default_options ( init = C . glorot_uniform (),   activation = C . relu ): \n         model   =   C . layers . Sequential ([ \n             C . layers . For ( range ( 3 ),   lambda   i :   [ \n                 C . layers . Convolution2D ( filter_shape = \n                     [( 5 , 5 ),   ( 5 , 5 ),   ( 3 , 3 )][ i ],  \n                                      num_filters = 10 ,  \n                                      pad = True ,  \n                                      strides = ( 1 , 1 )), \n                     [ C . layers . AveragePooling ( filter_shape = ( 2 , 2 ), \n                                     strides = 1 ,  \n                                     pad = True ), \n                     C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), \n                                     strides = 1 ,  \n                                     pad = True ), \n                     C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), \n                                     strides = 1 ,  \n                                     pad = True )][ i ]]), \n             C . layers . Dense ( 256 ), \n             C . layers . Dropout ( 0.5 ), \n             C . layers . Dense ( 128 ), \n             C . layers . Dropout ( 0.5 ), \n             C . layers . Dense ( num_output_classes ,   activation = None ) \n         ]) \n     return   model ( features )   The above CNN architecture which matched the paper, resulted in a 25.61% average test error.  Let's see if we can do even better.  A slightly modified, even more complex version of this architecture looks like this in the CNTK Python API (my update was to increase the number of filters for each conv layer):  def   create_model ( features ): \n     with   C . layers . default_options ( init = C . glorot_uniform (),   activation = C . relu ): \n         model   =   C . layers . Sequential ([ \n             C . layers . For ( range ( 3 ),   lambda   i :   [ \n                 C . layers . Convolution2D ( filter_shape = ( 5 , 5 ),  \n                                      num_filters = [ 48 ,   48 ,   64 ][ i ],  \n                                      pad = True ,  \n                                      strides = ( 1 , 1 ), \n                 C . layers . MaxPooling ( filter_shape = ( 2 , 2 ), \n                                     strides = 1 ,  \n                                     pad = True ) \n                 ]), \n             C . layers . Dense ( 256 ), \n             C . layers . Dropout ( 0.5 ), \n             C . layers . Dense ( 128 ), \n             C . layers . Dropout ( 0.5 ), \n             C . layers . Dense ( num_output_classes ,   activation = None ) \n         ]) \n     return   model ( features )   In the last architecture I had 37917906 parameters in 12 parameter tensors with an average test error of 22.35% (which, to be honest, could cause an issue of over-fitting due to the very large number of parameters - just something to consider).  A validation step would be highly recommended in this case given sufficient computational power!",
            "title": "Improvement #4:  Add more layers (but be careful of overfitting)"
        },
        {
            "location": "/cntk-has-feelings-too/#conclusion",
            "text": "I picked a few random pictures from an online image search of happy and sad faces.  With my error rate of 22% it did pretty well, even on pictures of my own face (not shown).  These, for instance, were correctly identified as happy and sad.   (Note, the images may not show correctly as their 48x48 square size in this browser).  The improvements are not an exhaustive list of everything one can try to gain better results of course, but do represent some common ways to deal with image data using CNNs.  If you have GPU-acceleration options, give some more complex network architectures a try - you could spin up an Azure Deep Learning VM which is what I usually do at my day job.  Happy deep learning!  The Jupyter notebooks associated with this post can be found on GitHub  here",
            "title": "Conclusion"
        },
        {
            "location": "/cntk-has-feelings-too/#references",
            "text": "Historical (Jan. 2016) Readme on CNTK Repo  Microsoft researchers win ImageNet computer vision challenge  Facial Emotion Detection Using Convolutional Neural\nNetworks and Representational Autoencoder Units by Prudhvi Raj Dachapally",
            "title": "References"
        },
        {
            "location": "/masks_to_polygons_and_back/",
            "text": "A Monarch butterfly courtesy of National Geographic Kids\n\n\n\ntl:dr\n:  Masks are areas of interest in an image set to one color, or pixel value, surrounded by a contrast color or colors.  In this technical how-to, I use the OpenCV Python binding and Shapely library to create a mask, convert it to shapes as polygons, and then back to a masked image - noting some interesting properties of OpenCV and useful tricks with these libraries.\n\n\nPosted:\n  2017-09-28\n\n\nAll of the code below can be found in \nthis\n Python jupyter notebook.\n\n\nWhy are masks and polygons important?  Imagine you'd like to identify all of the pixels in a brain scan that correspond to a certain feature of the brain - maybe identify the location and contours of a mass.  Creating a mask, or highlighting just the feature's pixels on a backdrop of one contrast color, would be a good start, then understanding the shape of that masked feature as identified as polygons would give more information and perhaps help a doctor better understand any abnormalities.  But say we had a machine that only identified shapes and we wanted the masked image.  We could do so with the following process and code.  Finally, and this is how I began exploring this topic, if one wanted to create a trained machine learning model for semantic image segmentation, or essentially classifying groups of pixels, a masked image and its class label (is this greenery or human-made structure?) plus shapes would be very nice to have for training.\n\n\nMy end \ngoal\n was to turn a masked image (image with pixels of interest set to zero) into some polygon shapes and then back again using a couple of popular tools in Python.  This was motivated by a real customer engagement around semantic image segmentation and I thought it might be useful to someone in the future.\n\n\n\n\nLesson 1: \nopencv\n reads in as BGR and \nmatplotlib\n reads in a RGB\n, just in case that is ever an issue.\n\n\n\n\nI tested this as follows:\n\n\nimg_file\n \n=\n \n'monarch.jpg'\n\n\n\n# Matplotlib reads as RGB\n\n\nimg_plt\n \n=\n \nplt\n.\nimread\n(\nimg_file\n)\n\n\nplt\n.\nimshow\n(\nimg_plt\n)\n\n\n\n# Read as unchanged so that the transparency is not ignored as it would normally be by default\n\n\n# Reads as BGR\n\n\nimg_cv2\n \n=\n \ncv2\n.\nimread\n(\nimg_file\n,\n \ncv2\n.\nIMREAD_UNCHANGED\n)\n\n\nplt\n.\nimshow\n(\nimg_cv2\n)\n\n\n\n# Convert opencv BGR back to RGB\n\n\n# See https://www.scivision.co/numpy-image-bgr-to-rgb/ for more conversions\n\n\nrgb\n \n=\n \nimg_cv2\n[\n...\n,::\n-\n1\n]\n\n\nplt\n.\nimshow\n(\nrgb\n)\n\n\n\n\n\nWith the following results:\n\n\nMatplotlib RGB:\n\n\n\nOpenCV BGR:\n\n\n\nOpenCV BGR converted back:\n\n\n\nLet's define our helper functions and not worry too much about the details right now (the original source of these helpers was a Kaggle post by Konstantin Lopuhin \nhere\n - you'll need to be logged into Kaggle to see it).\n\n\nHelper to create \nMultiPolygon\ns from a masked image as \nnumpy\n array:\n\n\ndef\n \nmask_to_polygons\n(\nmask\n,\n \nepsilon\n=\n10.\n,\n \nmin_area\n=\n10.\n):\n\n    \n\"\"\"Convert a mask ndarray (binarized image) to Multipolygons\"\"\"\n\n    \n# first, find contours with cv2: it's much faster than shapely\n\n    \nimage\n,\n \ncontours\n,\n \nhierarchy\n \n=\n \ncv2\n.\nfindContours\n(\nmask\n,\n\n                                  \ncv2\n.\nRETR_CCOMP\n,\n\n                                  \ncv2\n.\nCHAIN_APPROX_NONE\n)\n\n    \nif\n \nnot\n \ncontours\n:\n\n        \nreturn\n \nMultiPolygon\n()\n\n    \n# now messy stuff to associate parent and child contours\n\n    \ncnt_children\n \n=\n \ndefaultdict\n(\nlist\n)\n\n    \nchild_contours\n \n=\n \nset\n()\n\n    \nassert\n \nhierarchy\n.\nshape\n[\n0\n]\n \n==\n \n1\n\n    \n# http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n\n    \nfor\n \nidx\n,\n \n(\n_\n,\n \n_\n,\n \n_\n,\n \nparent_idx\n)\n \nin\n \nenumerate\n(\nhierarchy\n[\n0\n]):\n\n        \nif\n \nparent_idx\n \n!=\n \n-\n1\n:\n\n            \nchild_contours\n.\nadd\n(\nidx\n)\n\n            \ncnt_children\n[\nparent_idx\n]\n.\nappend\n(\ncontours\n[\nidx\n])\n\n    \n# create actual polygons filtering by area (removes artifacts)\n\n    \nall_polygons\n \n=\n \n[]\n\n    \nfor\n \nidx\n,\n \ncnt\n \nin\n \nenumerate\n(\ncontours\n):\n\n        \nif\n \nidx\n \nnot\n \nin\n \nchild_contours\n \nand\n \ncv2\n.\ncontourArea\n(\ncnt\n)\n \n>=\n \nmin_area\n:\n\n            \nassert\n \ncnt\n.\nshape\n[\n1\n]\n \n==\n \n1\n\n            \npoly\n \n=\n \nPolygon\n(\n\n                \nshell\n=\ncnt\n[:,\n \n0\n,\n \n:],\n\n                \nholes\n=\n[\nc\n[:,\n \n0\n,\n \n:]\n \nfor\n \nc\n \nin\n \ncnt_children\n.\nget\n(\nidx\n,\n \n[])\n\n                       \nif\n \ncv2\n.\ncontourArea\n(\nc\n)\n \n>=\n \nmin_area\n])\n\n            \nall_polygons\n.\nappend\n(\npoly\n)\n\n    \nall_polygons\n \n=\n \nMultiPolygon\n(\nall_polygons\n)\n\n\n    \nreturn\n \nall_polygons\n\n\n\n\n\nHelper to create masked image as \nnumpy\n array from \nMultiPolygon\ns:\n\n\ndef\n \nmask_for_polygons\n(\npolygons\n,\n \nim_size\n):\n\n    \n\"\"\"Convert a polygon or multipolygon list back to\n\n\n       an image mask ndarray\"\"\"\n\n    \nimg_mask\n \n=\n \nnp\n.\nzeros\n(\nim_size\n,\n \nnp\n.\nuint8\n)\n\n    \nif\n \nnot\n \npolygons\n:\n\n        \nreturn\n \nimg_mask\n\n    \n# function to round and convert to int\n\n    \nint_coords\n \n=\n \nlambda\n \nx\n:\n \nnp\n.\narray\n(\nx\n)\n.\nround\n()\n.\nastype\n(\nnp\n.\nint32\n)\n\n    \nexteriors\n \n=\n \n[\nint_coords\n(\npoly\n.\nexterior\n.\ncoords\n)\n \nfor\n \npoly\n \nin\n \npolygons\n]\n\n    \ninteriors\n \n=\n \n[\nint_coords\n(\npi\n.\ncoords\n)\n \nfor\n \npoly\n \nin\n \npolygons\n\n                 \nfor\n \npi\n \nin\n \npoly\n.\ninteriors\n]\n\n    \ncv2\n.\nfillPoly\n(\nimg_mask\n,\n \nexteriors\n,\n \n1\n)\n\n    \ncv2\n.\nfillPoly\n(\nimg_mask\n,\n \ninteriors\n,\n \n0\n)\n\n    \nreturn\n \nimg_mask\n\n\n\n\n\n\n\nLesson 2:  If you want a more \"blocky\" polygon representation to save space or memory use the approximation method (add to the \nmask_to_polygons\n method).\n\n\n\n\n    \n# Approximate contours for a smaller polygon array to save on memory\n\n    \ncontours\n \n=\n \n[\ncv2\n.\napproxPolyDP\n(\ncnt\n,\n \nepsilon\n,\n \nTrue\n)\n\n                       \nfor\n \ncnt\n \nin\n \ncontours\n]\n\n\n\n\n\nWe read the image in again:\n\n\n# Read in image unchanged\n\n\nimg\n \n=\n \ncv2\n.\nimread\n(\nimg_file\n,\n \ncv2\n.\nIMREAD_UNCHANGED\n)\n\n\n\n# View\n\n\nplt\n.\nimshow\n(\nimg\n,\n \ncmap\n=\n'gray'\n,\n \ninterpolation\n=\n'bicubic'\n)\n\n\n\n\n\nWe convert to a luminance image with Scikit-Image's \nrgb2gray\n flattening it in the channel dimension:\n\n\n# Convert to a luminance image or an array which is the same size as\n\n\n# the input array, but with the channel dimension removed - flattened\n\n\nBW\n \n=\n \nrgb2gray\n(\nimg\n)\n\n\n\n# View\n\n\nplt\n.\nimshow\n(\nBW\n,\n \ncmap\n=\n'gray'\n,\n \ninterpolation\n=\n'bicubic'\n)\n\n\n\n\n\nWith this result:\n\n\n\n\n\nLesson 3:  For a quick mask we can use OpenCV's \nconvertScaleAbs\n function and it also is needed for the helper\n\n\n\n\nAs far as this pre-processing step goes, \ncv2.convertScaleAbs\n converts the image to an 8-bit unsigned integer with 1 channel, essentially flattening it and getting it ready to create some polygons (actually \nMultiPolygon\ns in this case).\n\n\n# Convert to CV_8UC1 for creating polygons with shapely\n\n\n# CV_8UC1 is an 8-bit unsigned integer with 1 channel\n\n\nBW\n \n=\n \ncv2\n.\nconvertScaleAbs\n(\nBW\n)\n\n\n\n# View\n\n\nplt\n.\nimshow\n(\nBW\n,\n \ncmap\n=\n'gray'\n,\n \ninterpolation\n=\n'bicubic'\n)\n\n\n\n\n\nWith this result:\n\n\n\nNow let's let our functions do the real work!  Convert to polygons and then back to a masked image:\n\n\n# Get the polygons using shapely\n\n\npolys\n \n=\n \nmask_to_polygons\n(\nBW\n,\n \nmin_area\n=\n50\n)\n\n\n\n# Convert the polygons back to a mask image to validate that all went well\n\n\nmask\n \n=\n \nmask_for_polygons\n(\npolys\n,\n \nBW\n.\nshape\n[:\n2\n])\n\n\n\n# View - you'll see some loss in detail compared to the before-polygon \n\n\n# image if min_area is high - go ahead and try different numbers!\n\n\nplt\n.\nimshow\n(\nmask\n,\n \ncmap\n=\n'gray'\n,\n \ninterpolation\n=\n'nearest'\n)\n\n\n\n\n\nFinal result:\n\n\n\n\nNotice the slight loss of detail - this is because we are removing really tiny polygons (see \nmin_area\n parameter).\n\n\nI leave it up to you to download \nthis\n Python jupyter notebook and try using the RGB image for masking and creating Polygons and back.  Do the results change?  Try this on your own images and have fun!",
            "title": "Shapely Shapes and OpenCV Visions"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/",
            "text": "tl:dr\n:  Getting a simple, predictive framework distinguishing two types of leukemia based on biological markers from a single-layer neural network was not the intent of this exercise. It is, however, indicative of the power of a single artificial neuron and thoughtful feature reduction.\n\n\nPosted:\n  2017-07-19\n\n\nIntroduction\n\n\nThe intent of this post originally was to show the inner workings and limitations of a single artificial neuron using some moderately complex, noisy data; a challenge of sorts - \"is this noisy data linearly separable with a single artificial neuron and if not, why is that?\".  \n\n\nHowever, I found with some data and algorithm exploration, that I could distinguish between two types of leukemia \u2014 a naive approach and not really biologically significant, but an interesting outcome nonetheless.  So, even though this post is about the data science, it also touches on a potential method to use in the real world.\n\n\nIn this post, you'll find information on the use of PCA for data reduction/feature engineering, scaling and normalization for preprocessing, the Adaline algorithm (artificial neuron), different activation functions, among other topics and concepts.\n\n\n\n\nWhat is an Adaline artificial neuron\n\n\nAdaline with a sigmoid activation function\n\n\nChoosing an activation function\n\n\nThe noisy data\n\n\n3D to run through network and 2D to gain insights\n\n\nConclusion from my experiment\n\n\nCredits and further reading\n\n\n\n\nWhat is an Adaline artificial neuron\n\n\nThe ADAptive LInear NEuron (Adaline) algorithm is very similar to a Perceptron (simplest of the artificial neurons) except that in the Perceptron the weights are updated based on a unit step activation function output (see figure below) whereas Adaline uses a linear activation function to update it's weights giving it a more robust result (that even converges with samples that are not completely separable by a linear hyperplane, unlike the Perceptron).  In Adaline a \nquantizer\n after the activation function, is used to then predict class labels.\n\n\nBeyond the linear activation function and the \nquantizer\n, we see the use of a \ncost function\n, or \nobjective function\n, to update the weights.  In this case we want to minimize this function with an optimization method.  The optimization of the \ncost function\n happens with yet another function aptly and simply named an \noptimization function\n.  In this case our optimization function is \nstochastic gradient decent\n, which one can of as \"climbing down a hill\" (using part of the data to calculate, shuffled as well) to get to the minima of the cost function's convex curve (as it updates weights iteratively from a shuffled dataset).\n\n\nA really great discussion from which much of this information was adapted can be found in Sebastian Raschka's \nPython Machine Learning\n book (link \nhere\n) and excellent blog post on this topic \nhere\n on the single-layer neurons.\n\n\nAdaline with a sigmoid activation function\n\n\nI grabbed Raschka's ADAptive LInear NEuron (Adaline) classifier open-source code \nhere\n (the AdalineSGD class) and updated the activation function to logistic sigmoid from a linear function.\n\n\nNote, with the Adaline (versus the Perceptron) we use a continuous number rather than the binary class label, to compute the model error and update the weights.  Then to predict a class label, another function is used called a \nquantizer\n.  Also, the weights are updated in a more sophisticated manner.\n\n\nChoosing an activation function\n\n\n\n\nIn code, given this \"net input\" function:\n\n\n    \ndef\n \nnet_input\n(\nself\n,\n \nX\n):\n\n        \n\"\"\"Calculate net input\"\"\"\n\n        \nreturn\n \nnp\n.\ndot\n(\nX\n,\n \nself\n.\nw_\n[\n1\n:])\n \n+\n \nself\n.\nw_\n[\n0\n]\n\n\n\n\n\nI update the activation function from linear as in:   \n\n\n    \ndef\n \nactivation\n(\nself\n,\n \nX\n):\n\n        \n\"\"\"Compute linear activation\"\"\"\n\n        \nreturn\n \nself\n.\nnet_input\n(\nX\n)\n\n\n\n\n\nTo a logistic sigmoidal function:\n\n\n    \ndef\n \nactivation\n(\nself\n,\n \nX\n):\n\n        \n\"\"\"Compute sigmoidal activation\n\n\n\n        Returns\n\n\n        -------\n\n\n        A 1d array of length n_samples\n\n\n\n        \"\"\"\n\n        \nx\n \n=\n \nself\n.\nnet_input\n(\nX\n)\n\n        \nfunc\n \n=\n \nlambda\n \nv\n:\n \n1\n \n/\n \n(\n1\n \n+\n \nmath\n.\nexp\n(\n-\nv\n))\n\n        \nreturn\n \nnp\n.\narray\n(\nlist\n(\nmap\n(\nfunc\n,\n \nx\n)))\n\n\n\n\n\nFull code \nhere\n and \nhere\n.\n\n\nWe still get linear classification boundaries\n\n\nThese single-neuron classifiers can only result in linear decision boundaries, even if using a non-linear activation, because it's still using a single threshold value, \nz\n as in diagram above, to decide whether a data point is classified as 1 or -1.\n\n\nThe noisy data\n\n\nThe data was downloaded from the Machine Learning Data Set Repository \nmldata.org\n using a convenience function from \nscikit-learn\n.  \n\n\nfrom\n \nsklearn.datasets.mldata\n \nimport\n \nfetch_mldata\n\n\n\n# Fetch a small leukemia dataset from mldata.org\n\n\n#   http://mldata.org/repository/data/viewslug/leukemia-all-vs-aml/\n\n\ntest_data_home\n \n=\n \ntempfile\n.\nmkdtemp\n()\n\n\nleuk\n \n=\n \nfetch_mldata\n(\n'leukemia'\n,\n \ntranspose_data\n=\nTrue\n,\n\n                      \ndata_home\n=\ntest_data_home\n)\n\n\n\n\n\nThe data is a small, but wide acute lymphocytic leukemia (ALL) vs. acute myelogenous leukemia (AML) dataset.  It has approximately 7000 biological markers (our features), vs. 72 samples (our data points).\n\n\nGiven the noisy nature of the data and possible skewedness, it was standardized and normalized with convenience functions from \nscikit-learn\n:\n\n\nfrom\n \nsklearn.preprocessing\n \nimport\n \nRobustScaler\n,\n \nNormalizer\n\n\n\n# Fit the scalar to the training dataset for \n\n\n#   zero mean and unit variance of features.\n\n\n#   Using a robust scaler which is more resistent to outliers.\n\n\nscaler\n \n=\n \nRobustScaler\n()\n\n\nscaler\n.\nfit\n(\nX_train\n)\n\n\n\n# Apply the transform\n\n\nX_train\n \n=\n \nscaler\n.\ntransform\n(\nX_train\n)\n\n\n\n# Apply the same transform to the test dataset \n\n\n#   (simulating what happens when we get new data)\n\n\nX_test\n \n=\n \nscaler\n.\ntransform\n(\nX_test\n)\n\n\n\n# Normalizing data as well to scale samples to unit norm\n\n\nnormalizer\n \n=\n \nNormalizer\n()\n.\nfit\n(\nX_train\n)\n\n\nX_train\n \n=\n \nnormalizer\n.\ntransform\n(\nX_train\n)\n\n\nX_test\n \n=\n \nnormalizer\n.\ntransform\n(\nX_test\n)\n\n\n\n\n\nFull code \nhere\n and \nhere\n.\n\n\nI tried just one feature reduction with PCA to reduce all 7129 dimensions to 2D at first.  However, I could not separate out the ALL samples from AML - this wasn't necessarily important to my post on Adaline neurons I was writing, but I decided to try something I'd read about recently for kicks.  In fact the idea sprung from a comment in a Python script where a perceptron was used to create non-linear separation of data for a plot (from \nthis\n script on Github).  The comment went:\n\n\n# map the data into a space with one addition dimension so that\n# it becomes linearly separable\n\n\n\n\nSo, I gave it a shot.\n\n\n3D to run through network and 2D to gain insights\n\n\nMy next step was to try feeding the neural network the data in 3D space (the 3 features or components from the first PCA reduction).\n\n\nI then reduced the 3D data to 2D, mainly to visualize it.  A hyperplane was drawn (blank dashed line) to represent the decision boundary.  The surface in the diagram below is representative of a sigmoidal output along the direction of the weight vector.\n\n\n\n\nFull code \nhere\n and \nhere\n.\n\n\nNote, the stochastic part of the single-neuron optimizer, stochastic gradient decent, causes some variation in the results if run again.  It might be a good idea to do a batch version of the Adaline neuron.  Another note is that one does not necessarily have to use a logistic sigmoidal activation function; it was just used here as an experiment and to prove to myself I'd always get a linear decision boundary.\n\n\nConclusion from my experiment\n\n\nI was surprised and impressed that I got a linearly separable result!  Albeit, that was not the intent of this exercise, but indicative of the power of a single neuron and thoughtful feature reduction.  It makes me wonder what a small neural network could do!\n\n\nCredits and further reading\n\n\n\n\nSebastian Raschka's \nPython Machine Learning\n \nbook\n\n\nThe open-source notebooks with code accompanying the \nPython Machine Learning\n book \nhere\n and related code \nhere\n\n\nRaschka's blog \npost\n on \nSingle-Layer Neural Networks and Gradient Descent\n\n\nScikit-learn\n's preprocessing data module \nlink\n for scaling features and samples",
            "title": "On using an Adaline Artificial Neuron for Classification"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#introduction",
            "text": "The intent of this post originally was to show the inner workings and limitations of a single artificial neuron using some moderately complex, noisy data; a challenge of sorts - \"is this noisy data linearly separable with a single artificial neuron and if not, why is that?\".    However, I found with some data and algorithm exploration, that I could distinguish between two types of leukemia \u2014 a naive approach and not really biologically significant, but an interesting outcome nonetheless.  So, even though this post is about the data science, it also touches on a potential method to use in the real world.  In this post, you'll find information on the use of PCA for data reduction/feature engineering, scaling and normalization for preprocessing, the Adaline algorithm (artificial neuron), different activation functions, among other topics and concepts.   What is an Adaline artificial neuron  Adaline with a sigmoid activation function  Choosing an activation function  The noisy data  3D to run through network and 2D to gain insights  Conclusion from my experiment  Credits and further reading",
            "title": "Introduction"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#what-is-an-adaline-artificial-neuron",
            "text": "The ADAptive LInear NEuron (Adaline) algorithm is very similar to a Perceptron (simplest of the artificial neurons) except that in the Perceptron the weights are updated based on a unit step activation function output (see figure below) whereas Adaline uses a linear activation function to update it's weights giving it a more robust result (that even converges with samples that are not completely separable by a linear hyperplane, unlike the Perceptron).  In Adaline a  quantizer  after the activation function, is used to then predict class labels.  Beyond the linear activation function and the  quantizer , we see the use of a  cost function , or  objective function , to update the weights.  In this case we want to minimize this function with an optimization method.  The optimization of the  cost function  happens with yet another function aptly and simply named an  optimization function .  In this case our optimization function is  stochastic gradient decent , which one can of as \"climbing down a hill\" (using part of the data to calculate, shuffled as well) to get to the minima of the cost function's convex curve (as it updates weights iteratively from a shuffled dataset).  A really great discussion from which much of this information was adapted can be found in Sebastian Raschka's  Python Machine Learning  book (link  here ) and excellent blog post on this topic  here  on the single-layer neurons.",
            "title": "What is an Adaline artificial neuron"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#adaline-with-a-sigmoid-activation-function",
            "text": "I grabbed Raschka's ADAptive LInear NEuron (Adaline) classifier open-source code  here  (the AdalineSGD class) and updated the activation function to logistic sigmoid from a linear function.  Note, with the Adaline (versus the Perceptron) we use a continuous number rather than the binary class label, to compute the model error and update the weights.  Then to predict a class label, another function is used called a  quantizer .  Also, the weights are updated in a more sophisticated manner.",
            "title": "Adaline with a sigmoid activation function"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#choosing-an-activation-function",
            "text": "In code, given this \"net input\" function:       def   net_input ( self ,   X ): \n         \"\"\"Calculate net input\"\"\" \n         return   np . dot ( X ,   self . w_ [ 1 :])   +   self . w_ [ 0 ]   I update the activation function from linear as in:          def   activation ( self ,   X ): \n         \"\"\"Compute linear activation\"\"\" \n         return   self . net_input ( X )   To a logistic sigmoidal function:       def   activation ( self ,   X ): \n         \"\"\"Compute sigmoidal activation          Returns          -------          A 1d array of length n_samples          \"\"\" \n         x   =   self . net_input ( X ) \n         func   =   lambda   v :   1   /   ( 1   +   math . exp ( - v )) \n         return   np . array ( list ( map ( func ,   x )))   Full code  here  and  here .  We still get linear classification boundaries  These single-neuron classifiers can only result in linear decision boundaries, even if using a non-linear activation, because it's still using a single threshold value,  z  as in diagram above, to decide whether a data point is classified as 1 or -1.",
            "title": "Choosing an activation function"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#the-noisy-data",
            "text": "The data was downloaded from the Machine Learning Data Set Repository  mldata.org  using a convenience function from  scikit-learn .    from   sklearn.datasets.mldata   import   fetch_mldata  # Fetch a small leukemia dataset from mldata.org  #   http://mldata.org/repository/data/viewslug/leukemia-all-vs-aml/  test_data_home   =   tempfile . mkdtemp ()  leuk   =   fetch_mldata ( 'leukemia' ,   transpose_data = True , \n                       data_home = test_data_home )   The data is a small, but wide acute lymphocytic leukemia (ALL) vs. acute myelogenous leukemia (AML) dataset.  It has approximately 7000 biological markers (our features), vs. 72 samples (our data points).  Given the noisy nature of the data and possible skewedness, it was standardized and normalized with convenience functions from  scikit-learn :  from   sklearn.preprocessing   import   RobustScaler ,   Normalizer  # Fit the scalar to the training dataset for   #   zero mean and unit variance of features.  #   Using a robust scaler which is more resistent to outliers.  scaler   =   RobustScaler ()  scaler . fit ( X_train )  # Apply the transform  X_train   =   scaler . transform ( X_train )  # Apply the same transform to the test dataset   #   (simulating what happens when we get new data)  X_test   =   scaler . transform ( X_test )  # Normalizing data as well to scale samples to unit norm  normalizer   =   Normalizer () . fit ( X_train )  X_train   =   normalizer . transform ( X_train )  X_test   =   normalizer . transform ( X_test )   Full code  here  and  here .  I tried just one feature reduction with PCA to reduce all 7129 dimensions to 2D at first.  However, I could not separate out the ALL samples from AML - this wasn't necessarily important to my post on Adaline neurons I was writing, but I decided to try something I'd read about recently for kicks.  In fact the idea sprung from a comment in a Python script where a perceptron was used to create non-linear separation of data for a plot (from  this  script on Github).  The comment went:  # map the data into a space with one addition dimension so that\n# it becomes linearly separable  So, I gave it a shot.",
            "title": "The noisy data"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#3d-to-run-through-network-and-2d-to-gain-insights",
            "text": "My next step was to try feeding the neural network the data in 3D space (the 3 features or components from the first PCA reduction).  I then reduced the 3D data to 2D, mainly to visualize it.  A hyperplane was drawn (blank dashed line) to represent the decision boundary.  The surface in the diagram below is representative of a sigmoidal output along the direction of the weight vector.   Full code  here  and  here .  Note, the stochastic part of the single-neuron optimizer, stochastic gradient decent, causes some variation in the results if run again.  It might be a good idea to do a batch version of the Adaline neuron.  Another note is that one does not necessarily have to use a logistic sigmoidal activation function; it was just used here as an experiment and to prove to myself I'd always get a linear decision boundary.",
            "title": "3D to run through network and 2D to gain insights"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#conclusion-from-my-experiment",
            "text": "I was surprised and impressed that I got a linearly separable result!  Albeit, that was not the intent of this exercise, but indicative of the power of a single neuron and thoughtful feature reduction.  It makes me wonder what a small neural network could do!",
            "title": "Conclusion from my experiment"
        },
        {
            "location": "/single-artifical-neuron-for-nonlinear-separable-data/#credits-and-further-reading",
            "text": "Sebastian Raschka's  Python Machine Learning   book  The open-source notebooks with code accompanying the  Python Machine Learning  book  here  and related code  here  Raschka's blog  post  on  Single-Layer Neural Networks and Gradient Descent  Scikit-learn 's preprocessing data module  link  for scaling features and samples",
            "title": "Credits and further reading"
        },
        {
            "location": "/my-new-static-site-generator-hobby/",
            "text": "tl:dr\n:  A little post on using static site generators to overlay a website on top of a GitHub repo for displaying docs, portfolios/products, and blogs.\n\n\nPosted\n:  2017-06-18\n\n\nI recently discovered I can have a static website for each of my GitHub repositories, which seems like overkill for 35+ repos.  That being said, it certainly would be nice to attractively present some of my content I've painstakingly written in Markdown files to showcase my work.\n\n\nUsing static site generators, like Jekyll or MkDocs (and others \u2014 good article \nhere\n) combined with a templating engine like Jinga, opens up a lovely world of having human-friendly interfaces on top of my repos that I want to share.  The complexity of the sites can get quite intricate \u2014 to my hearts content.\n\n\n\n\nUsage of the Jekyll Webjeda theme for a course listing site\n\n\n\nMarkup languages like Markdown are pretty easy to write in and create text files with headings, lists, tables and such (GitHub made a nice guide \nhere\n).  However, if you like reStructuredText or HTML better there are generators out there for you.\n\n\nThere's a great site to shop for a Jekyll theme (where I began this journey), \nhere\n.  They are mostly slanted towards blog writers as that was the reason for the genesis of Jekyll by the founder of GitHub (thanks Tom Preston-Werner!  Find out more about Jekyll on \nthis\n blog).\n\n\nThere is of course the use of GitHub Pages to simply render the repo's README markdown file by clicking on \nSettings\n and scrolling down to \nGitHub Pages\n, then selecting a theme directly at that point.  \n\n\nBasically, I'm just scratching the surface here on another way.  Also, the generators I chose all look good on hand-held devices - an important aspect to consider.\n\n\nThese are some scenarios for the site generators I'm introducing based on research and actual work I've done:\n\n\n\n\nChalk\n\n\nBlogs\n\n\n\n\nThings listed by dates\n\n\n\n\n\n\nWebjeda Cards\n\n\n\n\nBlogs\n\n\nPortfolios\n\n\n\n\nModular presentations e.g. product cards\n\n\n\n\n\n\nMkDocs\n\n\n\n\nDocumentation\n\n\nGuides or tutorials\n\n\n\n\nWith the following information, hopefully you can begin to successfully build sites based on these generators.  I hope to fork them at some point for contributing back and encourage you to do so if you figure out something useful.\n\n\nClarification on project setups:  some themes or static site generators that use repositories for content, have a branch for building the site (source) and a branch (usually called \ngh-pages\n) for the sites deployment files (MkDocs sites do this).  Some themes just work under \nmaster\n or have a mirrored dev branch.\n\n\nChalk\n\n\n\n\nChalk demo site\n\n\nChalk is a high quality, completely customizable, performant and 100% free blog template for Jekyll.\n\n\nCreator's Profile:  \nhttps://github.com/nielsenramon\n\n\nChalk is my favorite Jekyll theme for blogging.  It's simple and clean in its look.  However, Chalk doesn't support the standard way of working with Jekyll on GitHub pages due to custom plugins.  There's a little bit more complexity around building the site with these plugins, but all of the scripts are provided so it's actually quite easy in the long run to build and deploy to GitHub Pages or another hosting service.\n\n\nCheck out these sites:\n\n\nDemo site\n\n\nChalk GitHub repo\n\n\nRecommended tweaks: \n\n\n\n\n\n\nUse the dark code highlighting theme inside of the light Chalk theme for a Sublime Editor-like effect.  Modify an import at the bottom of the \n/_assets/stylesheets/light.scss\n:\n\n\n@import \"modules/highlights-dark\";\n\n\n\n\n\n\n\n\nAdd an icon to the post listing page (main page) by editing the \nindex.html\n with a \ndiv\n tag:\n\n\n<\ndiv\n \nclass\n=\n\"article-list-tags\"\n>\n\n  \n<\nimg\n \nsrc\n=\n\"{{ post.logo }}\"\n \nalt\n=\n\"post logo\"\n \n      \nhref\n=\n\"{{ post.url | relative_url }}\"\n \n      \nwidth\n=\n\"30%\"\n \nalign\n=\n\"center\"\n></\nimg\n>\n\n\n</\ndiv\n>\n\n\n\nAnd adding the \npost.logo\n to the YAML heading on the post as in:\n\n\n\n\n\n\n ---\n ...\n comments: true\n description: A short chatterbot dev story\n logo:  \"../resources/images/ocrbot_local.png\"\n tags:\n ...\n ---\n\n\n\n\n\n\nA tweaked Chalk-based site with icons on main page\n\n\nWebjeda Cards\n\n\n\n\nWebjeda demo site\n\n\nWebjeda Cards is a Bootstrap based jekyll theme for portfolio, photography or any kind of blog.\n\n\nCreator's Profile:  \nhttps://github.com/sharu725\n\n\nThis Jekyll theme is fantastic for more modular postings (like course or product listings).  It could also be a great place to show off your work like photos you've taken with a nice write-up.  Perhaps, this could be the main page of your site and you could link to all of the repos you'd like to share from this card layout design.\n\n\nSetup note:  you can of course simply use the master branch here for building \nand\n deploying, but it's always nice to have a separate branch we often see called \ngh-pages\n for the sites actual deployed content.\n\n\nCheck out these sites:\n\n\nDemo site\n\n\nWebjeda Cards GitHub repo\n\n\nRecommended tweaks:\n\n\n\n\n\n\nIn \nindex.html\n added a variable to automatically pull in the \"Read\" button name (so, instead of just \"Read\" it's a custom label on the button).  This is done with the \npost.label\n:\n\n\n \n<\ndiv\n \nclass\n=\n\"panel-body\"\n>\n\n \n<\nh3\n \nclass\n=\n\"panel-title pull-left\"\n>\n{{ post.title }}\n    \n</\nh3\n><\nspan\n \nclass\n=\n\"post-meta pull-right\"\n><\nsmall\n></\nsmall\n></\nspan\n>\n\n \n<\na\n \nhref\n=\n\"{{ post.url | prepend: site.baseurl }}\"\n \n    \nclass\n=\n\"btn btn-primary btn-sm pull-right mt10\"\n>\n\n    Go to {{post.label}}\n</\na\n>\n\n \n</\ndiv\n>\n\n\n\n\n\n\n\n\n\nThen, in the post you'll have a YAML header with a \nlabel\n variable like:\n\n\n---\nlayout: post\ntitle:  Microsoft R Server and SQL Server R Services Labs\ncategories: mrs\nimg: hikeclouds.jpg\nlabel: Labs\nresource:\n---\n\n\n\n\n\n\n\n\nMkDocs\n\n\n\n\nDocs for one of my projects using the readthedocs theme\n\n\nMkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.\n\n\nCreators' profile:  \nhttps://github.com/mkdocs\n\n\nThis is a great alternative to a Sphinx build for a readthedocs style documentation page.  It's very easy to setup and use.  There are other builtin themes as well.  It's fairly pre-baked, but very good for what it's good for.\n\n\nDocumentation for MkDocs",
            "title": "Overlaying a Website ontop of a GitHub Repository"
        },
        {
            "location": "/my-new-static-site-generator-hobby/#chalk",
            "text": "Chalk demo site  Chalk is a high quality, completely customizable, performant and 100% free blog template for Jekyll.  Creator's Profile:   https://github.com/nielsenramon  Chalk is my favorite Jekyll theme for blogging.  It's simple and clean in its look.  However, Chalk doesn't support the standard way of working with Jekyll on GitHub pages due to custom plugins.  There's a little bit more complexity around building the site with these plugins, but all of the scripts are provided so it's actually quite easy in the long run to build and deploy to GitHub Pages or another hosting service.  Check out these sites:  Demo site  Chalk GitHub repo  Recommended tweaks:     Use the dark code highlighting theme inside of the light Chalk theme for a Sublime Editor-like effect.  Modify an import at the bottom of the  /_assets/stylesheets/light.scss :  @import \"modules/highlights-dark\";    Add an icon to the post listing page (main page) by editing the  index.html  with a  div  tag:  < div   class = \"article-list-tags\" > \n   < img   src = \"{{ post.logo }}\"   alt = \"post logo\"  \n       href = \"{{ post.url | relative_url }}\"  \n       width = \"30%\"   align = \"center\" ></ img >  </ div >  \nAnd adding the  post.logo  to the YAML heading on the post as in:     ---\n ...\n comments: true\n description: A short chatterbot dev story\n logo:  \"../resources/images/ocrbot_local.png\"\n tags:\n ...\n ---   A tweaked Chalk-based site with icons on main page",
            "title": "Chalk"
        },
        {
            "location": "/my-new-static-site-generator-hobby/#webjeda-cards",
            "text": "Webjeda demo site  Webjeda Cards is a Bootstrap based jekyll theme for portfolio, photography or any kind of blog.  Creator's Profile:   https://github.com/sharu725  This Jekyll theme is fantastic for more modular postings (like course or product listings).  It could also be a great place to show off your work like photos you've taken with a nice write-up.  Perhaps, this could be the main page of your site and you could link to all of the repos you'd like to share from this card layout design.  Setup note:  you can of course simply use the master branch here for building  and  deploying, but it's always nice to have a separate branch we often see called  gh-pages  for the sites actual deployed content.  Check out these sites:  Demo site  Webjeda Cards GitHub repo  Recommended tweaks:    In  index.html  added a variable to automatically pull in the \"Read\" button name (so, instead of just \"Read\" it's a custom label on the button).  This is done with the  post.label :    < div   class = \"panel-body\" > \n  < h3   class = \"panel-title pull-left\" > {{ post.title }}\n     </ h3 >< span   class = \"post-meta pull-right\" >< small ></ small ></ span > \n  < a   href = \"{{ post.url | prepend: site.baseurl }}\"  \n     class = \"btn btn-primary btn-sm pull-right mt10\" > \n    Go to {{post.label}} </ a > \n  </ div >     Then, in the post you'll have a YAML header with a  label  variable like:  ---\nlayout: post\ntitle:  Microsoft R Server and SQL Server R Services Labs\ncategories: mrs\nimg: hikeclouds.jpg\nlabel: Labs\nresource:\n---",
            "title": "Webjeda Cards"
        },
        {
            "location": "/my-new-static-site-generator-hobby/#mkdocs",
            "text": "Docs for one of my projects using the readthedocs theme  MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.  Creators' profile:   https://github.com/mkdocs  This is a great alternative to a Sphinx build for a readthedocs style documentation page.  It's very easy to setup and use.  There are other builtin themes as well.  It's fairly pre-baked, but very good for what it's good for.  Documentation for MkDocs",
            "title": "MkDocs"
        },
        {
            "location": "/data-science-story-part1/",
            "text": "tl;dr\n:  My path into data science involved dabbling in a new programming language, stalking GitHub for a popular machine learning package, doing some networking and finding some other people to teach.\n\n\nPosted:\n  2017-05-15\n\n\nLast week, I had the amazing opportunity of co-presenting a talk called \"Navigating the AI Revolution\" at Microsoft's developer conference, //build 2017.  I spoke about questions we can answer with machine learning and mentioned a little bit about \nmy path into data science\n.  Afterwards, many folks approached me about this path and asked for some more guidance and advice.  I thought I'd begin by expanding on my story of how I started out.  My hope is that something in here is helpful to you.\n\n\nBefore I was a data scientist I was a developer.  TBH I'm now a developer again and getting to do more data science than ever before.  One of my favorite articles of late states that the real prerequisite for machine learning is not math, but rather \ndata analysis\n skills like data viz and wrangling (link \nhere\n).  But what's next, i.e., how did I begin actually learning data science...\n\n\n\n\nTip:  Start doing data visualization and/or data cleaning\n\n\n\n\nAt the beginning, this historic tweet by Josh Wills was both funny and helpful to me -- giving me perspective.\n\n\n\n\nBefore I presented on AI to 700 people at //build, before I taught an ML package at a workshop, before I had read docs and gone through the tutorials and before I had trolled YouTube for ML videos, I had attended the Open Data Science Conference in the Fall of 2015.  \nI networked, as we do\n, and met a group of devs/data scientists/physicists/biologists interested in data science like me (a great, diverse group of super smart people!) and immediately got involved in a Python workshop group (GitHub \norganization\n).  I got asked to help them give a workshop on ML, scikit-learn and tensorflow they were planning for the following year because I had a some basic Python materials they could use.\n\n\n\n\nTip:  Network with those in your field interested in data science\n\n\n\n\nMost of what I had learned was from one single package, \nscikit-learn\n which I chose based on its popularity and GitHub activity.  I also studied videos on scikit-learn like \nthis\n one by Jake VanderPlas and \nthis\n one by Olivier Grisel.  After some YouTube'ing and reading the docs, plus talking about it with other data scientists in my workshop-planning group, \nI was ready to teach the basics to others\n.  The old adage is true, \nthe best way to learn is to teach\n.\n\n\nScikit-learn is a Python machine learning library with incredible docs that not only explain the package, \nactually taught me something useful\n once I had watched a couple of basic videos like the ones just mentioned (check out \nthis\n page from the docs).  The development of this package is now led by Andreas M\u00fcller, a lecturer at the Data Science Institute at Columbia University.  He's also a big proponent of women as contributors to scikit-learn and open source, openly encouraging more and more to contribute.\n\n\n\n\nTip:  Pick a package to learn that is popular and active\n\n\n\n\nIn May of 2016, I \nco-instructed\n a workshop (close to 200 folks, mostly women) at a Google office in Mountain View, CA.  They thought it was funny when I said \"I'm now in the belly of the beast\" (I'm a Microsofty).  I was still a new data scientist, but I knew how to use a powerful ML tool to understand some simple data (the \niris dataset\n) and could communicate that to others.\n\n\nI didn't start out teaching or creating courses by any means.  I started out by being curious about analytics, for many years doing genomics and proteomics.  I started dabbling in Python as necessary and a little R for my analyses and visualizations.  I chose to write some teaching material because I wanted to learn Python better (you can start out small even with a readme or two on GitHub) and off I went.\n\n\n\n\nTip:  Pick a language like R or Python and plan to create some training material on something analytical for work or fun\n\n\n\n\nAll of this certainly required experience in a new language, Python, adopted by the data science community along with R as the go-to programming language for its intuitive syntax, readability, and existing math libraries, among other reasons.  I had been dabbling in Python for a few years, doing basic math and web programming.  I really grokked Python, however, after I had \ncreated a Python course of my own\n (find it \nhere\n) \u2014 see a theme here? :)",
            "title": "On Being a Data Scientist"
        },
        {
            "location": "/two-cents-on-python-package-structure/",
            "text": "tl;dr\n:  There's a standard for structuring a Python package and \nsetup.py\n is at the heart of it.\n\n\nPosted\n:  2017-03-06\n\n\nI've pulled from several sources (see Resources below) and have mashed them together to create a brief synopsis of what I plan on doing for my first python package.  I thought I'd share these findings with you.  I've tried to be python \u2154 agnostic as needed.\n\n\nTo make the most barebones package we can use the following structure (if we include the right code in \nsetup.py\n this could be a \npip\n installable package in no time!).  \ncoolname_project\n is the GitHub repo name and what I refer to as the base folder.  This is the structure of our barebones package:\n\n\ncoolname_project\n    coolname/\n        __init__.py\n    setup.py\n\n\n\n\nAn example of a more common structure I've seen:\n\n\ncoolname_project\n    coolname/\n        __init__.py\n        somecoolmodule.py\n        command_line.py\n        test/\n            test_somecoolmodule.py\n            test_command_line.py\n    docs/\n        greatdoc.txt\n    bin/\n        runsomestuff.sh\n    examples/\n        snippet.py\n    setup.py\n    README\n\n\n\n\nKeep reading to find out what goes in these folders and files.\n\n\nA very quick example of a barebones package\n\n\nTo \n__init__.py\n add this:\n\n\nfrom\n \n__future__\n \nimport\n \nprint_function\n\n\ndef\n \nfoo\n():\n\n    \nprint\n(\n42\n)\n\n\n\n\n\nNow we can use our brand new package in python:\n\n\n>>> import coolname\n>>> coolname.foo\n()\n\n\n\n\n\nAdd the following to \nsetup.py\n (more should go here and we'll see in a bit, but this is barebones right now):\n\n\nfrom\n \nsetuptools\n \nimport\n \nsetup\n\n\n\nsetup\n(\nname\n=\n'coolname'\n,\n\n      \nversion\n=\n'0.1'\n,\n\n      \ndescription\n=\n'The coolest package around'\n,\n\n      \nurl\n=\n'http://github.com/<your username>/coolname_project'\n,\n\n      \nauthor\n=\n'Your Name'\n,\n\n      \nauthor_email\n=\n'name@example.com'\n,\n\n      \nlicense\n=\n'MIT'\n,\n\n      \npackages\n=\n[\n'coolname'\n],\n\n      \nzip_safe\n=\nFalse\n)\n\n\n\n\n\nAdding a \nsetup.py\n with certain info allows us to be able to do an awesome thing: \npip install\n our package (here, locally).  In our base package folder just type:\n\n\npip install .\n\n\nCongrats!  You have an awesome, little package (although it doesn't do anything very cool yet - that's up to you!).\n\n\nNote:  the \nsetup.py\n is a powerful tool and will likely contain much, much more like dependency specifications, more metadata around the package, entry points, testing framework specs, etc.\n\n\nRead on...the dual-purpose README\n\n\nDon't you just love reusability?\n\n\nIf we write our README in reStructuredText format it not only will look good on GitHub, it'll serve as the \nlong_description\n or detailed description of our package on PyPi.  To make sure this happens we need a file called \nMANIFEST.in\n as well.  \nMANIFEST.in\n also does some more useful things (see Jeff Knupp's article in Resources below).\n\n\nSo, we could, for example, have in our \nREADME.rst\n file:\n\n\nThe Coolest Package Ever\n--------\n\nTo use (with caution), simply do::\n    >>> import coolname\n    >>> coolname.foo()\n\n\n\n\nThe in our \nMANIFEST.in\n (this file does other things down the road, but for now we'll use it to include our README):\n\n\ninclude README.rst\n\n\n\n\nSummary of the folder/files in a package\n\n\nThis is what I've gleaned so far from guides.\n\n\nBasics:\n\n\ncoolname/\n \u2014 the source folder with sub-modules (e.g. sub-module file called \ndosomething.py\n) and containing an \n__init__.py\n file (usually empty, but req'd for installation)\n\n\ncoolname/test/\n \u2014 package folder to hold tests; place files that begin with \"test\" such as \ntest_dosomething.py\n so that programs like \npytest\n can find them and execute.\n\n\nNOTE:  There's an alternative test folder structure where the test-containing folder is named \ntests\n (plural) and placed at the base of the package (with \nsetup.py\n) - check out \nthis\n doc on \npytest\n'ing and folder structures.\n\n\nsetup.py\n \u2014 script to install/test the package and provide metadata (e.g. the long_description for PyPi) - necessary to have a \npip\n installable package.\n\n\nREADME\n \u2014 basic information on the package, how to use, how to install, etc.\n\n\nbin/\n - executables folder (non-py files)\n\n\nOften included:\n\n\ndocs/\n \u2014 documentation folder for the package (as .txt, .md, etc. \u2014 need to indicate this folder in \nsetup.py\n if you want it in distribution)\n\n\nexamples/\n - a folder with some samples and code snippets of package usage\n\n\nscripts/\n \u2014 folder for command line tools like entry points (e.g. with a \nmain()\n)\n\n\nMakefile\n \u2014 a file sometimes included for running the unit tests and more\n\n\nNote:   if there's only one file containing all the source code you can skip creating the \n/coolname\n project folder with the \n__init__.py\n and just place the source code file in the base directory.\n\n\nReferences and places to go for more\n\n\nCheck out the python-packaging guide which walks you through pip-friendly package creation \nhere\n (although it's targeted for python 2). \n\n\nCheck out \nOpen Sourcing a Python Project the Right Way\n for a detailed package dev workflow with tons of sample code and great explanations by Jeff Knupp.\n\n\nJake VanderPlas has a great blog post with videos talking about writing python packages and testing with PyTest among other things \nhere\n.\n\n\nCheck out the \ndo's and don'ts here\n for a quick \"do/don't-do\" synopsis around packaging in python by Jean-Paul Calderone.",
            "title": "Wading In a Tide Pool of Choices, How to Write a Package in Python?"
        },
        {
            "location": "/two-cents-on-python-package-structure/#a-very-quick-example-of-a-barebones-package",
            "text": "To  __init__.py  add this:  from   __future__   import   print_function  def   foo (): \n     print ( 42 )   Now we can use our brand new package in python:  >>> import coolname\n>>> coolname.foo ()   Add the following to  setup.py  (more should go here and we'll see in a bit, but this is barebones right now):  from   setuptools   import   setup  setup ( name = 'coolname' , \n       version = '0.1' , \n       description = 'The coolest package around' , \n       url = 'http://github.com/<your username>/coolname_project' , \n       author = 'Your Name' , \n       author_email = 'name@example.com' , \n       license = 'MIT' , \n       packages = [ 'coolname' ], \n       zip_safe = False )   Adding a  setup.py  with certain info allows us to be able to do an awesome thing:  pip install  our package (here, locally).  In our base package folder just type:  pip install .  Congrats!  You have an awesome, little package (although it doesn't do anything very cool yet - that's up to you!).  Note:  the  setup.py  is a powerful tool and will likely contain much, much more like dependency specifications, more metadata around the package, entry points, testing framework specs, etc.",
            "title": "A very quick example of a barebones package"
        },
        {
            "location": "/two-cents-on-python-package-structure/#read-onthe-dual-purpose-readme",
            "text": "Don't you just love reusability?  If we write our README in reStructuredText format it not only will look good on GitHub, it'll serve as the  long_description  or detailed description of our package on PyPi.  To make sure this happens we need a file called  MANIFEST.in  as well.   MANIFEST.in  also does some more useful things (see Jeff Knupp's article in Resources below).  So, we could, for example, have in our  README.rst  file:  The Coolest Package Ever\n--------\n\nTo use (with caution), simply do::\n    >>> import coolname\n    >>> coolname.foo()  The in our  MANIFEST.in  (this file does other things down the road, but for now we'll use it to include our README):  include README.rst",
            "title": "Read on...the dual-purpose README"
        },
        {
            "location": "/two-cents-on-python-package-structure/#summary-of-the-folderfiles-in-a-package",
            "text": "This is what I've gleaned so far from guides.",
            "title": "Summary of the folder/files in a package"
        },
        {
            "location": "/two-cents-on-python-package-structure/#basics",
            "text": "coolname/  \u2014 the source folder with sub-modules (e.g. sub-module file called  dosomething.py ) and containing an  __init__.py  file (usually empty, but req'd for installation)  coolname/test/  \u2014 package folder to hold tests; place files that begin with \"test\" such as  test_dosomething.py  so that programs like  pytest  can find them and execute.  NOTE:  There's an alternative test folder structure where the test-containing folder is named  tests  (plural) and placed at the base of the package (with  setup.py ) - check out  this  doc on  pytest 'ing and folder structures.  setup.py  \u2014 script to install/test the package and provide metadata (e.g. the long_description for PyPi) - necessary to have a  pip  installable package.  README  \u2014 basic information on the package, how to use, how to install, etc.  bin/  - executables folder (non-py files)",
            "title": "Basics:"
        },
        {
            "location": "/two-cents-on-python-package-structure/#often-included",
            "text": "docs/  \u2014 documentation folder for the package (as .txt, .md, etc. \u2014 need to indicate this folder in  setup.py  if you want it in distribution)  examples/  - a folder with some samples and code snippets of package usage  scripts/  \u2014 folder for command line tools like entry points (e.g. with a  main() )  Makefile  \u2014 a file sometimes included for running the unit tests and more  Note:   if there's only one file containing all the source code you can skip creating the  /coolname  project folder with the  __init__.py  and just place the source code file in the base directory.",
            "title": "Often included:"
        },
        {
            "location": "/two-cents-on-python-package-structure/#references-and-places-to-go-for-more",
            "text": "Check out the python-packaging guide which walks you through pip-friendly package creation  here  (although it's targeted for python 2).   Check out  Open Sourcing a Python Project the Right Way  for a detailed package dev workflow with tons of sample code and great explanations by Jeff Knupp.  Jake VanderPlas has a great blog post with videos talking about writing python packages and testing with PyTest among other things  here .  Check out the  do's and don'ts here  for a quick \"do/don't-do\" synopsis around packaging in python by Jean-Paul Calderone.",
            "title": "References and places to go for more"
        },
        {
            "location": "/ocrbot-gets-attached/",
            "text": "tl;dr\n:  Chatterbots are trending bigtime!  Here, we continue the story of OCRBot, a word recognizing chatbot.  OCRBot's new ability to get image text from attachments is revealed (adding to it's existing ability to take image web links).  So, we can snap a picture of some text and OCRbot finds the words.  This could lead us into even more exciting realms like text to speech or translation.\n\n\nPosted:\n  2017-03-01\n\n\nFor OCR, commonly a k-nearest neighbors classifier is used for character recognition\n\n\n\nFor the first two stories, see \nPart 1\n and \nPart 2\n.\n\n\nWhat's the wow factor?\n\n\nHave you ever just wanted the text extracted from, perhaps a page in a book or a funny comic that's sitting around in an image?  Or maybe it'd be helpful to take a snapshot of a menu item wherein the font is a bit too small or you forgot your reading glasses, however, it's easy to read on your phone as plain text. Now, if you send the OCRBot an image (jpeg, png, gif, bmp are its known formats), on one of its supported conversational platforms like Skype or Slack, you'll get that text you crave.\n\n\n\n\nOCRBot on Skype - using a photo I just took of my favorite coaster sitting on my coffee table currently\n\n\n\nThis is not only fun and useful it could be the precursor to adding text to speech, or TTS, to the bot itself as we've got a Cognitive Servies API for that (Bing Speech).  You can, of course at this point even, pipe this text into one of your TTS apps already on your device.\n\n\n(Re)Introducing OCR\n\n\nOCR, or optical character recognition, is the electronic or mechnanical recognition of text in images and the conversion into machine-encoded representaions.  The text could be handwritten, typed or printed.  The data is an image such as scanned passports or financial data, business cards, postal mail, or any document as an image that someone wishes to digitize.\n\n\nSometimes, OCR uses the human mind as the intelligence algorithms.  An example is the use of reCAPTCHA as a crowdsourcing effort for a two-fold purpose:  verification that the entity logging in somewhere is not a bot and crowdsourcing the recognition of hard to read text for the archiving of say 13 million articles from the NY Times starting with articles from 1851 which was accomplished in 2011 along with all of the books on Google Books.\n\n\nOCR is performed through \npattern recognition\n with components often pulled from AI and computer vision.  The process usually takes the form of:  pre-processing (fixing skew, noise reduction, conversion to black-and-white, and the like) - see figure below; character or word recognition through feature extraction and, often, a k-nearest neighbors classification (see figure below), one of the simplest of ML algorithms; post-processing such as validation through seeing co-occurrences of words (words that usually go together like \"ice cream\" instead of \"ice pizza\").  See the \nWikipedia article on OCR\n for more.\n\n\nIn the Cognitive Services OCR, we have the idea of word detection within bounding boxes, combining pre-processing and several ML algorithms such as the feature extraction mentioned above, another set of algorithms for classification (including convolutional neural networks) and validation through simillar means as above, plus using a vocabulary and other techniques.\n\n\nPre-processing example:\n\n\n\n\nAn example of image pre-processing for character recognition:  fixing skew, binarisation, despekling, and line removal\n\n\n\nk-nearest neighbor example:\n\n\n\n\nClassifying the orange letter as a blue D or green P.  Note, that if k is 3, the orange letter is classified as a blue D, but with a k of 7 it is classified as a green P.  The structure of the data can cause for a tricky problem in k-NN\n\n\n\nNowadays, especially on devices like smart phones, the OCR model used to do this conversion to text is done in the cloud through an API.  This is how the Computer Vision API for OCR under the Cognitive Services umbrealla on Azure gets it done.\n\n\nAnd for those who like a little history, note that OCR was known to be used in devices as early as 1914 with Emanuel Goldberg's invention of a machine that could read characters and convert them into standard \ntelegraph code\n like Morse code (see \nthis Wikipedia\n article for more history).  Skip ahead to today and we have optical word recognition (commonly called OCR) used for typewritten text and others like intelligent character recognition (ICR) for handwritten or cursive text.\n\n\nHow has OCRBot's code changed\n\n\nFrom the original OCRBot\n\n\nOCRBot began with the ability to take a web link of an image with text and give us back the actual text in \nPost 1\n.  Now we've updated OCRBot quite a bit to also accept images as attachments to the conversation.\n\n\nSimply check out the \nserver.js\n at github link \nhere\n for the new changes to OCRBot of which there are quite a few.  Certainly an overhaul of sorts, one of which was the incorporation of promises...\n\n\nA Promise...\n\n\nWhat is a Promise?  Let's start with a promise of a promise that I like (ok, I had to go there) from an article by Marc Harter found \nhere\n.\n\n\n\n\nPromises are a compelling alternative to callbacks when dealing with asynchronous code. [...] Callbacks are the simplest possible mechanism for asynchronous code in JavaScript. Unfortunately, raw callbacks sacrifice the control flow, exception handling, and function semantics familiar from synchronous code. Promises provide a way to get those things back.\n\n\n\n\nSee \nthis\n article if you are unfamiliar with the meaning of asychronous.\n\n\nKey concepts (don't worry if this doesn't quite make sense yet) of a promise are as follows (paraphrased from info in Harter's article).\n\n\n\n\nA \npromise\n (one way to think about them), is a value representing an asynchronous operation that is only fufilled once, either being resolved or rejected (we'll see in code soon).\n\n\nthen\n, a reserved word for promises, allows anyone with access to the promise to be able to consume it \nregardless of if the asynchronous operation is done or not\n.  One way to think about them is as a function which unwraps the asynchronous operation's result from the promise.  \nthen\n returns a promise.\n\n\ncatch\n is often used after a \nthen\n, series of \nthen\ns, or some nested \nthen\ns to handle errors both implicitly and explicitly if the ned arises.\n\n\nWe can still use \nreturn\n to model syncrhonous functions by returning a promise or any other value that and then signaling the next \nthen\n, possibly giving this value to the next \nonFufilled\n.\n\n\nonFufilled\n and \nonRejected\n are the callbacks and handlers that \nthen\n handles to signal what to do with the results of a promise.\n\n\n\n\nHere's a simple representation in code of the logic changes a promise provides.\n\n\nvar\n \npromise\n \n=\n \nreadFile\n()\n\n\npromise\n.\nthen\n(\nconsole\n.\nlog\n,\n \nconsole\n.\nerror\n)\n\n\n\n\n\nIn this example, \nreadFile\n does, in fact, need to return a promise.  This object can now be used with any \nthen\n that has access to the promise.  You'll notice that the \nthen\n has two bits to it.  If the promise is fufilled in \nreadFile\n the first (\nconsole.log\n) chunk of code is called, and if rejected, the second chunk is called (\nconsole.error\n).\n\n\nWe can create raw promises in the following way.  In this example, we are using the \nfs\n library's \nreadFile\n method, using \nreject\n to pass on the result of the promise in the case of an error, and \nresolve\n if it is fufilled or simply not rejected, wrapping all of this up in a function which returns the promise to be used by a \nthen\n.  Then, the next \nthen\n that consumes this promise-returning function \"unwraps\" that logic.\n\n\nfunction\n \nreadFileAsync\n \n(\nfile\n,\n \nencoding\n)\n \n{\n\n  \nreturn\n \nnew\n \nPromise\n(\nfunction\n \n(\nresolve\n,\n \nreject\n)\n \n{\n\n    \nfs\n.\nreadFile\n(\nfile\n,\n \nencoding\n,\n \nfunction\n \n(\nerr\n,\n \ndata\n)\n \n{\n\n      \nif\n \n(\nerr\n)\n \nreturn\n \nreject\n(\nerr\n)\n \n// rejects the promise with `err` as the reason\n\n      \nresolve\n(\ndata\n)\n               \n// fulfills the promise with `data` as the value\n\n    \n})\n\n  \n})\n\n\n}\n\n\nreadFileAsync\n(\n'myfile.txt'\n).\nthen\n(\nconsole\n.\nlog\n,\n \nconsole\n.\nerror\n)\n\n\n\n\n\nIn a similar way, the OCRBot's code now includes promises and this looks like the raw promise above with the clever use of a \ncatch\n to catch any unhandled errors and give some information back to the app and user.\n\n\nvar\n \nfileDownload\n \n=\n \nnew\n \nPromise\n(\n\n                \nfunction\n(\nresolve\n,\n \nreject\n)\n \n{\n\n                    \nvar\n \ncheck\n \n=\n \ncheckRequiresToken\n(\nmsg\n);\n\n                    \nif\n  \n(\ncheck\n==\ntrue\n)\n \n{\n\n                        \nresolve\n(\nrequestWithToken\n(\nattachment\n.\ncontentUrl\n));\n\n                    \n}\n \nelse\n \n{\n\n                        \nresolve\n(\nrequest_promise\n(\nattachment\n.\ncontentUrl\n));\n\n                    \n}\n\n                \n}\n\n            \n);\n\n\n            \nfileDownload\n.\nthen\n(\n\n                \nfunction\n \n(\nresponse\n)\n \n{\n\n\n                \nreadImageText\n(\nresponse\n,\n \nattachment\n.\ncontentType\n,\n \nfunction\n \n(\nerror\n,\n \nresponse\n,\n \nbody\n)\n \n{\n\n                    \nsession\n.\nsend\n(\nextractText\n(\nbody\n));\n\n                \n});\n\n\n                \n}).\ncatch\n(\nfunction\n \n(\nerr\n,\n \nreply\n)\n \n{\n\n                    \nconsole\n.\nlog\n(\n'Error with attachment: '\n,\n \n{\n \n                        \nstatusCode\n:\n \nerr\n.\nstatusCode\n,\n \n                        \nmessage\n:\n \nerr\n \n});\n\n                        \nsession\n.\nsend\n(\n\"Error with attachment or reading image with %s\"\n,\n \nerr\n);\n\n            \n});\n\n\n\n\n\nPromises are quite amazing, leading to resolving many issues around Node.js code such as too many nested callbacks (aka \"callback hell\") and clean error handling.  I hope you got a bit of that enthusiasm from reading through this section.\n\n\nThe end of this chapter and OCRBot's next adventure\n\n\nI'm actually not sure what will be in store for OCRBot next.  There are so many fantastic \"smarts\" we could add or clever functionality.  It'll have to wait and be revealed when OCRBot returns to this blog.",
            "title": "OCRBot Gets Attached"
        },
        {
            "location": "/ocrbot-gets-attached/#whats-the-wow-factor",
            "text": "Have you ever just wanted the text extracted from, perhaps a page in a book or a funny comic that's sitting around in an image?  Or maybe it'd be helpful to take a snapshot of a menu item wherein the font is a bit too small or you forgot your reading glasses, however, it's easy to read on your phone as plain text. Now, if you send the OCRBot an image (jpeg, png, gif, bmp are its known formats), on one of its supported conversational platforms like Skype or Slack, you'll get that text you crave.   OCRBot on Skype - using a photo I just took of my favorite coaster sitting on my coffee table currently  This is not only fun and useful it could be the precursor to adding text to speech, or TTS, to the bot itself as we've got a Cognitive Servies API for that (Bing Speech).  You can, of course at this point even, pipe this text into one of your TTS apps already on your device.",
            "title": "What's the wow factor?"
        },
        {
            "location": "/ocrbot-gets-attached/#reintroducing-ocr",
            "text": "OCR, or optical character recognition, is the electronic or mechnanical recognition of text in images and the conversion into machine-encoded representaions.  The text could be handwritten, typed or printed.  The data is an image such as scanned passports or financial data, business cards, postal mail, or any document as an image that someone wishes to digitize.  Sometimes, OCR uses the human mind as the intelligence algorithms.  An example is the use of reCAPTCHA as a crowdsourcing effort for a two-fold purpose:  verification that the entity logging in somewhere is not a bot and crowdsourcing the recognition of hard to read text for the archiving of say 13 million articles from the NY Times starting with articles from 1851 which was accomplished in 2011 along with all of the books on Google Books.  OCR is performed through  pattern recognition  with components often pulled from AI and computer vision.  The process usually takes the form of:  pre-processing (fixing skew, noise reduction, conversion to black-and-white, and the like) - see figure below; character or word recognition through feature extraction and, often, a k-nearest neighbors classification (see figure below), one of the simplest of ML algorithms; post-processing such as validation through seeing co-occurrences of words (words that usually go together like \"ice cream\" instead of \"ice pizza\").  See the  Wikipedia article on OCR  for more.  In the Cognitive Services OCR, we have the idea of word detection within bounding boxes, combining pre-processing and several ML algorithms such as the feature extraction mentioned above, another set of algorithms for classification (including convolutional neural networks) and validation through simillar means as above, plus using a vocabulary and other techniques.  Pre-processing example:   An example of image pre-processing for character recognition:  fixing skew, binarisation, despekling, and line removal  k-nearest neighbor example:   Classifying the orange letter as a blue D or green P.  Note, that if k is 3, the orange letter is classified as a blue D, but with a k of 7 it is classified as a green P.  The structure of the data can cause for a tricky problem in k-NN  Nowadays, especially on devices like smart phones, the OCR model used to do this conversion to text is done in the cloud through an API.  This is how the Computer Vision API for OCR under the Cognitive Services umbrealla on Azure gets it done.  And for those who like a little history, note that OCR was known to be used in devices as early as 1914 with Emanuel Goldberg's invention of a machine that could read characters and convert them into standard  telegraph code  like Morse code (see  this Wikipedia  article for more history).  Skip ahead to today and we have optical word recognition (commonly called OCR) used for typewritten text and others like intelligent character recognition (ICR) for handwritten or cursive text.",
            "title": "(Re)Introducing OCR"
        },
        {
            "location": "/ocrbot-gets-attached/#how-has-ocrbots-code-changed",
            "text": "",
            "title": "How has OCRBot's code changed"
        },
        {
            "location": "/ocrbot-gets-attached/#from-the-original-ocrbot",
            "text": "OCRBot began with the ability to take a web link of an image with text and give us back the actual text in  Post 1 .  Now we've updated OCRBot quite a bit to also accept images as attachments to the conversation.  Simply check out the  server.js  at github link  here  for the new changes to OCRBot of which there are quite a few.  Certainly an overhaul of sorts, one of which was the incorporation of promises...",
            "title": "From the original OCRBot"
        },
        {
            "location": "/ocrbot-gets-attached/#a-promise",
            "text": "What is a Promise?  Let's start with a promise of a promise that I like (ok, I had to go there) from an article by Marc Harter found  here .   Promises are a compelling alternative to callbacks when dealing with asynchronous code. [...] Callbacks are the simplest possible mechanism for asynchronous code in JavaScript. Unfortunately, raw callbacks sacrifice the control flow, exception handling, and function semantics familiar from synchronous code. Promises provide a way to get those things back.   See  this  article if you are unfamiliar with the meaning of asychronous.  Key concepts (don't worry if this doesn't quite make sense yet) of a promise are as follows (paraphrased from info in Harter's article).   A  promise  (one way to think about them), is a value representing an asynchronous operation that is only fufilled once, either being resolved or rejected (we'll see in code soon).  then , a reserved word for promises, allows anyone with access to the promise to be able to consume it  regardless of if the asynchronous operation is done or not .  One way to think about them is as a function which unwraps the asynchronous operation's result from the promise.   then  returns a promise.  catch  is often used after a  then , series of  then s, or some nested  then s to handle errors both implicitly and explicitly if the ned arises.  We can still use  return  to model syncrhonous functions by returning a promise or any other value that and then signaling the next  then , possibly giving this value to the next  onFufilled .  onFufilled  and  onRejected  are the callbacks and handlers that  then  handles to signal what to do with the results of a promise.   Here's a simple representation in code of the logic changes a promise provides.  var   promise   =   readFile ()  promise . then ( console . log ,   console . error )   In this example,  readFile  does, in fact, need to return a promise.  This object can now be used with any  then  that has access to the promise.  You'll notice that the  then  has two bits to it.  If the promise is fufilled in  readFile  the first ( console.log ) chunk of code is called, and if rejected, the second chunk is called ( console.error ).  We can create raw promises in the following way.  In this example, we are using the  fs  library's  readFile  method, using  reject  to pass on the result of the promise in the case of an error, and  resolve  if it is fufilled or simply not rejected, wrapping all of this up in a function which returns the promise to be used by a  then .  Then, the next  then  that consumes this promise-returning function \"unwraps\" that logic.  function   readFileAsync   ( file ,   encoding )   { \n   return   new   Promise ( function   ( resolve ,   reject )   { \n     fs . readFile ( file ,   encoding ,   function   ( err ,   data )   { \n       if   ( err )   return   reject ( err )   // rejects the promise with `err` as the reason \n       resolve ( data )                 // fulfills the promise with `data` as the value \n     }) \n   })  }  readFileAsync ( 'myfile.txt' ). then ( console . log ,   console . error )   In a similar way, the OCRBot's code now includes promises and this looks like the raw promise above with the clever use of a  catch  to catch any unhandled errors and give some information back to the app and user.  var   fileDownload   =   new   Promise ( \n                 function ( resolve ,   reject )   { \n                     var   check   =   checkRequiresToken ( msg ); \n                     if    ( check == true )   { \n                         resolve ( requestWithToken ( attachment . contentUrl )); \n                     }   else   { \n                         resolve ( request_promise ( attachment . contentUrl )); \n                     } \n                 } \n             ); \n\n             fileDownload . then ( \n                 function   ( response )   { \n\n                 readImageText ( response ,   attachment . contentType ,   function   ( error ,   response ,   body )   { \n                     session . send ( extractText ( body )); \n                 }); \n\n                 }). catch ( function   ( err ,   reply )   { \n                     console . log ( 'Error with attachment: ' ,   {  \n                         statusCode :   err . statusCode ,  \n                         message :   err   }); \n                         session . send ( \"Error with attachment or reading image with %s\" ,   err ); \n             });   Promises are quite amazing, leading to resolving many issues around Node.js code such as too many nested callbacks (aka \"callback hell\") and clean error handling.  I hope you got a bit of that enthusiasm from reading through this section.",
            "title": "A Promise..."
        },
        {
            "location": "/ocrbot-gets-attached/#the-end-of-this-chapter-and-ocrbots-next-adventure",
            "text": "I'm actually not sure what will be in store for OCRBot next.  There are so many fantastic \"smarts\" we could add or clever functionality.  It'll have to wait and be revealed when OCRBot returns to this blog.",
            "title": "The end of this chapter and OCRBot's next adventure"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/",
            "text": "tl;dr\n:  Once you learn how to use one kind of notebook system, the knowledge will transfer easily to another.  Here, we're discussing two, Jupyter and Beaker.  Because Jupyter is much more mature of a project, it'd probably be the best place to start.  But for those with that extreme sense of adventure and/or use Python 2 a lot with other languages, give the Beaker notebooks a chance.  Loads of potential for data scientists there.\n\n\nPosted:\n  2017-02-19\n\n\nWhat's a notebook?\n\n\nHave you ever taught programming and wished to have the class notes, sample code and exercises with instructions all in one place?  Have you ever heavily commented your code and wished it was more readable?  Have you used R Markdown and wished to run individual code chunks with only a button or keyboard shortcut?  Have you ever wished to use multiple programming languages in the same place, same document?\n\n\nSo, the story begins with my wish for a better way.  When I discovered notebooks, at first, I felt strange programming in a browser until I discovered I could annotate the code with pleasant, easy-to-read text and for some reason that opened up a whole world.  I began documenting my research work more, creating clear and rich teaching aids, and enhancing my work to share with others in a reproducible way or at least with clear instructions and notes in nice looking text rather than sometimes hard-to-read comments within the code (which I still do of course).  It was the annotations that made it worth my time to learn.\n\n\nThere are several notebook systems out there and they all seem to behave, at their core, the same way in that I can run interactive code cells and document my work in pleasant-to-read formats.  They do vary in their use cases, such as RStudio's notebook being more geared towards the R programmer (although it has extension packages for other languages now) or the Beaker notebooks for combining multiple languages into a workflow.  A sample of notebook \"providers\" is as follows.\n\n\n\n\nBeaker\n\n\nZeppelin\n\n\nSpark\n\n\nJupyter\n\n\nJupyterlab\n (preview)\n\n\nRStudio\n\n\nand more\n\n\n\n\nIntroducing our players\n\n\nBeaker\n, a polyglot notebook system, is based on IPython (amongst other things) and \nJupyter\n, supporting over 40 programming languages, is based on IPython (amongst other things).  They both allow multiple languages from within the same notebook and both run on top of Python.  I found I was able to install either one without the command line so they seemed pretty easy to get going on (Jupyter did require one command in the terminal to start which was a simple task).\n\n\nThey are both open source projects and being built on IPython have similar notebook interfaces so it'd be easy to switch over from one to another once you get the hang of notebooks.\n\n\nDifferences and distinguishing factors discussed below.\n\n\nJupyter:  customizable and sometimes magic\n\n\n\n\nA Python 3 flavored Jupyter notebook with a \"grade3\" theme (theme from Kyle Dunovan's jupyter-themes repo)\n\n\nThe Jupyter project is much more mature than the Beaker project and thus has a reliable and almost totally bug-free experience (nothing is completely bug-free).  It's pretty much the classic notebook system, but gives the us the ability to use it for reproducible research, publish papers, do presentations with live code, create blogs (not this one, although it's in markdown at least), and the list goes on.  It's a mature project with many add-ons and features available.\n\n\nReturning to basecamp, Jupyter notebook setups can be simple and basic, one language supported and the basic theme, or much more complex, supporting several languages chosen from a drop-down menu and having extensions to check spelling and perhaps a custom theme to pretty it up.  Out of the box, they simply work with the default Python.  It's a very transparent system.  What you add on is done by you, but you must take care of what that add-on requires.  Jupyter notebooks are meant to be simple, useful and clean (I've seen and made many of messes so I aim for this).\n\n\nAn Anaconda install gives us Jupyter notebooks automatically.  Whichever is the default Python, becomes the default Python version for the notebook (basically whatever is first in our PATH if we have 2 and 3).  We could also install with \npip\n, Python's package manager.\n\n\nJupyter, when using the python kernel, can incorporate \"magics\" or other languages within the same notebook (and sometimes passing variables back and forth like with R kernel and rpy2 or javascript).  Some cell magics are listed here (the \"%%\" is literally the syntax we use in a notebook cell to designate):  \n\n\n\n\n%%fortran\n\n\n%%cython\n\n\n%%javascript\n\n\n%%html\n\n\n%%bash\n\n\n%%latex\n\n\n%%perl\n\n\n%%python2\n\n\n%%python3\n\n\n%%ruby\n\n\n%%R\n\n\nothers (incl. incubator projects like \nsparkmagic\n, which created magics within it, in the context of working with spark clusters)\n\n\n\n\nThese languages, of course, must be on the system hosting the notebook.  In addition, the Jupyter project reports over 40 languages supported, but this does not mean they all have magics and can be run from an IPython notebook (IPython, here, referring to the Python kernel, but it can also refer to a previous Python notebook project).  Also, custom kernels for languages not supported can be made according to the Jupyter docs.\n\n\nOne customization I really love is \nnbextensions\n (more \nhere\n) which adds functionality to a Jupyter notebooks such as a table of contents, section numbering, highlighting, and spellcheck to name a few.  Personally, I found the TOC and spellcheck very, very useful as I get lost easily and spell quite horribly.\n\n\nAnother customization is around adding a theme, but more on that below.\n\n\nA really nifty meta-feature is that GitHub renders static versions of IPython/Jupyter notebooks (.ipynb files which are just JSON) which makes viewing your work and the work of others very easy from GitHub.\n\n\nYou can find the Jupyter project on GitHub at \nhttps://github.com/jupyter/notebook\n.\n\n\nBeaker:  a true polyglot\n\n\n\n\nPython 2 and JavaScript with D3 sharing variables (entire code sample is in the D3 Demo notebook that comes with Beaker)\n\n\n\nA Beaker notebook is different from a Jupyter notebook in that it can easily pass data from one cell to the next even if the code in each cell is in a different programming language.  This is the \nbig\n selling point of Beaker notebooks.  Literally, we can share one variable from Python to Javascript, for example, by just prefacing it with \nbeaker.\n.  Woah.  This opens up a realm of possibilities.\n\n\nBeaker notebooks give us more default functionality and ease-of-use than a Jupyter notebook at the expense of being less transparent.  If all you need is Python 2, they are super easy and very user-friendly.  Also, Beaker starts up with tons of sample code, called Demos, at your fingertips for most if not all of the supported languages.\n\n\nBeaker, so far, out-of-the-box, supports* 17 languages:\n\n\n\n\nClojure\n\n\nC++\n\n\nPython2 (called IPython by Beaker)\n\n\nPython3\n\n\nR\n\n\nSQL\n\n\nJavaScript\n\n\nScala\n\n\nNode\n\n\nTorch\n\n\nJulia\n\n\nJava\n\n\nKdb\n\n\nGroovy\n\n\nHTML\n\n\nRuby\n\n\nTeX\n\n\n\n\n*\nYou still need to have the backend interpreter or compiler (just like in Jupyter) and certain plugins in most cases to connect it up to Beaker.\n\n\nOn the origins of Beaker, in their own words... \n\n\n\n\nBeaker is built on many fantastic open source projects including Angular, Bootstrap, CometD, Gradle, Guice, IPython, Jackson, Jetty, Nginx, Rserve, and others for which we are very grateful.\n\n\n\n\nBeaker, too, is open source as a \"base\" or something to run locally, host oneself, or use with Docker.  You can check it out on their GiHhub repo at:  \nhttps://github.com/twosigma/beaker-notebook\n.\n\n\nBeaker has better notebooks management features (such as listing your open notebooks with time stamps).  The UI looks a bit nicer as well.  \n\n\n\n\nAside:  Those who like to see their files listed, however, should try Jupyterlab which feels more like RStudio than a notebook system.  It has nice management features as well, but more around transparency into the file system and has the ability to open different file formats plus a classic interpreter.  It's out of the scope of this post for sure.\n\n\n\n\nAnd some may not see this tiny little note in a screenshot of a guide for what you can put in the text parts right on their GitHub readme, but they totally mention Donald Knuth, one of my favorite people ever.  +1.\n\n\nInstalling it\n\n\nNeither Beaker, nor Jupyter, require the command line for installation.  An install of Anaconda for Python includes Jupyter notebooks.  To run it however, one will need to type \njupyter notebook\n from the command line, but that's really it (you can also install Jupyter from the command line with \npip\n). \n\n\nThe tricky part sometimes for Jupyter is getting other kernels (support for other languages) installed.  But my other \nde facto\n language is R and I simply used the conda compatible \nR-Essentials\n, which gives me the R kernel option (Yay!!) (and by far the easiest way to get the R kernel working that I've found - see \nthis\n blog for more on R-Essentials).  I gave up getting R to work in Beaker after toying around for an hour or so (granted that wasn't a long time and smarter folks could probably get it working) running up against an undocumented R package called Rserve, a dependency for R in Beaker.\n\n\nIt appears Beaker by default expects, as it \nsays here\n, a Python 2 conda install (which is weird I thought due to Python 2 becoming a legacy language soon).  So, when I tried it with my Python-3-only conda install, I had bad luck running an IPython cell, although Python 3 cells worked.  I did solve the IPython cell issue according to some pretty easy-to-follow advice on their wiki about specifying the IPython path and adding a Python path pointing to a Python 2 install (in a config file called \nbeaker.pref.json\n).  Beaker's wiki is, in general, very helpful I've found thus far.\n\n\nThemes:  design can win people over\n\n\n\n\nA Jupyter notebook with the \nmm_dark_theme\n (theme by Milos Miljkovic and found on GitHub)\n\n\nOut of the box, we get a couple of themes for our notebooks with Beaker,  Default and Ambiance.  With Jupyter, we can add any theme we'd like or like to create in a few ways, but my favorite and simplest is just adding a \ncustom.js\n and \ncustom.css\n  to a folder called \n~/.jupyter/custom/\n (create one if it's not there).  The \n~\n refers to the user's base directory, often used in Unix systems. \n\n\nFor the Jupyter notebook above, I used a dark theme which is found \nhere\n, but you can find them all over place on GitHub.  The installs will vary so I stick with the \ncustom.js\n from the \nmm_dark_theme\n project/repository and just switch out the css files (probably not the best practice mind you, but I always give credit to the creators of the css, or stylesheets).\n\n\nYes, the Jupyter method to include a theme is one or two steps more complicated, but it's truly custom and themes can be important for various reasons, like emphasizing the graphs and text to non-technical folk in a more pleasant background color than white and nicer fonts.  I do have to admit I was pleasantly surprised by having a theme choice in Beaker and then how easy it was to switch them.\n\n\nCompetition or no?\n\n\nSo, is it a competition?  I'd say not so much given we don't really have an apples to apples situation.  While Beaker may be trying to fill the gaps or make its niche, I'd say it's in fact creating a whole new experience for data scientists which could be extraordinary if they'd only make some small adjustments (such as make the deployment Python version agnostic).  I hope these two projects, and the others, continue to complement each other and grow even better.\n\n\nQuestions I have, but didn't answer here\n\n\n\n\nDo I really need to pass variables from one language to another or are magics in Jupyter sufficient for incorporating the same languages into one notebook?\n\n\nWhy would I choose Jupyterlab over Jupyter notebooks?  Is it a step forward or simply a divergence?\n\n\nRStudio does similar things these days and with XRPython we can embed python in a notebook-eqsue environment \u2014 and might I add tidyverse.  Why not use that?\n\n\nWhich one is the best for Spark?\n\n\n\n\nThanks for reading.",
            "title": "The Notebook Superhero -- Is It Always a Contest?"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#whats-a-notebook",
            "text": "Have you ever taught programming and wished to have the class notes, sample code and exercises with instructions all in one place?  Have you ever heavily commented your code and wished it was more readable?  Have you used R Markdown and wished to run individual code chunks with only a button or keyboard shortcut?  Have you ever wished to use multiple programming languages in the same place, same document?  So, the story begins with my wish for a better way.  When I discovered notebooks, at first, I felt strange programming in a browser until I discovered I could annotate the code with pleasant, easy-to-read text and for some reason that opened up a whole world.  I began documenting my research work more, creating clear and rich teaching aids, and enhancing my work to share with others in a reproducible way or at least with clear instructions and notes in nice looking text rather than sometimes hard-to-read comments within the code (which I still do of course).  It was the annotations that made it worth my time to learn.  There are several notebook systems out there and they all seem to behave, at their core, the same way in that I can run interactive code cells and document my work in pleasant-to-read formats.  They do vary in their use cases, such as RStudio's notebook being more geared towards the R programmer (although it has extension packages for other languages now) or the Beaker notebooks for combining multiple languages into a workflow.  A sample of notebook \"providers\" is as follows.   Beaker  Zeppelin  Spark  Jupyter  Jupyterlab  (preview)  RStudio  and more",
            "title": "What's a notebook?"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#introducing-our-players",
            "text": "Beaker , a polyglot notebook system, is based on IPython (amongst other things) and  Jupyter , supporting over 40 programming languages, is based on IPython (amongst other things).  They both allow multiple languages from within the same notebook and both run on top of Python.  I found I was able to install either one without the command line so they seemed pretty easy to get going on (Jupyter did require one command in the terminal to start which was a simple task).  They are both open source projects and being built on IPython have similar notebook interfaces so it'd be easy to switch over from one to another once you get the hang of notebooks.  Differences and distinguishing factors discussed below.",
            "title": "Introducing our players"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#jupyter-customizable-and-sometimes-magic",
            "text": "A Python 3 flavored Jupyter notebook with a \"grade3\" theme (theme from Kyle Dunovan's jupyter-themes repo)  The Jupyter project is much more mature than the Beaker project and thus has a reliable and almost totally bug-free experience (nothing is completely bug-free).  It's pretty much the classic notebook system, but gives the us the ability to use it for reproducible research, publish papers, do presentations with live code, create blogs (not this one, although it's in markdown at least), and the list goes on.  It's a mature project with many add-ons and features available.  Returning to basecamp, Jupyter notebook setups can be simple and basic, one language supported and the basic theme, or much more complex, supporting several languages chosen from a drop-down menu and having extensions to check spelling and perhaps a custom theme to pretty it up.  Out of the box, they simply work with the default Python.  It's a very transparent system.  What you add on is done by you, but you must take care of what that add-on requires.  Jupyter notebooks are meant to be simple, useful and clean (I've seen and made many of messes so I aim for this).  An Anaconda install gives us Jupyter notebooks automatically.  Whichever is the default Python, becomes the default Python version for the notebook (basically whatever is first in our PATH if we have 2 and 3).  We could also install with  pip , Python's package manager.  Jupyter, when using the python kernel, can incorporate \"magics\" or other languages within the same notebook (and sometimes passing variables back and forth like with R kernel and rpy2 or javascript).  Some cell magics are listed here (the \"%%\" is literally the syntax we use in a notebook cell to designate):     %%fortran  %%cython  %%javascript  %%html  %%bash  %%latex  %%perl  %%python2  %%python3  %%ruby  %%R  others (incl. incubator projects like  sparkmagic , which created magics within it, in the context of working with spark clusters)   These languages, of course, must be on the system hosting the notebook.  In addition, the Jupyter project reports over 40 languages supported, but this does not mean they all have magics and can be run from an IPython notebook (IPython, here, referring to the Python kernel, but it can also refer to a previous Python notebook project).  Also, custom kernels for languages not supported can be made according to the Jupyter docs.  One customization I really love is  nbextensions  (more  here ) which adds functionality to a Jupyter notebooks such as a table of contents, section numbering, highlighting, and spellcheck to name a few.  Personally, I found the TOC and spellcheck very, very useful as I get lost easily and spell quite horribly.  Another customization is around adding a theme, but more on that below.  A really nifty meta-feature is that GitHub renders static versions of IPython/Jupyter notebooks (.ipynb files which are just JSON) which makes viewing your work and the work of others very easy from GitHub.  You can find the Jupyter project on GitHub at  https://github.com/jupyter/notebook .",
            "title": "Jupyter:  customizable and sometimes magic"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#beaker-a-true-polyglot",
            "text": "Python 2 and JavaScript with D3 sharing variables (entire code sample is in the D3 Demo notebook that comes with Beaker)  A Beaker notebook is different from a Jupyter notebook in that it can easily pass data from one cell to the next even if the code in each cell is in a different programming language.  This is the  big  selling point of Beaker notebooks.  Literally, we can share one variable from Python to Javascript, for example, by just prefacing it with  beaker. .  Woah.  This opens up a realm of possibilities.  Beaker notebooks give us more default functionality and ease-of-use than a Jupyter notebook at the expense of being less transparent.  If all you need is Python 2, they are super easy and very user-friendly.  Also, Beaker starts up with tons of sample code, called Demos, at your fingertips for most if not all of the supported languages.  Beaker, so far, out-of-the-box, supports* 17 languages:   Clojure  C++  Python2 (called IPython by Beaker)  Python3  R  SQL  JavaScript  Scala  Node  Torch  Julia  Java  Kdb  Groovy  HTML  Ruby  TeX   * You still need to have the backend interpreter or compiler (just like in Jupyter) and certain plugins in most cases to connect it up to Beaker.  On the origins of Beaker, in their own words...    Beaker is built on many fantastic open source projects including Angular, Bootstrap, CometD, Gradle, Guice, IPython, Jackson, Jetty, Nginx, Rserve, and others for which we are very grateful.   Beaker, too, is open source as a \"base\" or something to run locally, host oneself, or use with Docker.  You can check it out on their GiHhub repo at:   https://github.com/twosigma/beaker-notebook .  Beaker has better notebooks management features (such as listing your open notebooks with time stamps).  The UI looks a bit nicer as well.     Aside:  Those who like to see their files listed, however, should try Jupyterlab which feels more like RStudio than a notebook system.  It has nice management features as well, but more around transparency into the file system and has the ability to open different file formats plus a classic interpreter.  It's out of the scope of this post for sure.   And some may not see this tiny little note in a screenshot of a guide for what you can put in the text parts right on their GitHub readme, but they totally mention Donald Knuth, one of my favorite people ever.  +1.",
            "title": "Beaker:  a true polyglot"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#installing-it",
            "text": "Neither Beaker, nor Jupyter, require the command line for installation.  An install of Anaconda for Python includes Jupyter notebooks.  To run it however, one will need to type  jupyter notebook  from the command line, but that's really it (you can also install Jupyter from the command line with  pip ).   The tricky part sometimes for Jupyter is getting other kernels (support for other languages) installed.  But my other  de facto  language is R and I simply used the conda compatible  R-Essentials , which gives me the R kernel option (Yay!!) (and by far the easiest way to get the R kernel working that I've found - see  this  blog for more on R-Essentials).  I gave up getting R to work in Beaker after toying around for an hour or so (granted that wasn't a long time and smarter folks could probably get it working) running up against an undocumented R package called Rserve, a dependency for R in Beaker.  It appears Beaker by default expects, as it  says here , a Python 2 conda install (which is weird I thought due to Python 2 becoming a legacy language soon).  So, when I tried it with my Python-3-only conda install, I had bad luck running an IPython cell, although Python 3 cells worked.  I did solve the IPython cell issue according to some pretty easy-to-follow advice on their wiki about specifying the IPython path and adding a Python path pointing to a Python 2 install (in a config file called  beaker.pref.json ).  Beaker's wiki is, in general, very helpful I've found thus far.",
            "title": "Installing it"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#themes-design-can-win-people-over",
            "text": "A Jupyter notebook with the  mm_dark_theme  (theme by Milos Miljkovic and found on GitHub)  Out of the box, we get a couple of themes for our notebooks with Beaker,  Default and Ambiance.  With Jupyter, we can add any theme we'd like or like to create in a few ways, but my favorite and simplest is just adding a  custom.js  and  custom.css   to a folder called  ~/.jupyter/custom/  (create one if it's not there).  The  ~  refers to the user's base directory, often used in Unix systems.   For the Jupyter notebook above, I used a dark theme which is found  here , but you can find them all over place on GitHub.  The installs will vary so I stick with the  custom.js  from the  mm_dark_theme  project/repository and just switch out the css files (probably not the best practice mind you, but I always give credit to the creators of the css, or stylesheets).  Yes, the Jupyter method to include a theme is one or two steps more complicated, but it's truly custom and themes can be important for various reasons, like emphasizing the graphs and text to non-technical folk in a more pleasant background color than white and nicer fonts.  I do have to admit I was pleasantly surprised by having a theme choice in Beaker and then how easy it was to switch them.",
            "title": "Themes:  design can win people over"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#competition-or-no",
            "text": "So, is it a competition?  I'd say not so much given we don't really have an apples to apples situation.  While Beaker may be trying to fill the gaps or make its niche, I'd say it's in fact creating a whole new experience for data scientists which could be extraordinary if they'd only make some small adjustments (such as make the deployment Python version agnostic).  I hope these two projects, and the others, continue to complement each other and grow even better.",
            "title": "Competition or no?"
        },
        {
            "location": "/jupyter-and-beaker-make-a-case/#questions-i-have-but-didnt-answer-here",
            "text": "Do I really need to pass variables from one language to another or are magics in Jupyter sufficient for incorporating the same languages into one notebook?  Why would I choose Jupyterlab over Jupyter notebooks?  Is it a step forward or simply a divergence?  RStudio does similar things these days and with XRPython we can embed python in a notebook-eqsue environment \u2014 and might I add tidyverse.  Why not use that?  Which one is the best for Spark?   Thanks for reading.",
            "title": "Questions I have, but didn't answer here"
        },
        {
            "location": "/javascript-and-python-have-a-party/",
            "text": "Posted:\n  2017-02-18\n\n\nA little exchange\n\n\nPython\n:  \"I want your silly game to have a starting value of 100 for life points for all players.\"\n\n\nJavascript\n:  \"I read you loud and clear.  Let's take that starting life and play this silly game.  Alfred and Wallace are on the same side battling orcs.  Alfred decides to give life to Wallace because Wallace appears to be about to bravely charge onto the battle field and may need it.  Done.  That's all the game can do right now because I just started making it.\"\n\n\nPython\n:  \"Let's see...oh wow.  Your game code actually worked.  Alfred now has 99 life points and Wallace has 101.  Good job us.\"\n\n\nI went on a hunt to find the cleanest and most succinct way to pass a variable from Python to Javascript and then back to Python in a Jupyter notebook.  I wanted a proof of principle upon which I could later base my D3 graphics using Python data as \npandas\n dataframes.\n\n\nSo, this silly example can be translated into code as follows.\n\n\nFirst, things are actually easy for us because we are in a Jupyter notebook living in a browser, utilizing all sorts of widgets and HTML elements already.  We can tap into that (and actually the DOM) to get some of the functionality we require.\n\n\nSo, in Python we can use the IPython.display module with the HTML function allowing us to embed an element for later use.\n\n\n# Python\n\n\nfrom\n \nIPython.display\n \nimport\n \nHTML\n\n\n\npystartlife\n \n=\n \nstr\n(\n100\n)\n\n\nHTML\n(\n\"<div id='textid'>\"\n \n+\n \npystartlife\n \n+\n \n\"</div>\"\n)\n\n\n\n\n\nWe've created a Python variable, \npystartlife\n, and embedded it as a \ndiv\n element, literally just using raw HTML.  Now we use the Javascript magics (\n%%javascript\n) to create a Javascript coding environment for the next cell.  (BTW there are magics for many more languages - very cool.)  \n\n\nIn the Javascript, now, we grab the \ndiv\n element with the Python variable from the document or webpage and play our game.  We also write the game.  (This game is based on a \nNode.js tutorial\n by thenewboston on YouTube - thank you Bucky!).\n\n\n%%\njavascript\n\n\n\n// Get the python variable from the DOM\n\n\nvar\n \nstartlife\n \n=\n \ndocument\n.\ngetElementById\n(\n'textid'\n).\ninnerHTML\n;\n\n\n\n// Define a User class with a method\n\n\nfunction\n \nUser\n()\n \n{\n\n    \nthis\n.\nname\n \n=\n \n''\n;\n\n    \nthis\n.\nlife\n \n=\n \nNumber\n(\nstartlife\n);\n\n    \nthis\n.\ngiveLife\n \n=\n \nfunction\n \ngiveLife\n(\ntargetPlayer\n)\n \n{\n\n        \ntargetPlayer\n.\nlife\n \n+=\n \n1\n;\n\n        \nthis\n.\nlife\n \n-=\n \n1\n;\n\n    \n}\n\n\n}\n\n\n\n// Use class\n\n\nvar\n \nAlfred\n \n=\n \nnew\n \nUser\n();\n\n\nvar\n \nWallace\n \n=\n \nnew\n \nUser\n();\n\n\n\n// Names were blank so give them name values\n\n\nAlfred\n.\nname\n \n=\n \n'Alfred'\n;\n\n\nWallace\n.\nname\n \n=\n \n'Wallace'\n;\n\n\n\n// Let's play a game!\n\n\n\n// Let Alfred give life to Wallace\n\n\nAlfred\n.\ngiveLife\n(\nWallace\n);\n\n\n\n// Save these variables back to python variables to work with later\n\n\nIPython\n.\nnotebook\n.\nkernel\n.\nexecute\n(\n'Alfred_life=\"'\n \n+\n \nAlfred\n.\nlife\n \n+\n \n'\";'\n);\n\n\nIPython\n.\nnotebook\n.\nkernel\n.\nexecute\n(\n'Wallace_life=\"'\n \n+\n \nWallace\n.\nlife\n \n+\n \n'\";'\n);\n\n\n\n\n\nWe grab the Python variable now embedded with \ndocument.getElementById('idname').innerHTML\n, a DOM function and attribute which grabs the \ndiv\n by its \nid\n (not name) and takes what is in the text part of the \ndiv\n.\n\n\nThen we define a class in Javascript that contains an altruistic method for giving life.  We create some users and give them names.  We then \"play\" the game and allow one to give life to another with the method in the User class - how generous of Alfred!\n\n\nLastly, the lovely magic part, we use this sneaky Javascript function from the IPython class that executes Python statements.  We could execute any Python statement really in this way, e.g., \nIPython.notebook.kernel.execute('print(\"Hello World!\")');\n  We include our life values (Javascript variables) into this executable string and, well, execute it.\n\n\nAnd to see if the magic worked, we run a simple couple of print statements back in Python:\n\n\n# Python\n\n\nprint\n(\nAlfred_life\n)\n\n\nprint\n(\nWallace_life\n)\n\n\n\n\n\nWith low and behold and output of:  99 and 101.  Way to pay it forward, Alfred!\n\n\nYou can find this code and more in \nthis\n Jupyter notebook.",
            "title": "Javascript and Python Meet through Magic and IPython"
        },
        {
            "location": "/javascript-and-python-have-a-party/#a-little-exchange",
            "text": "Python :  \"I want your silly game to have a starting value of 100 for life points for all players.\"  Javascript :  \"I read you loud and clear.  Let's take that starting life and play this silly game.  Alfred and Wallace are on the same side battling orcs.  Alfred decides to give life to Wallace because Wallace appears to be about to bravely charge onto the battle field and may need it.  Done.  That's all the game can do right now because I just started making it.\"  Python :  \"Let's see...oh wow.  Your game code actually worked.  Alfred now has 99 life points and Wallace has 101.  Good job us.\"  I went on a hunt to find the cleanest and most succinct way to pass a variable from Python to Javascript and then back to Python in a Jupyter notebook.  I wanted a proof of principle upon which I could later base my D3 graphics using Python data as  pandas  dataframes.  So, this silly example can be translated into code as follows.  First, things are actually easy for us because we are in a Jupyter notebook living in a browser, utilizing all sorts of widgets and HTML elements already.  We can tap into that (and actually the DOM) to get some of the functionality we require.  So, in Python we can use the IPython.display module with the HTML function allowing us to embed an element for later use.  # Python  from   IPython.display   import   HTML  pystartlife   =   str ( 100 )  HTML ( \"<div id='textid'>\"   +   pystartlife   +   \"</div>\" )   We've created a Python variable,  pystartlife , and embedded it as a  div  element, literally just using raw HTML.  Now we use the Javascript magics ( %%javascript ) to create a Javascript coding environment for the next cell.  (BTW there are magics for many more languages - very cool.)    In the Javascript, now, we grab the  div  element with the Python variable from the document or webpage and play our game.  We also write the game.  (This game is based on a  Node.js tutorial  by thenewboston on YouTube - thank you Bucky!).  %% javascript  // Get the python variable from the DOM  var   startlife   =   document . getElementById ( 'textid' ). innerHTML ;  // Define a User class with a method  function   User ()   { \n     this . name   =   '' ; \n     this . life   =   Number ( startlife ); \n     this . giveLife   =   function   giveLife ( targetPlayer )   { \n         targetPlayer . life   +=   1 ; \n         this . life   -=   1 ; \n     }  }  // Use class  var   Alfred   =   new   User ();  var   Wallace   =   new   User ();  // Names were blank so give them name values  Alfred . name   =   'Alfred' ;  Wallace . name   =   'Wallace' ;  // Let's play a game!  // Let Alfred give life to Wallace  Alfred . giveLife ( Wallace );  // Save these variables back to python variables to work with later  IPython . notebook . kernel . execute ( 'Alfred_life=\"'   +   Alfred . life   +   '\";' );  IPython . notebook . kernel . execute ( 'Wallace_life=\"'   +   Wallace . life   +   '\";' );   We grab the Python variable now embedded with  document.getElementById('idname').innerHTML , a DOM function and attribute which grabs the  div  by its  id  (not name) and takes what is in the text part of the  div .  Then we define a class in Javascript that contains an altruistic method for giving life.  We create some users and give them names.  We then \"play\" the game and allow one to give life to another with the method in the User class - how generous of Alfred!  Lastly, the lovely magic part, we use this sneaky Javascript function from the IPython class that executes Python statements.  We could execute any Python statement really in this way, e.g.,  IPython.notebook.kernel.execute('print(\"Hello World!\")');   We include our life values (Javascript variables) into this executable string and, well, execute it.  And to see if the magic worked, we run a simple couple of print statements back in Python:  # Python  print ( Alfred_life )  print ( Wallace_life )   With low and behold and output of:  99 and 101.  Way to pay it forward, Alfred!  You can find this code and more in  this  Jupyter notebook.",
            "title": "A little exchange"
        },
        {
            "location": "/confusion-matrix-code-revealed/",
            "text": "tl;dr\n:  We make a confusion matrix (or ML metric) in python for a k-means algorithm and it's good lookin' :)\n\n\nPosted:\n  2017-02-12\n\n\nStep 1 The AML Workflow\n\n\nOur story starts with an Azure Machine Learning experiment or what I like to call data science workflow (I'll use the word workflow here).   We could also have started with a file (see \nStep 2 Second Way\n) instead, but either way, cleansed data gets fed into a k-means clustering algorithm after some initial processing (I like this brief post on \nk-means\n and it's got python snippets as well!).  This post is about coding up one of the metrics that tells us how well an algorithm did if we have some \"groundtruth\" data to which to compare (remember that often we won't in \nunsupervised\n learning, but we are lucky today).\n\n\nThis workflow is for text feature extraction, selection and clustering based on extracted features as n-grams (check out the intro \nhere\n for a quick explanation of this workflow and n-grams).  I have one workflow with an \na priori\n value for the centroids of 10 for the k-means algorithm.  Here's a screenshot of the workflow (starting dataset is a listing of 500 Wikipedia articles, cleaned up, along with some category labels for \"groundtruth\" comparisons later - remember, k-means is unsupervised).\n\n\n\n\nThis workflow is already ready for you to use for free (using a Microsoft ID like outlook.com, xbox, hotmail, etc. accounts.)  Find it in Cortana Intelligence Gallery (love this place for all of its abundance of resources):  \n\n\nhttps://gallery.cortanaintelligence.com/Experiment/N-Grams-and-Clustering-Find-similar-companies-Training-1\n\n\nJust to highlight, in the AML workflow I selected my desired columns for the confusion matrix with \nSelect Columns in Dataset\n module to get 'Category' and 'Assignment' (cluster assignment as an integer from 0 to number of centroids I specified at the beginning).\n\n\nStep 2 First Way\n\n\n\n\nNotice, I added a \nConvert to CSV\n module (as you can see in above workflow diagram) after the \nSelect Columns in Dataset\n.\n\n\nI right clicked on the output node of the \nConvert to CSV\n and a little menu popped up from which I selected \"Open in a new Notebook\" and \"Python 3\" (because Python 3 rules of course - my R colleagues are going to really chide me now).\n\n\n\n\n\n\nThis opened up a jupyter notebook with the following code snippet:\n\n\nfrom\n \nazureml\n \nimport\n \nWorkspace\n\n\nws\n \n=\n \nWorkspace\n()\n\n\nexperiment\n \n=\n \nws\n.\nexperiments\n[\n'<your experiment id shows up here>'\n]\n\n\nds\n \n=\n \nexperiment\n.\nget_intermediate_dataset\n(\n\n    \nnode_id\n=\n'<your node id shows up here>'\n,\n\n    \nport_name\n=\n'Results dataset'\n,\n\n    \ndata_type_id\n=\n'GenericCSV'\n\n\n)\n\n\nframe\n \n=\n \nds\n.\nto_dataframe\n()\n\n\n\n\n\nAnd imported my final dataset as a \npandas\n DataFrame.\n\n\nTo get a confusion matrix I used \npandas.crosstab\n and \nmatplotlib\n.\n\n\nI created a cell and used \npandas\n's \ncrosstab\n to aggregate the Categories by Assignments and place into a matrix.\n\n\n# Creating our confusion matrix data\n\n\ncm\n \n=\n \npd\n.\ncrosstab\n(\nframe\n[\n'Category'\n],\n \nframe\n[\n'Assignments'\n])\n\n\nprint\n(\ncm\n)\n\n\n\n\n\nSo we went from \n\n\nCategory    Assignments\n0   Information Technology  0\n1   Information Technology  9\n2   Consumer Discretionary  0\n3   Energy  4\n4   Consumer Discretionary  0\n5   Information Technology  2\n6   Information Technology  0\n7   Consumer Discretionary  0\n8   Information Technology  3\n9   Information Technology  2\n10  Financials  8\n11  Consumer Staples    0\n12  Information Technology  6\n13  Consumer Discretionary  7\n14  Information Technology  2\n15  Information Technology  2\n16  Information Technology  0\n17  Industrials 6\n18  Consumer Staples    9\n19  Health Care 9\n\n...\n\n\n\n\nto\n\n\nAssignments                   0  1   2  3   4  5   6   7   8   9\nCategory                                                        \nConsumer Discretionary       43  0   3  1   0  0   1  20   4   4\nConsumer Staples             14  0   0  0   9  0   2   4   0   6\nEnergy                        2  1   0  1  12  0  28   0   0   0\nFinancials                   16  0   3  3   0  0   3   8  42   3\nHealth Care                   3  0   0  1   1  0   0   0   0  47\n\n...\n\n\n\n\nAnd finally, I used \nmatplotlib\n and a modified example from the python docs, with this code,\n\n\n# Plot our confusion matrix\n\n\n# Code based on:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n\n\ndef\n \nplot_confusion_matrix\n(\ncm\n,\n \ntitle\n=\n'Confusion matrix'\n,\n \ncmap\n=\nplt\n.\ncm\n.\nBuPu\n,\n \nnormalize\n=\nFalse\n):\n\n    \n# Set figure size before anything else\n\n    \ncolcnt\n \n=\n \nlen\n(\ncm\n.\ncolumns\n)\n\n    \nrowcnt\n \n=\n \nlen\n(\ncm\n.\nindex\n)\n\n\n    \n# Adjust the size of the plot area ()\n\n    \nplt\n.\nfigure\n(\nfigsize\n=\n(\ncolcnt\n/\n0.8\n,\n \nrowcnt\n/\n0.8\n))\n\n\n    \nif\n \nnormalize\n:\n\n        \n# Normalize each row by the row sum\n\n        \nsum_row\n \n=\n \n[\na\n[\n0\n]\n \nfor\n \na\n \nin\n \ncm\n.\nsum\n(\naxis\n=\n1\n)[:,\n \nnp\n.\nnewaxis\n]]\n\n        \ndf_cm\n \n=\n \npd\n.\nDataFrame\n(\ncm\n)\n\n        \ndf_sum\n \n=\n \npd\n.\nDataFrame\n(\nsum_row\n)\n\n        \ndf\n \n=\n \ndf_cm\n.\nas_matrix\n()\n \n/\n \ndf_sum\n.\nas_matrix\n()\n\n        \ncm\n \n=\n \npd\n.\nDataFrame\n(\ndf\n,\n \nindex\n=\ncm\n.\nindex\n,\n \ncolumns\n=\ncm\n.\ncolumns\n)\n\n\n    \n# Show the plot\n\n    \nplt\n.\nimshow\n(\ncm\n,\n \ninterpolation\n=\n'nearest'\n,\n \ncmap\n=\ncmap\n)\n\n\n    \n# Give the plot a title and colorbar legend\n\n    \nplt\n.\ntitle\n(\ntitle\n,\n \nsize\n=\n12\n)\n\n    \nplt\n.\ncolorbar\n()\n\n\n    \n# All thes stuff for the tick mark labels\n\n    \nxtick_marks\n \n=\n \nnp\n.\narange\n(\nlen\n(\ncm\n.\ncolumns\n))\n\n    \nytick_marks\n \n=\n \nnp\n.\narange\n(\nlen\n(\ncm\n.\nindex\n))\n\n    \nplt\n.\nxticks\n(\nxtick_marks\n,\n \ncm\n.\ncolumns\n,\n \nsize\n=\n12\n)\n\n    \nplt\n.\nyticks\n(\nytick_marks\n,\n \ncm\n.\nindex\n,\n \nsize\n=\n12\n)\n\n\n    \n# Just the regular xlabel and ylabel for plot\n\n    \nplt\n.\nylabel\n(\ncm\n.\nindex\n.\nname\n,\n \nsize\n=\n12\n)\n\n    \nplt\n.\nxlabel\n(\ncm\n.\ncolumns\n.\nname\n,\n \nsize\n=\n12\n)\n\n\n    \n# Setting to offset the labels with some space so they show up\n\n    \nplt\n.\nsubplots_adjust\n(\nleft\n \n=\n \n0.5\n,\n \nbottom\n=\n0.5\n)\n\n\n\n\n# Plot the confusion matrix DataFrame\n\n\n\nplot_confusion_matrix\n(\ncm\n,\n \nnormalize\n=\nFalse\n,\n \n                      \ntitle\n=\n'Confusion matrix (\n%d\n centroids):  no normalization'\n \n%\n \nlen\n(\ncm\n.\ncolumns\n))\n\n\n\nplot_confusion_matrix\n(\ncm\n,\n \nnormalize\n=\nTrue\n,\n\n                      \ntitle\n=\n'Confusion matrix (\n%d\n centroids):  with normalization'\n \n%\n \nlen\n(\ncm\n.\ncolumns\n))\n\n\n\n\n\nto create the following awesome plots (a non-normalized and normalized confusion matrix):\n\n\n\n\nStep 2 Second Way\n\n\nI could have exported the AML Studio data as a file from the \nConvert to CSV\n module and downloaded the dataset after running.  I would then upload the dataset to a notebook (as is also shown in the sample notebook \nhere\n) and use the csv file with a 'Category' column and 'Assigments' column like is found \nhere\n.  It imports the data as a \npandas\n dataframe.\n\n\nThe code snippet would have been:\n\n\n# Dataset living on my github account exported from Azure ML\n\n\nurl\n \n=\n \n'https://raw.githubusercontent.com/michhar/michhar.github.io/gh-pages-source/data/ngrams_and_clustering_result_dataset.csv'\n\n\n\n# Importing the csv data with pandas\n\n\nframe\n \n=\n \npd\n.\nread_csv\n(\nurl\n)\n\n\n\n\n\nThanks for reading, check out the sample (static) jupyter notebook \nhere\n and best of luck with those confusion matrices!",
            "title": "A Simple, Presentable Confusion Matrix with K-means Data"
        },
        {
            "location": "/confusion-matrix-code-revealed/#step-1-the-aml-workflow",
            "text": "Our story starts with an Azure Machine Learning experiment or what I like to call data science workflow (I'll use the word workflow here).   We could also have started with a file (see  Step 2 Second Way ) instead, but either way, cleansed data gets fed into a k-means clustering algorithm after some initial processing (I like this brief post on  k-means  and it's got python snippets as well!).  This post is about coding up one of the metrics that tells us how well an algorithm did if we have some \"groundtruth\" data to which to compare (remember that often we won't in  unsupervised  learning, but we are lucky today).  This workflow is for text feature extraction, selection and clustering based on extracted features as n-grams (check out the intro  here  for a quick explanation of this workflow and n-grams).  I have one workflow with an  a priori  value for the centroids of 10 for the k-means algorithm.  Here's a screenshot of the workflow (starting dataset is a listing of 500 Wikipedia articles, cleaned up, along with some category labels for \"groundtruth\" comparisons later - remember, k-means is unsupervised).   This workflow is already ready for you to use for free (using a Microsoft ID like outlook.com, xbox, hotmail, etc. accounts.)  Find it in Cortana Intelligence Gallery (love this place for all of its abundance of resources):    https://gallery.cortanaintelligence.com/Experiment/N-Grams-and-Clustering-Find-similar-companies-Training-1  Just to highlight, in the AML workflow I selected my desired columns for the confusion matrix with  Select Columns in Dataset  module to get 'Category' and 'Assignment' (cluster assignment as an integer from 0 to number of centroids I specified at the beginning).",
            "title": "Step 1 The AML Workflow"
        },
        {
            "location": "/confusion-matrix-code-revealed/#step-2-first-way",
            "text": "Notice, I added a  Convert to CSV  module (as you can see in above workflow diagram) after the  Select Columns in Dataset .  I right clicked on the output node of the  Convert to CSV  and a little menu popped up from which I selected \"Open in a new Notebook\" and \"Python 3\" (because Python 3 rules of course - my R colleagues are going to really chide me now).    This opened up a jupyter notebook with the following code snippet:  from   azureml   import   Workspace  ws   =   Workspace ()  experiment   =   ws . experiments [ '<your experiment id shows up here>' ]  ds   =   experiment . get_intermediate_dataset ( \n     node_id = '<your node id shows up here>' , \n     port_name = 'Results dataset' , \n     data_type_id = 'GenericCSV'  )  frame   =   ds . to_dataframe ()   And imported my final dataset as a  pandas  DataFrame.  To get a confusion matrix I used  pandas.crosstab  and  matplotlib .  I created a cell and used  pandas 's  crosstab  to aggregate the Categories by Assignments and place into a matrix.  # Creating our confusion matrix data  cm   =   pd . crosstab ( frame [ 'Category' ],   frame [ 'Assignments' ])  print ( cm )   So we went from   Category    Assignments\n0   Information Technology  0\n1   Information Technology  9\n2   Consumer Discretionary  0\n3   Energy  4\n4   Consumer Discretionary  0\n5   Information Technology  2\n6   Information Technology  0\n7   Consumer Discretionary  0\n8   Information Technology  3\n9   Information Technology  2\n10  Financials  8\n11  Consumer Staples    0\n12  Information Technology  6\n13  Consumer Discretionary  7\n14  Information Technology  2\n15  Information Technology  2\n16  Information Technology  0\n17  Industrials 6\n18  Consumer Staples    9\n19  Health Care 9\n\n...  to  Assignments                   0  1   2  3   4  5   6   7   8   9\nCategory                                                        \nConsumer Discretionary       43  0   3  1   0  0   1  20   4   4\nConsumer Staples             14  0   0  0   9  0   2   4   0   6\nEnergy                        2  1   0  1  12  0  28   0   0   0\nFinancials                   16  0   3  3   0  0   3   8  42   3\nHealth Care                   3  0   0  1   1  0   0   0   0  47\n\n...  And finally, I used  matplotlib  and a modified example from the python docs, with this code,  # Plot our confusion matrix  # Code based on:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html  def   plot_confusion_matrix ( cm ,   title = 'Confusion matrix' ,   cmap = plt . cm . BuPu ,   normalize = False ): \n     # Set figure size before anything else \n     colcnt   =   len ( cm . columns ) \n     rowcnt   =   len ( cm . index ) \n\n     # Adjust the size of the plot area () \n     plt . figure ( figsize = ( colcnt / 0.8 ,   rowcnt / 0.8 )) \n\n     if   normalize : \n         # Normalize each row by the row sum \n         sum_row   =   [ a [ 0 ]   for   a   in   cm . sum ( axis = 1 )[:,   np . newaxis ]] \n         df_cm   =   pd . DataFrame ( cm ) \n         df_sum   =   pd . DataFrame ( sum_row ) \n         df   =   df_cm . as_matrix ()   /   df_sum . as_matrix () \n         cm   =   pd . DataFrame ( df ,   index = cm . index ,   columns = cm . columns ) \n\n     # Show the plot \n     plt . imshow ( cm ,   interpolation = 'nearest' ,   cmap = cmap ) \n\n     # Give the plot a title and colorbar legend \n     plt . title ( title ,   size = 12 ) \n     plt . colorbar () \n\n     # All thes stuff for the tick mark labels \n     xtick_marks   =   np . arange ( len ( cm . columns )) \n     ytick_marks   =   np . arange ( len ( cm . index )) \n     plt . xticks ( xtick_marks ,   cm . columns ,   size = 12 ) \n     plt . yticks ( ytick_marks ,   cm . index ,   size = 12 ) \n\n     # Just the regular xlabel and ylabel for plot \n     plt . ylabel ( cm . index . name ,   size = 12 ) \n     plt . xlabel ( cm . columns . name ,   size = 12 ) \n\n     # Setting to offset the labels with some space so they show up \n     plt . subplots_adjust ( left   =   0.5 ,   bottom = 0.5 )  # Plot the confusion matrix DataFrame  plot_confusion_matrix ( cm ,   normalize = False ,  \n                       title = 'Confusion matrix ( %d  centroids):  no normalization'   %   len ( cm . columns ))  plot_confusion_matrix ( cm ,   normalize = True , \n                       title = 'Confusion matrix ( %d  centroids):  with normalization'   %   len ( cm . columns ))   to create the following awesome plots (a non-normalized and normalized confusion matrix):",
            "title": "Step 2 First Way"
        },
        {
            "location": "/confusion-matrix-code-revealed/#step-2-second-way",
            "text": "I could have exported the AML Studio data as a file from the  Convert to CSV  module and downloaded the dataset after running.  I would then upload the dataset to a notebook (as is also shown in the sample notebook  here ) and use the csv file with a 'Category' column and 'Assigments' column like is found  here .  It imports the data as a  pandas  dataframe.  The code snippet would have been:  # Dataset living on my github account exported from Azure ML  url   =   'https://raw.githubusercontent.com/michhar/michhar.github.io/gh-pages-source/data/ngrams_and_clustering_result_dataset.csv'  # Importing the csv data with pandas  frame   =   pd . read_csv ( url )   Thanks for reading, check out the sample (static) jupyter notebook  here  and best of luck with those confusion matrices!",
            "title": "Step 2 Second Way"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/",
            "text": "tl;dr\n:  Azure Machine Learning + Visual Studio + Python Flask + GitHub + Azure = A Live Custom ML Model for You!\n\n\nPosted:\n  2017-02-05\n\n\nIntroduction\n\n\nOk, so I have an interesting REST endpoint (in my case, a machine learning model for using a company's Wikipedia article to find similar companies), what can I do next?  Why not serve it up in a simple web app to impress friends and wow colleagues?  (Really, you can use this intel to create a web app around any REST endpoint, as half of my purpose in writing this is to show how fast and easy Python Flask is).\n\n\nEssentially, we are making a web app wrapper around a data submission and retrieval REST endpoint that is created through Azure Machine Learning (AML) Studio (\nhttps://studio.azureml.net\n), a friendly and powerful machine learning tool with a handy browser UI.  In this post, the endpoint is a service that clusters companies based on descriptive text (our input data).  The clustering model, a k-means algorithm, has been trained on close to 500 wikipedia entries, a cool example of \nunsupervised learning\n.  If you don't know much yet about AML Studio and would like to know more \nthis\n is a good place to start or dive in and learn by doing with a quick getting-started tutorial \nhere\n.  You'll need to know, at least, how to publish an experiment from Studio to get your Flask web app going.\n\n\nThe ML web service is based around an AML scoring experiment, built from a training experiment in which \nK-Means Clustering\n module is used to assign companies to groups based on features in their processed Wikipedia text.   The \nExtract N-Gram Features from Text\n module (more info \nhere\n) is used after some initial cleansing of the text data (remove stop words, numbers, special characters, detect sentences, etc. - see the \nPreprocess Text\n AML module \nhere\n) to extract features upon which to train the k-means clustering model and reduce the dimensionality to the most important chunks of information.  The scoring experiment uses a stored vocabulary from the training data n-gram feature extraction process (a good explanation of n-grams can be found in this blog on extracting features from text for classification, a different kind of ML algorithm - check it out \nhere\n).\n\n\nReal quick, an example of extracting n-grams from:  \"\nTime lost is never found.\n\"\n\n\n\n\nAn example from the blog link I just listed above (this \none\n)\n\n\n\n\n\n\n\n\n\n\nWhere n=1, that is a uni-gram\n\n\nWhere n=2, that is a bi-gram\n\n\nWhere n=3, that is a tri-gram\n\n\n\n\n\n\n\n\n\n\nTime\n\n\nTime lost\n\n\nTime lost is\n\n\n\n\n\n\nlost\n\n\nlost is\n\n\nlost is never\n\n\n\n\n\n\nis\n\n\nis never\n\n\nis never found\n\n\n\n\n\n\nnever\n\n\nnever found\n\n\n\n\n\n\n\n\nfound\n\n\n\n\n\n\n\n\n\n\n\n\nSo, you have an idea of the initial training dataset (but imagine 10,000 or more of these n-grams as our features from all of that Wikipedia text - it can be seen why feature selection is sometimes helpful for narrowing down to the most important features and we can also do this with the \nExtract N-Gram Features from Text\n module in AML).  Ok, let's move on to the app building fun.\n\n\nOur web app is going to utilize a microframework for building web apps purely in the Python programming language.  A big reason to begin in this framework is that Python, a popular Data Science language, is easy to read and learn and Visual Studio has a Flask web app template as part of the Python Tools for Visual Studio extension, making life much easier for us.  Python, as a language, is also known for being a popular web app development language and has other projects like \nDjango\n and \nBottle\n for these ends (also with templates in VS).\n\n\nThat all being said, most of this post is about creating the Flask web app.  I'll leave it to other guides and articles to discuss working with AML and k-means in detail.\n\n\n\n\nAbove:  The deployed web app site\n\n\nBefore you Begin, a Few Things to Do...\n\n\nTools\n\n\n\n\n[recommended]\nVisual Studio\n installed (Community 2015 Edition is what I use; NB:  the preview of 2017 is adding PTVS soon...I'll update on this later;  also, VS 2017 is available for Mac OSX) (\nVisual Studio Community\n) with \nPython Tools for Visual Studio\n installed (to get the Flask Web App template) which can be added during the install of VS or separately from \nhere\n\n\nGit Bash\n or \ngit\n installed - included in git download\n\n\nhttps://git-scm.com/downloads\n\n\n\n\nAccounts\n\n\n\n\nAzure Machine Learning Studio account\n from \nhttps://studio.azureml.net\n (free)\n\n\nGitHub Account\n - a code repository and collaboration tool we'll use (free)\n\n\nhttps://github.com/join\n\n\nAzure account\n - use the one you have, sign up for a free trial at \nhttps://azure.microsoft.com/en-us/free/\n, or, if you have an MSDN account and Azure as a benefit, link your Microsoft Account or Work/School Account to MSDN and activate the Azure benefit by following \nthis\n guide\n\n\n\n\nPrerequisites\n\n\n\n\nThe deployed Azure Machine Learning \nscoring\n experiment\n\n\nNote:  We won't cover this experiment and model here as it's not the focus of this particular post, but a link to instructions is just below.\n\n\nAside:  These experiments are often called \"predictive\", but in a clustering model we really just look for scores and cluster assignments, not predictions so let's call it \nscoring\n experiment \n\n\n\n\nThe scoring experiment which utilizes the k-means model and n-gram featurizer vocabulary created in the training experiment has the following layout in AML Studio:\n\n\n\n\nThe scoring experiment you will need can be found \nhere\n (this will allow you to launch it in AML Studio).  Essentially, we are using AML Studio as a clever way to deploy a web service and not much more, but it's capabilities as a canvas for creating a data science workflow are worth checking out if you like a visual workflow-type setup.\n\n\nStart at this spot in the Azure Docs to get this experiment deployed as a web service to use later in this guide post:  \nhttps://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-walkthrough-5-publish-web-service#deploy-the-web-service\n.\n\n\nA similar guide with good diagrams on deploying the experiment as a web service can be picked up at section \"4.2.2. Publishing a trained model as Web Service\" in this tutorial: \nhttps://github.com/Azure-Readiness/hol-azure-machine-learning/blob/master/004-lab-azureml-experiment.md#422-publishing-a-trained-model-as-web-service\n.\n\n\nThe Web App Development\n\n\nNote:  Under REQUEST/RESPONSE for the AML Studio experiment (found after deploying as web service from Studio), one will find all of the specs needed to work with this endpoint.\n\n\nLet's write a web app!  We're going to begin in Visual Studio.  As an aside, did you know VS 2017 is available for Mac?!  What's especially cool is that developers can share projects across Mac and Windows.  The Python Tools for Visual Studio extension isn't available, however, on VS 2017 so I'm eagerly awaiting this capability.  Will report back later.  Since VS 2015 with PTVS is available for us on Windows we will be using the awesome Flask Web Project template that comes with it to kick start our web app dev in Windows 10.\n\n\n\n\n\n\nOpen VS\n\n\n\n\n\n\nCreate a new Python Flask web app project (this template should exist if one chooses Python - scroll down Templates -> Python -> Flask Web Project) with Python 3.5 (or whichever 3 you have will do) into a virtual environment.  At this point, you literally have a functioning web app.  Hit the Run (your default browser choice is next to the button) in VS and test out this template.\n\n\n\n\n\n\nAdd a new file called \nforms.py\n to the main directory (alongside \nviews.py\n).  This will contain the form-building code through which data will be sent to the REST endpoint for analysis.  There are three fields we need in our input form: title, category and text.  Title is the company title, category is an optional field for the category of company (e.g. information technology) and text is the Wikipedia article text about that company or some descriptive corpus.\n\n\n\n\n\n\nThe input form:  define in \"forms\"\n\n\nPlace the following text in the \nforms.py\n file:\n\n\nfrom\n \nwtforms\n \nimport\n \nForm\n,\n \nStringField\n,\n \nTextAreaField\n,\n \nvalidators\n\n\n\n# This class will be used in the webapp as the main input form\n\n\nclass\n \nSubmissionForm\n(\nForm\n):\n\n    \ntitle\n \n=\n \nStringField\n(\n'Title'\n,\n \n[\nvalidators\n.\nLength\n(\nmin\n=\n2\n,\n \nmax\n=\n30\n)])\n\n    \ncategory\n \n=\n \nStringField\n(\n'Category'\n,\n \n[\nvalidators\n.\nLength\n(\nmin\n=\n0\n,\n \nmax\n=\n30\n)])\n\n    \ntext\n \n=\n \nTextAreaField\n(\n'Text'\n,\n \n[\nvalidators\n.\nLength\n(\nmin\n=\n1\n,\n \nmax\n=\n1000\n)])\n\n\n\n\n\nThe routing on the page:  define in the \"views\"\n\n\nWe will be modifying the existing template code as follows.\n\n\nThe imports should look like:\n\n\nimport\n \njson\n\n\nimport\n \nurllib.request\n\n\nimport\n \nos\n\n\n\nfrom\n \ndatetime\n \nimport\n \ndatetime\n\n\nfrom\n \nflask\n \nimport\n \nrender_template\n,\n \nrequest\n,\n \nredirect\n\n\nfrom\n \nFlaskAppAML\n \nimport\n \napp\n\n\n\nfrom\n \nFlaskAppAML.forms\n \nimport\n \nSubmissionForm\n\n\n\n\n\nWhich added json handling, http request handling, os interaction and the way in which the forms class from above is available for use.\n\n\nAdd a way to grab the API_KEY and URL at the beginning of the \nviews.py\n file:\n\n\n# Deployment environment variables defined on Azure (pull in with os.environ)\n\n\nAPI_KEY\n \n=\n \nos\n.\nenviron\n.\nget\n(\n'API_KEY'\n,\n \n\"optionally place a default value for local dev here\"\n)\n\n\nURL\n \n=\n \nos\n.\nenviron\n.\nget\n(\n'URL'\n,\n \n\"optionally place a default value for local dev here\"\n)\n\n\n\n\n\nand HEADERS global variable:\n\n\n# Construct the HTTP request header\n\n\nHEADERS\n \n=\n \n{\n'Content-Type'\n:\n'application/json'\n,\n \n'Authorization'\n:(\n'Bearer '\n+\n \nAPI_KEY\n)}\n\n\n\nChange the \"home route\" (landing page functionality), \ndef home\n method definition, to be:\n\n\n# Our main app page/route\n\n\n@app.route\n(\n'/'\n,\n \nmethods\n=\n[\n'GET'\n,\n \n'POST'\n])\n\n\n@app.route\n(\n'/home'\n,\n \nmethods\n=\n[\n'GET'\n,\n \n'POST'\n])\n\n\ndef\n \nhome\n():\n\n    \n\"\"\"Renders the home page which is the CNS of the web app currently, nothing pretty.\"\"\"\n\n\n    \nform\n \n=\n \nSubmissionForm\n(\nrequest\n.\nform\n)\n\n\n    \n# Form has been submitted\n\n    \nif\n \nrequest\n.\nmethod\n \n==\n \n'POST'\n \nand\n \nform\n.\nvalidate\n():\n\n\n        \n# Plug in the data into a dictionary object \n\n        \n#  - data from the input form\n\n        \n#  - text data must be converted to lowercase\n\n        \ndata\n \n=\n  \n{\n\n              \n\"Inputs\"\n:\n \n{\n\n                \n\"input1\"\n:\n \n{\n\n                  \n\"ColumnNames\"\n:\n \n[\n\n                    \n\"Title\"\n,\n\n                    \n\"Category\"\n,\n\n                    \n\"Text\"\n\n                  \n],\n\n                  \n\"Values\"\n:\n \n[\n \n[\n\n                      \nform\n.\ntitle\n.\ndata\n,\n\n                      \nform\n.\ncategory\n.\ndata\n,\n\n                      \nform\n.\ntext\n.\ndata\n.\nlower\n()\n\n                    \n]\n\n                  \n]\n\n                \n}\n\n              \n},\n\n              \n\"GlobalParameters\"\n:\n \n{}\n\n            \n}\n\n\n        \n# Serialize the input data into json string\n\n        \nbody\n \n=\n \nstr\n.\nencode\n(\njson\n.\ndumps\n(\ndata\n))\n\n\n        \n# Formulate the request\n\n        \nreq\n \n=\n \nurllib\n.\nrequest\n.\nRequest\n(\nURL\n,\n \nbody\n,\n \nHEADERS\n)\n\n\n        \n# Send this request to the AML service and render the results on page\n\n        \ntry\n:\n\n            \n# response = requests.post(URL, headers=HEADERS, data=body)\n\n            \nresponse\n \n=\n \nurllib\n.\nrequest\n.\nurlopen\n(\nreq\n)\n\n            \nrespdata\n \n=\n \nresponse\n.\nread\n()\n\n            \nresult\n \n=\n \njson\n.\nloads\n(\nstr\n(\nrespdata\n,\n \n'utf-8'\n))\n\n            \nresult\n \n=\n \njson\n.\ndumps\n(\nresult\n,\n \nindent\n=\n4\n,\n \nsort_keys\n=\nTrue\n)\n\n            \nreturn\n \nrender_template\n(\n\n                \n'result.html'\n,\n\n                \ntitle\n=\n\"From your friendly AML experiment's Web Service:\"\n,\n\n                \nresult\n=\nresult\n)\n\n\n        \n# An HTTP error\n\n        \nexcept\n \nException\n \nas\n \nerr\n:\n\n            \nresult\n \n=\n \njson\n.\nloads\n(\nstr\n(\nerr\n.\ncode\n))\n\n            \nreturn\n \nrender_template\n(\n\n                \n'result.html'\n,\n\n                \ntitle\n=\n'There was an error'\n,\n\n                \nresult\n=\nresult\n)\n\n\n    \n# Just serve up the input form\n\n    \nreturn\n \nrender_template\n(\n\n        \n'form.html'\n,\n\n        \nform\n=\nform\n,\n\n        \ntitle\n=\n'Run App'\n,\n\n        \nyear\n=\ndatetime\n.\nnow\n()\n.\nyear\n,\n\n        \nmessage\n=\n'Input form to gain insights into a company using Azure Machine Learning'\n)\n\n\n\n\n\nThe html templates:  how the information gets served\n\n\n\n\nWe add two new templates:  form.html, result.html\n\n\n\n\nThe \nform.html\n gives us a construct for the user to enter in input data and the \nresult.html\n, a construct in which the results from the machine learning experiment can be displayed.\n\n\nGrab the \nform.html\n code \nhere\n.\nGrab the \nresult.html\n code \nhere\n.  Note, this code may result in slightly different web app appearances to this article.\n\n\nNow that we have some new code to handle calling the AML web service and html templates to handle input and output, let's prepare to deploy by taking a look at some configuration.\n\n\nPrepare to Deploy the Web App to Azure\n\n\nBefore we publish, we must add two configuration-type files:\n\n\n\n\nA web configuration file (web.config)\n\n\nVirtual environment proxy (ptvs_virtualenv_proxy.py)\n\n\n\n\nThe \nweb.config\n file may need some modifications, however the virtual environment proxy file should work as is from this folder.\n\n\nWeb Configuration file\n\n\n\n\nAdd web.config at project level (alongside requirements.txt file)\n\n\n\n\nIt should look something like the following (you can actually add a template web.config similar to this one in VS by right-clicking on the FlaskAppAML folder -> Add -> New Item -> Azure web.config for FastCGI, but it will need a few modifications).  Note that the Python version may change in the future and this script might need modification.\n\n\n<?xml version=\"1.0\"?>\n\n\n<configuration>\n\n  \n<appSettings>\n\n    \n<add\n \nkey=\n\"WSGI_ALT_VIRTUALENV_HANDLER\"\n \nvalue=\n\"FlaskAppAML.app\"\n \n/>\n\n    \n<add\n \nkey=\n\"WSGI_ALT_VIRTUALENV_ACTIVATE_THIS\"\n\n         \nvalue=\n\"D:\\home\\site\\wwwroot\\env\\Scripts\\python.exe\"\n \n/>\n\n    \n<add\n \nkey=\n\"WSGI_HANDLER\"\n\n         \nvalue=\n\"ptvs_virtualenv_proxy.get_venv_handler()\"\n \n/>\n\n    \n<add\n \nkey=\n\"PYTHONPATH\"\n \nvalue=\n\"D:\\home\\site\\wwwroot\"\n \n/>\n\n  \n</appSettings>\n\n  \n<system.web>\n\n    \n<compilation\n \ndebug=\n\"true\"\n \ntargetFramework=\n\"4.0\"\n \n/>\n\n  \n</system.web>\n\n  \n<system.webServer>\n\n    \n<modules\n \nrunAllManagedModulesForAllRequests=\n\"true\"\n \n/>\n\n    \n<handlers>\n\n      \n<remove\n \nname=\n\"Python27_via_FastCGI\"\n \n/>\n\n      \n<remove\n \nname=\n\"Python34_via_FastCGI\"\n \n/>\n\n      \n<add\n \nname=\n\"Python FastCGI\"\n\n           \npath=\n\"handler.fcgi\"\n\n           \nverb=\n\"*\"\n\n           \nmodules=\n\"FastCgiModule\"\n\n           \nscriptProcessor=\n\"D:\\Python34\\python.exe|D:\\Python34\\Scripts\\wfastcgi.py\"\n\n           \nresourceType=\n\"Unspecified\"\n\n           \nrequireAccess=\n\"Script\"\n \n/>\n\n    \n</handlers>\n\n    \n<rewrite>\n\n      \n<rules>\n\n        \n<rule\n \nname=\n\"Static Files\"\n \nstopProcessing=\n\"true\"\n>\n\n          \n<match\n \nurl=\n\"^/static/.*\"\n \nignoreCase=\n\"true\"\n \n/>\n\n          \n<action\n \ntype=\n\"Rewrite\"\n \nurl=\n\"^/FlaskAppAML/static/.*\"\n \nappendQueryString=\n\"true\"\n \n/>\n\n        \n</rule>\n\n        \n<rule\n \nname=\n\"Configure Python\"\n \nstopProcessing=\n\"true\"\n>\n\n          \n<match\n \nurl=\n\"(.*)\"\n \nignoreCase=\n\"false\"\n \n/>\n\n          \n<conditions>\n\n          \n</conditions>\n\n          \n<action\n \ntype=\n\"Rewrite\"\n \nurl=\n\"handler.fcgi/{R:1}\"\n \nappendQueryString=\n\"true\"\n \n/>\n\n        \n</rule>\n\n      \n</rules>\n\n    \n</rewrite>\n\n  \n</system.webServer>\n\n\n</configuration>\n\n\n\n\n\nPossible modifications or places of note in the web config:\n\n\n\n\n\n\nThe \nWSGI_ALT_VIRTUALENV_HANDLER\n will very likely need to be modified.  Here it is \nFlaskAppAML.app\n, referring to my flask application itself.\n\n\n\n\n\n\nThe line \n<action type=\"Rewrite\" url=\"^/FlaskAppAML/static/.*\" appendQueryString=\"true\" />\n under rules MUST have the correct project name (here mine was FlaskAppAML).  This section ensures the static files (important for the web service appearance) can be found.\n\n\n\n\n\n\nThe \nscriptProcessor\n, under \nhandlers\n in the web.config xml above, must correspond to the resources existing on the web server's file system (e.g. \nD:\\Python34\\python.exe\n).\n\n\n\n\n\n\nVirtual Environment Proxy\n\n\nThe code for this set of helper functions can be found \nhere\n in the Azure documentation (that similar article talks about deploying continuously from a git repository - a good method to know as well).\n\n\nJust include \nptvs_virtualenv_proxy\n in the base of your project along with the \nweb.config\n (and auto-created \nrequirements.txt\n and \nrunserver.py\n).\n\n\nFinally, to test all of this code locally just click the run button in the navi in VS (your default browser should also appear there).\n\n\nCongrats, you have tested this locally and things seem to be good.\n\n\nDeploy the Flask Web App\n\n\nOption 1: Set up a GitHub Repository as the Deployment Option\n\n\nThis is the most customizable way, hence more complex, but also the most transparent and easy to troubleshoot.  \n\n\n\n\nLog in to GitHub and create a new repository (I called mine \nflask-webapp-aml\n, initializing with a README and a .gitignore for Visual Studio files.\n\n\nIn Git bash on the Desktop, type into the terminal the command to clone the new repository, for example: (I'm using SSH because it will allow me to push changes back up):\n\n\ngit clone git@github.com:<your github username without these triangle brackets>/flask-webapp-aml.git\n\n\nCopy all of the project code to this new repository folder locally (I just \ncp\n on the command line in Git bash) to match this structure:\n\n  FlaskAppAML/\n  FlaskAppAML/__init__.py\n  FlaskAppAML/forms.py\n  FlaskAppAML/views.py\n  FlaskAppAML/static -> *our static files*\n  FlaskAppAML/templates -> *the html page templates*\n  env/ -> *the entire python environment*\n  ptvs_virtualenv_proxy.py\n  README.md\n  requirements.txt\n  runserver.py\n  runtime.txt\n  web.config\n  .skipPythonDeployment\n\n\n\nIf the empty \n.skipPythonDeployment\n file is not in the base of your repository, add one now.  Also, make sure the \nenv\n folder from the VS project is present.  This contains all of the python environment needed for running this web app (really anywhere).  We are skipping having the web service custom install all of the necessary modules by giving the service this \n.skipPythonDeployment\n file and the \nenv\n folder.\n\n\nAdd \"__pycache__\" on it's own line to my \".gitignore\" file and anything you don't want uploaded to the GitHub when we \"push\" changes.\n\n\n\n\nNow it all seems pretty tidy, so it's time to push the changes up to be hosted on GitHub.  I \nadd\n (\"stage\"), \ncommit\n (commit my code locally with a message) and \npush\n (push up to the web to be hosted on GitHub) all of my additions or any changes I've made.  I can do this \nadd/commit/push\n again as many times as I want in the future.  I must, however, do all three consecutively and in that order otherwise it gets complicated.  So, my commands look like:\n\n\n\n\ngit add .\n (from the base of the repository)\n\n\ngit commit -m \"initial commit message\"\n (the -m is our message so be brief, but descriptive - visible to the world)\n\n\ngit push\n (we could also have written \ngit push origin master\n, but it's not necessary to be so verbose right now)\n\n\n\n\n\n\n\n\nCreate an App Service Web App in the Azure Portal (\nhttps://portal.azure.com\n) by clicking \"+\" and search for \"web app\", then go through the wizard to create one.\n\n\n\n\nUpdate the Deployment options in the Azure Portal for the web app.  For our Web App, under \"APP DEPLOYMENT\", open the \"Deployment options\" blade.  For Choose Source, choose GitHub (you may have to log in to your GitHub here to link it).  Under Choose project, pick the GitHub repository to which you just pushed code and click OK.\n\n\nNow we add a couple of variables to the Azure Portal Web App for safe-keeping.  There are \"environmental variables\" in the code (they look like \nos.environ.get('FOO')\n):  one for the AML Web Service's URL and one for the API_KEY - these are the necessary values we need to access our published AML scoring experiment.  To have these available for our web app we need to put them somewhere discoverable and that is as variables under \"App settings\" in the \"Application settings\" blade for our own Web App in the Azure Portal.\n ![image of entering in keys to Azure portal app service]({{ site.baseurl }}/img/flaskapp-adding-sys-vars.jpg)\n\n\nEnsure that, in the Portal, under Application Settings, Python is set to the appropriate version (default is that \"Python\" is Off in settings - so will need to manually switch to it's version).\n\n\nIf we go back to the \"Deployment options\" we can see how our build is going.  This process will automatically happen for us every time a new change is made to our GitHub repository.  Ensure that this build completes successfully.\n\n\nFinally, if you go to \"Overview\" and click on the web app's URL, you'll see your site.\n\n\n\n\nCongrats on completing this process!  You should now have a functioning barebones, machine learning web app.  Go ahead and try it out. :)\n\n\n\n\nIf you encounter any problems, check the Troubleshooting section below, Azure docs, or StackOverflow.  Also, leave a comment if it's a bug in the code or process.\n\n\nOption 2: Publish and Deploy from VS as an Azure App Service Web App\n\n\nTo deploy we must also publish this project to Azure (it's done together with VS).  Fortunately, from within VS (note, I'm in VS 2017, but it's available in previous releases) there's a \n\"Publish...\"\n option.  Right-click on the project name and in the pop-up \n\"Publish...\"\n should be available.  Click this and simply go through the wizard to set up an Azure App Service Web App.  It should be very straightforward and easy to do.\n\n\nAs an alternative to publishing/deploying directly from VS, one can leverage a git repository or use code on GitHub as a deployment option.  Similar instructions can be found in \nthis\n Azure article.\n\n\nMake it Your Own\n\n\nModify the \nlayout.html\n file with app name and navi layout changes.  Or change your custom stylesheet under static -> content -> site.css.\n\n\nGo grab all of the code at \nhttps://github.com/michhar/flask-webapp-aml\n and add it to a project, test, develop and deploy.  You could even if you wish just fork this repository and deploy directly from that in the Azure Portal, but then that would have been too easy. ;)\n\n\nTroubleshooting\n\n\n\n\nAll sample code can be found at \nhttps://github.com/michhar/flask-webapp-aml\n - it may, over time, have more complex samples, so check it out.\n\n\nGo to Application Settings and ensure Python is enabled along with other key settings in the Azure Portal\n\n\nGo to Console (under Development Tools) and make sure all files and programs specified in the \nweb.config\n exists.\n\n\nEnsure in \nweb.config\n, that the \"scriptProcessor\" key/value in handlers is correct (that these paths exist on the server file system).\n\n\nCheck FREB Logs in the Portal for more information around warnings and errors (make sure you are logging for those during this phase).\n\n\nPost comments here or if around the code, under issues here:  \nhttps://github.com/michhar/flask-webapp-aml/issues\n - many thanks!",
            "title": "Creating a Smart Python Flask Web App using Azure Machine Learning"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#introduction",
            "text": "Ok, so I have an interesting REST endpoint (in my case, a machine learning model for using a company's Wikipedia article to find similar companies), what can I do next?  Why not serve it up in a simple web app to impress friends and wow colleagues?  (Really, you can use this intel to create a web app around any REST endpoint, as half of my purpose in writing this is to show how fast and easy Python Flask is).  Essentially, we are making a web app wrapper around a data submission and retrieval REST endpoint that is created through Azure Machine Learning (AML) Studio ( https://studio.azureml.net ), a friendly and powerful machine learning tool with a handy browser UI.  In this post, the endpoint is a service that clusters companies based on descriptive text (our input data).  The clustering model, a k-means algorithm, has been trained on close to 500 wikipedia entries, a cool example of  unsupervised learning .  If you don't know much yet about AML Studio and would like to know more  this  is a good place to start or dive in and learn by doing with a quick getting-started tutorial  here .  You'll need to know, at least, how to publish an experiment from Studio to get your Flask web app going.  The ML web service is based around an AML scoring experiment, built from a training experiment in which  K-Means Clustering  module is used to assign companies to groups based on features in their processed Wikipedia text.   The  Extract N-Gram Features from Text  module (more info  here ) is used after some initial cleansing of the text data (remove stop words, numbers, special characters, detect sentences, etc. - see the  Preprocess Text  AML module  here ) to extract features upon which to train the k-means clustering model and reduce the dimensionality to the most important chunks of information.  The scoring experiment uses a stored vocabulary from the training data n-gram feature extraction process (a good explanation of n-grams can be found in this blog on extracting features from text for classification, a different kind of ML algorithm - check it out  here ).  Real quick, an example of extracting n-grams from:  \" Time lost is never found. \"   An example from the blog link I just listed above (this  one )      Where n=1, that is a uni-gram  Where n=2, that is a bi-gram  Where n=3, that is a tri-gram      Time  Time lost  Time lost is    lost  lost is  lost is never    is  is never  is never found    never  never found     found       So, you have an idea of the initial training dataset (but imagine 10,000 or more of these n-grams as our features from all of that Wikipedia text - it can be seen why feature selection is sometimes helpful for narrowing down to the most important features and we can also do this with the  Extract N-Gram Features from Text  module in AML).  Ok, let's move on to the app building fun.  Our web app is going to utilize a microframework for building web apps purely in the Python programming language.  A big reason to begin in this framework is that Python, a popular Data Science language, is easy to read and learn and Visual Studio has a Flask web app template as part of the Python Tools for Visual Studio extension, making life much easier for us.  Python, as a language, is also known for being a popular web app development language and has other projects like  Django  and  Bottle  for these ends (also with templates in VS).  That all being said, most of this post is about creating the Flask web app.  I'll leave it to other guides and articles to discuss working with AML and k-means in detail.   Above:  The deployed web app site",
            "title": "Introduction"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#before-you-begin-a-few-things-to-do",
            "text": "",
            "title": "Before you Begin, a Few Things to Do..."
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#tools",
            "text": "[recommended] Visual Studio  installed (Community 2015 Edition is what I use; NB:  the preview of 2017 is adding PTVS soon...I'll update on this later;  also, VS 2017 is available for Mac OSX) ( Visual Studio Community ) with  Python Tools for Visual Studio  installed (to get the Flask Web App template) which can be added during the install of VS or separately from  here  Git Bash  or  git  installed - included in git download  https://git-scm.com/downloads",
            "title": "Tools"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#accounts",
            "text": "Azure Machine Learning Studio account  from  https://studio.azureml.net  (free)  GitHub Account  - a code repository and collaboration tool we'll use (free)  https://github.com/join  Azure account  - use the one you have, sign up for a free trial at  https://azure.microsoft.com/en-us/free/ , or, if you have an MSDN account and Azure as a benefit, link your Microsoft Account or Work/School Account to MSDN and activate the Azure benefit by following  this  guide",
            "title": "Accounts"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#prerequisites",
            "text": "The deployed Azure Machine Learning  scoring  experiment  Note:  We won't cover this experiment and model here as it's not the focus of this particular post, but a link to instructions is just below.  Aside:  These experiments are often called \"predictive\", but in a clustering model we really just look for scores and cluster assignments, not predictions so let's call it  scoring  experiment    The scoring experiment which utilizes the k-means model and n-gram featurizer vocabulary created in the training experiment has the following layout in AML Studio:   The scoring experiment you will need can be found  here  (this will allow you to launch it in AML Studio).  Essentially, we are using AML Studio as a clever way to deploy a web service and not much more, but it's capabilities as a canvas for creating a data science workflow are worth checking out if you like a visual workflow-type setup.  Start at this spot in the Azure Docs to get this experiment deployed as a web service to use later in this guide post:   https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-walkthrough-5-publish-web-service#deploy-the-web-service .  A similar guide with good diagrams on deploying the experiment as a web service can be picked up at section \"4.2.2. Publishing a trained model as Web Service\" in this tutorial:  https://github.com/Azure-Readiness/hol-azure-machine-learning/blob/master/004-lab-azureml-experiment.md#422-publishing-a-trained-model-as-web-service .",
            "title": "Prerequisites"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#the-web-app-development",
            "text": "Note:  Under REQUEST/RESPONSE for the AML Studio experiment (found after deploying as web service from Studio), one will find all of the specs needed to work with this endpoint.  Let's write a web app!  We're going to begin in Visual Studio.  As an aside, did you know VS 2017 is available for Mac?!  What's especially cool is that developers can share projects across Mac and Windows.  The Python Tools for Visual Studio extension isn't available, however, on VS 2017 so I'm eagerly awaiting this capability.  Will report back later.  Since VS 2015 with PTVS is available for us on Windows we will be using the awesome Flask Web Project template that comes with it to kick start our web app dev in Windows 10.    Open VS    Create a new Python Flask web app project (this template should exist if one chooses Python - scroll down Templates -> Python -> Flask Web Project) with Python 3.5 (or whichever 3 you have will do) into a virtual environment.  At this point, you literally have a functioning web app.  Hit the Run (your default browser choice is next to the button) in VS and test out this template.    Add a new file called  forms.py  to the main directory (alongside  views.py ).  This will contain the form-building code through which data will be sent to the REST endpoint for analysis.  There are three fields we need in our input form: title, category and text.  Title is the company title, category is an optional field for the category of company (e.g. information technology) and text is the Wikipedia article text about that company or some descriptive corpus.",
            "title": "The Web App Development"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#the-input-form-define-in-forms",
            "text": "Place the following text in the  forms.py  file:  from   wtforms   import   Form ,   StringField ,   TextAreaField ,   validators  # This class will be used in the webapp as the main input form  class   SubmissionForm ( Form ): \n     title   =   StringField ( 'Title' ,   [ validators . Length ( min = 2 ,   max = 30 )]) \n     category   =   StringField ( 'Category' ,   [ validators . Length ( min = 0 ,   max = 30 )]) \n     text   =   TextAreaField ( 'Text' ,   [ validators . Length ( min = 1 ,   max = 1000 )])",
            "title": "The input form:  define in \"forms\""
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#the-routing-on-the-page-define-in-the-views",
            "text": "We will be modifying the existing template code as follows.  The imports should look like:  import   json  import   urllib.request  import   os  from   datetime   import   datetime  from   flask   import   render_template ,   request ,   redirect  from   FlaskAppAML   import   app  from   FlaskAppAML.forms   import   SubmissionForm   Which added json handling, http request handling, os interaction and the way in which the forms class from above is available for use.  Add a way to grab the API_KEY and URL at the beginning of the  views.py  file:  # Deployment environment variables defined on Azure (pull in with os.environ)  API_KEY   =   os . environ . get ( 'API_KEY' ,   \"optionally place a default value for local dev here\" )  URL   =   os . environ . get ( 'URL' ,   \"optionally place a default value for local dev here\" )   and HEADERS global variable:  # Construct the HTTP request header  HEADERS   =   { 'Content-Type' : 'application/json' ,   'Authorization' :( 'Bearer ' +   API_KEY )}  \nChange the \"home route\" (landing page functionality),  def home  method definition, to be:  # Our main app page/route  @app.route ( '/' ,   methods = [ 'GET' ,   'POST' ])  @app.route ( '/home' ,   methods = [ 'GET' ,   'POST' ])  def   home (): \n     \"\"\"Renders the home page which is the CNS of the web app currently, nothing pretty.\"\"\" \n\n     form   =   SubmissionForm ( request . form ) \n\n     # Form has been submitted \n     if   request . method   ==   'POST'   and   form . validate (): \n\n         # Plug in the data into a dictionary object  \n         #  - data from the input form \n         #  - text data must be converted to lowercase \n         data   =    { \n               \"Inputs\" :   { \n                 \"input1\" :   { \n                   \"ColumnNames\" :   [ \n                     \"Title\" , \n                     \"Category\" , \n                     \"Text\" \n                   ], \n                   \"Values\" :   [   [ \n                       form . title . data , \n                       form . category . data , \n                       form . text . data . lower () \n                     ] \n                   ] \n                 } \n               }, \n               \"GlobalParameters\" :   {} \n             } \n\n         # Serialize the input data into json string \n         body   =   str . encode ( json . dumps ( data )) \n\n         # Formulate the request \n         req   =   urllib . request . Request ( URL ,   body ,   HEADERS ) \n\n         # Send this request to the AML service and render the results on page \n         try : \n             # response = requests.post(URL, headers=HEADERS, data=body) \n             response   =   urllib . request . urlopen ( req ) \n             respdata   =   response . read () \n             result   =   json . loads ( str ( respdata ,   'utf-8' )) \n             result   =   json . dumps ( result ,   indent = 4 ,   sort_keys = True ) \n             return   render_template ( \n                 'result.html' , \n                 title = \"From your friendly AML experiment's Web Service:\" , \n                 result = result ) \n\n         # An HTTP error \n         except   Exception   as   err : \n             result   =   json . loads ( str ( err . code )) \n             return   render_template ( \n                 'result.html' , \n                 title = 'There was an error' , \n                 result = result ) \n\n     # Just serve up the input form \n     return   render_template ( \n         'form.html' , \n         form = form , \n         title = 'Run App' , \n         year = datetime . now () . year , \n         message = 'Input form to gain insights into a company using Azure Machine Learning' )",
            "title": "The routing on the page:  define in the \"views\""
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#the-html-templates-how-the-information-gets-served",
            "text": "We add two new templates:  form.html, result.html   The  form.html  gives us a construct for the user to enter in input data and the  result.html , a construct in which the results from the machine learning experiment can be displayed.  Grab the  form.html  code  here .\nGrab the  result.html  code  here .  Note, this code may result in slightly different web app appearances to this article.  Now that we have some new code to handle calling the AML web service and html templates to handle input and output, let's prepare to deploy by taking a look at some configuration.",
            "title": "The html templates:  how the information gets served"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#prepare-to-deploy-the-web-app-to-azure",
            "text": "Before we publish, we must add two configuration-type files:   A web configuration file (web.config)  Virtual environment proxy (ptvs_virtualenv_proxy.py)   The  web.config  file may need some modifications, however the virtual environment proxy file should work as is from this folder.",
            "title": "Prepare to Deploy the Web App to Azure"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#web-configuration-file",
            "text": "Add web.config at project level (alongside requirements.txt file)   It should look something like the following (you can actually add a template web.config similar to this one in VS by right-clicking on the FlaskAppAML folder -> Add -> New Item -> Azure web.config for FastCGI, but it will need a few modifications).  Note that the Python version may change in the future and this script might need modification.  <?xml version=\"1.0\"?>  <configuration> \n   <appSettings> \n     <add   key= \"WSGI_ALT_VIRTUALENV_HANDLER\"   value= \"FlaskAppAML.app\"   /> \n     <add   key= \"WSGI_ALT_VIRTUALENV_ACTIVATE_THIS\" \n          value= \"D:\\home\\site\\wwwroot\\env\\Scripts\\python.exe\"   /> \n     <add   key= \"WSGI_HANDLER\" \n          value= \"ptvs_virtualenv_proxy.get_venv_handler()\"   /> \n     <add   key= \"PYTHONPATH\"   value= \"D:\\home\\site\\wwwroot\"   /> \n   </appSettings> \n   <system.web> \n     <compilation   debug= \"true\"   targetFramework= \"4.0\"   /> \n   </system.web> \n   <system.webServer> \n     <modules   runAllManagedModulesForAllRequests= \"true\"   /> \n     <handlers> \n       <remove   name= \"Python27_via_FastCGI\"   /> \n       <remove   name= \"Python34_via_FastCGI\"   /> \n       <add   name= \"Python FastCGI\" \n            path= \"handler.fcgi\" \n            verb= \"*\" \n            modules= \"FastCgiModule\" \n            scriptProcessor= \"D:\\Python34\\python.exe|D:\\Python34\\Scripts\\wfastcgi.py\" \n            resourceType= \"Unspecified\" \n            requireAccess= \"Script\"   /> \n     </handlers> \n     <rewrite> \n       <rules> \n         <rule   name= \"Static Files\"   stopProcessing= \"true\" > \n           <match   url= \"^/static/.*\"   ignoreCase= \"true\"   /> \n           <action   type= \"Rewrite\"   url= \"^/FlaskAppAML/static/.*\"   appendQueryString= \"true\"   /> \n         </rule> \n         <rule   name= \"Configure Python\"   stopProcessing= \"true\" > \n           <match   url= \"(.*)\"   ignoreCase= \"false\"   /> \n           <conditions> \n           </conditions> \n           <action   type= \"Rewrite\"   url= \"handler.fcgi/{R:1}\"   appendQueryString= \"true\"   /> \n         </rule> \n       </rules> \n     </rewrite> \n   </system.webServer>  </configuration>   Possible modifications or places of note in the web config:    The  WSGI_ALT_VIRTUALENV_HANDLER  will very likely need to be modified.  Here it is  FlaskAppAML.app , referring to my flask application itself.    The line  <action type=\"Rewrite\" url=\"^/FlaskAppAML/static/.*\" appendQueryString=\"true\" />  under rules MUST have the correct project name (here mine was FlaskAppAML).  This section ensures the static files (important for the web service appearance) can be found.    The  scriptProcessor , under  handlers  in the web.config xml above, must correspond to the resources existing on the web server's file system (e.g.  D:\\Python34\\python.exe ).",
            "title": "Web Configuration file"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#virtual-environment-proxy",
            "text": "The code for this set of helper functions can be found  here  in the Azure documentation (that similar article talks about deploying continuously from a git repository - a good method to know as well).  Just include  ptvs_virtualenv_proxy  in the base of your project along with the  web.config  (and auto-created  requirements.txt  and  runserver.py ).  Finally, to test all of this code locally just click the run button in the navi in VS (your default browser should also appear there).  Congrats, you have tested this locally and things seem to be good.",
            "title": "Virtual Environment Proxy"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#deploy-the-flask-web-app",
            "text": "",
            "title": "Deploy the Flask Web App"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#option-1-set-up-a-github-repository-as-the-deployment-option",
            "text": "This is the most customizable way, hence more complex, but also the most transparent and easy to troubleshoot.     Log in to GitHub and create a new repository (I called mine  flask-webapp-aml , initializing with a README and a .gitignore for Visual Studio files.  In Git bash on the Desktop, type into the terminal the command to clone the new repository, for example: (I'm using SSH because it will allow me to push changes back up):  git clone git@github.com:<your github username without these triangle brackets>/flask-webapp-aml.git  Copy all of the project code to this new repository folder locally (I just  cp  on the command line in Git bash) to match this structure:   FlaskAppAML/\n  FlaskAppAML/__init__.py\n  FlaskAppAML/forms.py\n  FlaskAppAML/views.py\n  FlaskAppAML/static -> *our static files*\n  FlaskAppAML/templates -> *the html page templates*\n  env/ -> *the entire python environment*\n  ptvs_virtualenv_proxy.py\n  README.md\n  requirements.txt\n  runserver.py\n  runtime.txt\n  web.config\n  .skipPythonDeployment  If the empty  .skipPythonDeployment  file is not in the base of your repository, add one now.  Also, make sure the  env  folder from the VS project is present.  This contains all of the python environment needed for running this web app (really anywhere).  We are skipping having the web service custom install all of the necessary modules by giving the service this  .skipPythonDeployment  file and the  env  folder.  Add \"__pycache__\" on it's own line to my \".gitignore\" file and anything you don't want uploaded to the GitHub when we \"push\" changes.   Now it all seems pretty tidy, so it's time to push the changes up to be hosted on GitHub.  I  add  (\"stage\"),  commit  (commit my code locally with a message) and  push  (push up to the web to be hosted on GitHub) all of my additions or any changes I've made.  I can do this  add/commit/push  again as many times as I want in the future.  I must, however, do all three consecutively and in that order otherwise it gets complicated.  So, my commands look like:   git add .  (from the base of the repository)  git commit -m \"initial commit message\"  (the -m is our message so be brief, but descriptive - visible to the world)  git push  (we could also have written  git push origin master , but it's not necessary to be so verbose right now)     Create an App Service Web App in the Azure Portal ( https://portal.azure.com ) by clicking \"+\" and search for \"web app\", then go through the wizard to create one.   Update the Deployment options in the Azure Portal for the web app.  For our Web App, under \"APP DEPLOYMENT\", open the \"Deployment options\" blade.  For Choose Source, choose GitHub (you may have to log in to your GitHub here to link it).  Under Choose project, pick the GitHub repository to which you just pushed code and click OK.  Now we add a couple of variables to the Azure Portal Web App for safe-keeping.  There are \"environmental variables\" in the code (they look like  os.environ.get('FOO') ):  one for the AML Web Service's URL and one for the API_KEY - these are the necessary values we need to access our published AML scoring experiment.  To have these available for our web app we need to put them somewhere discoverable and that is as variables under \"App settings\" in the \"Application settings\" blade for our own Web App in the Azure Portal.\n ![image of entering in keys to Azure portal app service]({{ site.baseurl }}/img/flaskapp-adding-sys-vars.jpg)  Ensure that, in the Portal, under Application Settings, Python is set to the appropriate version (default is that \"Python\" is Off in settings - so will need to manually switch to it's version).  If we go back to the \"Deployment options\" we can see how our build is going.  This process will automatically happen for us every time a new change is made to our GitHub repository.  Ensure that this build completes successfully.  Finally, if you go to \"Overview\" and click on the web app's URL, you'll see your site.   Congrats on completing this process!  You should now have a functioning barebones, machine learning web app.  Go ahead and try it out. :)   If you encounter any problems, check the Troubleshooting section below, Azure docs, or StackOverflow.  Also, leave a comment if it's a bug in the code or process.",
            "title": "Option 1: Set up a GitHub Repository as the Deployment Option"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#option-2-publish-and-deploy-from-vs-as-an-azure-app-service-web-app",
            "text": "To deploy we must also publish this project to Azure (it's done together with VS).  Fortunately, from within VS (note, I'm in VS 2017, but it's available in previous releases) there's a  \"Publish...\"  option.  Right-click on the project name and in the pop-up  \"Publish...\"  should be available.  Click this and simply go through the wizard to set up an Azure App Service Web App.  It should be very straightforward and easy to do.  As an alternative to publishing/deploying directly from VS, one can leverage a git repository or use code on GitHub as a deployment option.  Similar instructions can be found in  this  Azure article.",
            "title": "Option 2: Publish and Deploy from VS as an Azure App Service Web App"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#make-it-your-own",
            "text": "Modify the  layout.html  file with app name and navi layout changes.  Or change your custom stylesheet under static -> content -> site.css.  Go grab all of the code at  https://github.com/michhar/flask-webapp-aml  and add it to a project, test, develop and deploy.  You could even if you wish just fork this repository and deploy directly from that in the Azure Portal, but then that would have been too easy. ;)",
            "title": "Make it Your Own"
        },
        {
            "location": "/a-python-flask-webapp-gets-smart/#troubleshooting",
            "text": "All sample code can be found at  https://github.com/michhar/flask-webapp-aml  - it may, over time, have more complex samples, so check it out.  Go to Application Settings and ensure Python is enabled along with other key settings in the Azure Portal  Go to Console (under Development Tools) and make sure all files and programs specified in the  web.config  exists.  Ensure in  web.config , that the \"scriptProcessor\" key/value in handlers is correct (that these paths exist on the server file system).  Check FREB Logs in the Portal for more information around warnings and errors (make sure you are logging for those during this phase).  Post comments here or if around the code, under issues here:   https://github.com/michhar/flask-webapp-aml/issues  - many thanks!",
            "title": "Troubleshooting"
        },
        {
            "location": "/ocrbot-makes-a-connection/",
            "text": "Posted:\n  2016-11-15\n\n\nA short conversation with OCRBot on Skype - using the Bot Framework on the Azure Cloud\n\n\n\nUpdate:  November 16, 2016 Microsoft announced the \nAzure Bot Service\n in Preview.\n\n\ntl;dr\n: In \nPart 1\n I built an OCR bot using the Bot Framework (BF) from Microsoft and the Cognitive Services Computer Vision API and conversed with it using the command line on my Mac.  In this HowTo article I deploy the OCR bot to the cloud.    If you keep reading you'll learn the technical know-how to take the bot code and turn it into deployed Skype bot.\n\n\n\"Hello, you've reached [name of company].  How can I help you today?\"  --says the mechanical voice on the other end of the phone\n\n\nWe know right away that this is a bot.  But bots can be much more than a question and answer machine that deals in natural language.  In fact, it doesn't have to be language intelligence at all.\n\n\nI could, for instance, send my bot an audio clip of my favorite song and it could send back the name of the singer.  Or I could send my bot an image and it could tell me what the scene is like, who is in it, what other objects are there, etc.\n\n\nI could even leave intelligence out of it and use a bot to order a sandwich.  Bots are just apps.\n\n\nIn this case our bot uses optical character recognition (OCR) to extract text from images.  All of the code is \nhere\n.\n\n\nIt almost goes without saying, but since the sky is really the limit, it's a good idea to be thoughtful in our creation and usage of these apps or bots.  A current favorite quote is from Satya Nadella (from \nthis\n article):\n\n\nA.I. must have algorithmic accountability so that humans can undo unintended harm. We must design these technologies for the expected and the unexpected.\n\n\nNow, let's continue our story of a chat bot, ocrbot.  Ocrbot takes an image link as input and sends back the text found in that image, if any.  I could imagine, then, doing more with that text (e.g. sentiment, key phrases) or extending this bot in other ways (e.g. speech, search).\n\n\nThe Bot Framework gives me an easy way to connect my bot so that it's compatible and available on channels like Slack, Skype, Facebook Messenger, Twilio, and more.\n\n\nIn the last post (\"Building an OCR Chat Bot with the Microsoft Bot Framework on my Mac\"), we met ocrbot and chatted with this bot locally (the upper path in the diagram below).  This time we are going to deploy ocrbot to the cloud and communicate on a real channel (the lower path in the diagram below).\n\n\n\n\n\n\nMy process for connecting ocrbot to the cloud\n\n\n\n\nTo start, these are my subscriptions used:\n\n\n\n\nGithub account (free) - for hosting code\n\n\nAzure account (free trial) - for continuous cloud deployment\n\n\nMicrosoft account (free) - for Cognitive Services, BF and Skype\n\n\nCognitive Services Computer Vision API key (free) - for OCR\n\n\n\n\nAnd these are my steps at a glance:\n\n\n\n\nThe ocrbot gets forked GitHub\n\n\nFork the repo (easier to start with the existing code)\n\n\nUpdate the README to say something useful for my purposes\n\n\nThe ocrbot gets a web app service for continuous deployment\n\n\nCreate a Web App in the Azure Portal for the bot's endpoint\n\n\nChoose GitHub as my deployment source\n\n\nGet the Cognitive Services Computer Vision API key\n\n\nAdd some environment variables\n\n\nThe ocrbot gets registered on the BF\n\n\nFill out profile including url endpoint\n\n\nRecord app ID and app password after configuration\n\n\nUpdate app service with the new app ID and password\n\n\nThe ocrbot takes a test\n\n\nTest connection in BF Developer's Portal\n\n\nTest on Skype\n\n\nUpdate the bot's message on GitHub and observe the change mid-conversation\n\n\n\n\nThe ocrbot gets forked on GitHub\n\n\nI logged into \nGitHub\n and navigated to the \nbot-education-ocrbot\n repository.  Next, I forked the repository so that it would appear in my GitHub account profile.  From there, I can now use it, push/pull and annotate with markdown text.\n\n\n\n\n\n\nForking the ocrbot repository\n\n\n\n\nI like to change the README to say something specific to why I forked it like:\n\n\n\n\n\nModifying README markdown file\n\n\n\n\nThe ocrbot gets a web app service for continuous deployment\n\n\nHonestly, except for communicating on a channel with the bot, this is the coolest part in my opinion.  I've set my bot up such that any change I commit or push to my GitHub repository, reflects immediately, even if I'm mid-conversation.\n\n\nSo, since I'm using Microsoft's cloud, Azure, I signed into the Azure portal at \nhttps://portal.azure.com\n.  I then added a Web App by clicking the \"+\" icon and searching for \"web app\" (also, found under \"Web and Mobile\").\n\n\n\n\n\n\nSelecting Web App from portal menu\n\n\n\n\nI filled out all of the information and created the web app.\n\n\nI then went to my resources in the portal (the blue cube - first icon below the \"+\" icon on the left panel) and selected my newly created resource group.  In that resource group I found my web app (labelled as an \"App Service\").  It opened what we call a \"blade\" and in that I navigated to \"Deployment options\" from which I can select different sources.  In this instance I selected \"GitHub\" as in:\n\n\n\n\n\n\nSelecting GitHub from the web app deployment source blade (aka App Service) menu\n\n\n\n\nUsing this wizard, I authorized with my GitHub account credentials (through GitHub launched within the wizard) for the web app to be able to pull in my code each time a change happens. I selected my project or repo (bot-education-ocrbot in this case) and clicked \"Ok.\"  Continuous deployment deployed!\n\n\nThe final setup step in this section was to add placeholder variables for the BF app ID and password that I obtain in the next section.  This is going to make it so that the BF and my app can talk to each other.  \n\n\nTo do this I clicked on \"Application Settings\" under \"Settings\" (just below the \"Deployment options\").  This took me to a blade within which I scrolled down to \"App settings\" and entered in key-value pairs with filler text that correspond to the variable names in the \nconfiguration.js\n from my project (so, \nMICROSOFT_APP_ID\n and \nMICROSOFT_APP_PASSWORD\n).  I didn't need to do it right at that point, but thought it'd be a good idea so I didn't overlook later (a string on my finger):\n\n\n\n\n\n\nApp environment variables which correspond to the environment variables in ocrbot's config file\n\n\n\n\nThe actual values will be filled in in the next section.\n\n\nAlso, in this settings blade, I  created a variable corresponding to my Cognitive Services Computer Vision API key so I could use their OCR service.  Therefore, I entered in a third variable, the \nVISION_API_KEY\n below my other two.  I set it to my actual, real key from my subscription.  \n\n\n\n\nTo get this free key, btw, I simply went to the Cognitive Services APIs \nwebsite\n, My Account (I used my Microsoft Account - used to be called Live ID - which is just my gmail account linked up; if I had an xbox, hotmail, or outlook.com account I would already have one), and signed up for a trial subscription for Computer Vision.  It's just the free tier of a Microsoft service.\n\n\n\n\nMake sure to then save the settings in this blade.\n\n\nThe ocrbot gets registered on the BF\n\n\nThis part is pretty painless.  Basically, we go to the Bot Framework Developer's Portal (same site where the docs and Bot Directory live) at \nhttps://dev.botframework.com\n, fill out a profile, do a small config and that's it.\n\n\nI called my bot, ocrbot (doesn't have to be unique) and gave it a public handle, ocrbot100 (has to be globally unique).\n\n\n\n\n\n\nMy ocrbot's profile\n\n\n\n\nFor the Configuration part, the messaging endpoint is the web app service URL (I went back to the Azure portal to grab this URL from the web app service - In \"Overview\") appended \"/api/messages\" to the end of it and changed \nhttp\n to \nhttps\n, all of which so that the Bot Connector can route the messages correctly (the Bot Connector is a component of the BF which handles many things including routing messages).  For me this was something like:  \nhttps://nameofwebapp.azurewebsites.net/api/messages\n.\n\n\nAlso in the configuration part, a wizard took me through the process of getting the app ID and password and I just had to make sure to record the password in the pop-up and the app ID on the profile page.  Yup, that's the same app ID and password I set up dummy environment variables earlier in the Azure portal.  Now they will have real values.  \n\n\nExcept for pasting these values back into the Azure portal, the registration in the BF Developer's portal is done.  So, I went ahead and did the pasting.\n\n\nThe ocrbot takes a test\n\n\nFinally, the really fun part:  here, I got to check my bot's connection and then have a real conversation.  Back in the BF Dev Portal I went to \"Test connection to your bot\" and clicked on the \"Test\" button as shown here which pings my bot's messaging endpoint to confirm a connection.\n\n\n\n\n\n\nTesting in the Developer's Portal\n\n\n\n\nI finally and with excitement scrolled down on the page shown above and clicked on \"Add to Skype.\"  After launching Skype (I had to make sure I was logged into Skype with the same Microsoft ID I was using in the Dev Portal) I tried sending some messages:  a greeting and some image URLs from the web.  I was curious to see if ocrbot liked Johnny Cash.  Why not?\n\n\n\n\n\n\nocrbot goes country - or at least reads country song lyrics from an image\n\n\n\n\nTo test the nifty continuous deployment from GitHub, I changed ocrbot's message on GitHub and sync'd that repository in the Azure Portal (under the web app service and \"Deployment Options\").  This happened mid-conversation:\n\n\n\n\n\n\nocrbot's message updated mid-conversation\n\n\n\n\nWell, that's it folks.  To recap, ocrbot and I accomplished:\n\n\n\n\nForking the original ocrbot repository from GitHub into my GitHub\n\n\nDeploying ocrbot as a web app service on Azure\n\n\nRegistering ocrbot with the Bot Framework\n\n\nTaking ocrbot out for a spin on Skype\n\n\n\n\nOcrbot stays busy in the next topics in this blog series:\n\n\n\n\nOcrbot makes a friend (on Slack)\n\n\nOcrbot gets attached (attachments)\n\n\nOcrbot learns to talk (speech APIs)\n\n\nOcrbot goes to the store (bring your own data)",
            "title": "OCRBot Makes a Connection to the Cloud"
        },
        {
            "location": "/ocrbot-makes-a-connection/#the-ocrbot-gets-forked-on-github",
            "text": "I logged into  GitHub  and navigated to the  bot-education-ocrbot  repository.  Next, I forked the repository so that it would appear in my GitHub account profile.  From there, I can now use it, push/pull and annotate with markdown text.    Forking the ocrbot repository   I like to change the README to say something specific to why I forked it like:   Modifying README markdown file",
            "title": "The ocrbot gets forked on GitHub"
        },
        {
            "location": "/ocrbot-makes-a-connection/#the-ocrbot-gets-a-web-app-service-for-continuous-deployment",
            "text": "Honestly, except for communicating on a channel with the bot, this is the coolest part in my opinion.  I've set my bot up such that any change I commit or push to my GitHub repository, reflects immediately, even if I'm mid-conversation.  So, since I'm using Microsoft's cloud, Azure, I signed into the Azure portal at  https://portal.azure.com .  I then added a Web App by clicking the \"+\" icon and searching for \"web app\" (also, found under \"Web and Mobile\").    Selecting Web App from portal menu   I filled out all of the information and created the web app.  I then went to my resources in the portal (the blue cube - first icon below the \"+\" icon on the left panel) and selected my newly created resource group.  In that resource group I found my web app (labelled as an \"App Service\").  It opened what we call a \"blade\" and in that I navigated to \"Deployment options\" from which I can select different sources.  In this instance I selected \"GitHub\" as in:    Selecting GitHub from the web app deployment source blade (aka App Service) menu   Using this wizard, I authorized with my GitHub account credentials (through GitHub launched within the wizard) for the web app to be able to pull in my code each time a change happens. I selected my project or repo (bot-education-ocrbot in this case) and clicked \"Ok.\"  Continuous deployment deployed!  The final setup step in this section was to add placeholder variables for the BF app ID and password that I obtain in the next section.  This is going to make it so that the BF and my app can talk to each other.    To do this I clicked on \"Application Settings\" under \"Settings\" (just below the \"Deployment options\").  This took me to a blade within which I scrolled down to \"App settings\" and entered in key-value pairs with filler text that correspond to the variable names in the  configuration.js  from my project (so,  MICROSOFT_APP_ID  and  MICROSOFT_APP_PASSWORD ).  I didn't need to do it right at that point, but thought it'd be a good idea so I didn't overlook later (a string on my finger):    App environment variables which correspond to the environment variables in ocrbot's config file   The actual values will be filled in in the next section.  Also, in this settings blade, I  created a variable corresponding to my Cognitive Services Computer Vision API key so I could use their OCR service.  Therefore, I entered in a third variable, the  VISION_API_KEY  below my other two.  I set it to my actual, real key from my subscription.     To get this free key, btw, I simply went to the Cognitive Services APIs  website , My Account (I used my Microsoft Account - used to be called Live ID - which is just my gmail account linked up; if I had an xbox, hotmail, or outlook.com account I would already have one), and signed up for a trial subscription for Computer Vision.  It's just the free tier of a Microsoft service.   Make sure to then save the settings in this blade.",
            "title": "The ocrbot gets a web app service for continuous deployment"
        },
        {
            "location": "/ocrbot-makes-a-connection/#the-ocrbot-gets-registered-on-the-bf",
            "text": "This part is pretty painless.  Basically, we go to the Bot Framework Developer's Portal (same site where the docs and Bot Directory live) at  https://dev.botframework.com , fill out a profile, do a small config and that's it.  I called my bot, ocrbot (doesn't have to be unique) and gave it a public handle, ocrbot100 (has to be globally unique).    My ocrbot's profile   For the Configuration part, the messaging endpoint is the web app service URL (I went back to the Azure portal to grab this URL from the web app service - In \"Overview\") appended \"/api/messages\" to the end of it and changed  http  to  https , all of which so that the Bot Connector can route the messages correctly (the Bot Connector is a component of the BF which handles many things including routing messages).  For me this was something like:   https://nameofwebapp.azurewebsites.net/api/messages .  Also in the configuration part, a wizard took me through the process of getting the app ID and password and I just had to make sure to record the password in the pop-up and the app ID on the profile page.  Yup, that's the same app ID and password I set up dummy environment variables earlier in the Azure portal.  Now they will have real values.    Except for pasting these values back into the Azure portal, the registration in the BF Developer's portal is done.  So, I went ahead and did the pasting.",
            "title": "The ocrbot gets registered on the BF"
        },
        {
            "location": "/ocrbot-makes-a-connection/#the-ocrbot-takes-a-test",
            "text": "Finally, the really fun part:  here, I got to check my bot's connection and then have a real conversation.  Back in the BF Dev Portal I went to \"Test connection to your bot\" and clicked on the \"Test\" button as shown here which pings my bot's messaging endpoint to confirm a connection.    Testing in the Developer's Portal   I finally and with excitement scrolled down on the page shown above and clicked on \"Add to Skype.\"  After launching Skype (I had to make sure I was logged into Skype with the same Microsoft ID I was using in the Dev Portal) I tried sending some messages:  a greeting and some image URLs from the web.  I was curious to see if ocrbot liked Johnny Cash.  Why not?    ocrbot goes country - or at least reads country song lyrics from an image   To test the nifty continuous deployment from GitHub, I changed ocrbot's message on GitHub and sync'd that repository in the Azure Portal (under the web app service and \"Deployment Options\").  This happened mid-conversation:    ocrbot's message updated mid-conversation   Well, that's it folks.  To recap, ocrbot and I accomplished:   Forking the original ocrbot repository from GitHub into my GitHub  Deploying ocrbot as a web app service on Azure  Registering ocrbot with the Bot Framework  Taking ocrbot out for a spin on Skype   Ocrbot stays busy in the next topics in this blog series:   Ocrbot makes a friend (on Slack)  Ocrbot gets attached (attachments)  Ocrbot learns to talk (speech APIs)  Ocrbot goes to the store (bring your own data)",
            "title": "The ocrbot takes a test"
        },
        {
            "location": "/how-to-bot-on-mac/",
            "text": "Posted:\n  2016-11-07\n\n\nCommand Line Emulator for the Bot Framework - interacting with ocrbot on Shakespeare\n\n\n\n\nUPDATE:  The Command Line Emulator has been replaced with a full-fledged emulator which is cross-platform (info and install \nhere\n).\n\n\ntl;dr\n: I built a simple OCR bot using the Bot Framework (BF) from Microsoft and the Cognitive Services Computer Vision API.  I chose the Node.js Bot Builder SDK from BF.  I tested with the BF's unix-compatible emulator (black box above).  It was nice and straightforward the whole way through.  All of the instructions are \nhere\n, but feel free to keep reading.\n\n\nThere's really almost too much to say about chat bots:  definitions, bot builders, history (\nmore\n), warnings and cautionary tales (\nmore\n), guidelines, delightful stories, sad stories, etc.  For our purposes, other than a few considerations, I'll sum it up with: they've been around for decades and will be for many more.\n\n\nHowever, let's cover some vocabulary to start.\n\n\nA blog post \nhere\n details the different types of chat bots, progressing from simple to ones that can hold down a conversation.  These are the bot types discussed and an example to illustrate:\n\n\n\n\nNotifier - sends a one-way message e.g. \nping me with today's weather forecast at the start of the day (\"push\" bot i.e. bot initiates)\n\n\nReactor - replies when I send a message, but does not remember anything \ne.g. send me the weather forecast when I ask for it (\"pull bot\" i.e. I initiate), but don't remember me or what I ask for\n\n\nResponder - replies and remembers me and my message history e.g. \nsend me today's weather forecast, use my user name on this channel, and remember what cities I choose\n\n\nConversationalist - replies, remembers me and my message history, knows what service I'm on, if there are others there, and when I come and go e.g. \nsend me today's weather forecast, use my user name on this channel, remember what cities I choose, format it nicely for this channel, and if the conversation is old, archive it and send as email\n\n\n\n\nBot builders lower the activation barrier for developing bots and the MS Bot Framework (BF) Bot Builder SDKs give us a wealth of methods for building dialog and user prompts, making the creation of effective waterfalls really easy.  Along with the SDKs, the BF provides free emulator tools, a Windows-compatible desktop app and a Mac OS X / Linux-compatible console app (more information on emulators \nhere\n).\n\n\nI know you've been waiting to dive into the code, so let's begin...\n\n\nThere are two choices on bot builder functions for testing locally.  We can use the \nConsoleConnector\n which simply and directly allows us to run our Node.js code on the command line.  Using the bot builder SDK our code is pretty concise (see more examples on the Core Concepts page for the BF \nhere\n):\n\n\nvar\n \nbuilder\n \n=\n \nrequire\n(\n'botbuilder'\n);\n\n\n\n// Create a bot and connect it to the console\n\n\nvar\n \nconnector\n \n=\n \nnew\n \nbuilder\n.\nConsoleConnector\n().\nlisten\n();\n\n\nvar\n \nbot\n \n=\n \nnew\n \nbuilder\n.\nUniversalBot\n(\nconnector\n);\n\n\n\n\n\n\nInteracting with ocrbot could look like:\n\n\n\n\nIt's simple, but we don't get to see the actual JSON that gets passed to the bot and the JSON passed back.  If we want to be able to see the message and also write code that can be used for production later, the Bot Framework Emulator is the way to go.   Note, this is the beginning of my server.js Node.js file - see my \nocrbot github repo\n for the complete project and code and the included \nlab file\n for more instructions on doing this at home.  We replace  \nConsoleConnector\n with \nChatConnector\n, for a full deployment-compatible setup, as follows:\n\n\nvar\n \nrestify\n \n=\n \nrequire\n(\n'restify'\n);\n\n\nvar\n \nbuilder\n \n=\n \nrequire\n(\n'botbuilder'\n);\n\n\n\n// Create bot\n\n\nvar\n \nconnector\n \n=\n \nnew\n \nbuilder\n.\nChatConnector\n(\nbotConnectorOptions\n);\n\n\nvar\n \nbot\n \n=\n \nnew\n \nbuilder\n.\nUniversalBot\n(\nconnector\n);\n\n\n\n// Setup Restify Server\n\n\nvar\n \nserver\n \n=\n \nrestify\n.\ncreateServer\n();\n\n\n\n// Handle Bot Framework messages\n\n\nserver\n.\npost\n(\n'/api/messages'\n,\n \nconnector\n.\nlisten\n());\n\n\n\n// Serve a static web page - for testing deployment\n\n\nserver\n.\nget\n(\n/.*/\n,\n \nrestify\n.\nserveStatic\n({\n\n    \n'directory'\n:\n \n'.'\n,\n\n    \n'default'\n:\n \n'index.html'\n\n\n}));\n\n\n\nserver\n.\nlisten\n(\nprocess\n.\nenv\n.\nport\n \n||\n \nprocess\n.\nenv\n.\nPORT\n \n||\n \n3978\n,\n \nfunction\n \n()\n \n{\n\n    \nconsole\n.\nlog\n(\n'%s listening to %s'\n,\n \nserver\n.\nname\n,\n \nserver\n.\nurl\n);\n \n\n});\n\n\n\n\n\n\nOne of the major reasons I used github to host this project is that (and outlined in a later blog, TBD) it afforded me the ability to do a continuous deployment directly from the repo.  Any change I push up, immediately reflects in my bot on whichever channel I'm on - it was actually pretty astonishing to see a quick typo fix show up so immediately when chatting to my bot at the same time as pushing up the change.  But I'll save this for the deployment article to come.\n\n\nI've always been a fan of command terminals, so even on a Windows machine I'd probably choose to download and use the BF Command Terminal Emulator (download instructions are \nhere\n).  Honestly, I enjoy the simplicity and control a command prompt affords.  And now I can develop bots in the \nexact\n same way agnostic of OS.\n\n\nSo, I decided to create a bot I could give image links to and if there was text in the picture it would reply with that text.  Pretty simple and straightforward since I knew about the free subscriptions to Microsoft Cognitive Services and in particular the Vision APIs.  I went to the Cognitive Services main page and clicked on My Account and signed up (or I could have signed up \nthis way\n with MS, github, or LinkedIn accounts).  After that I had a secret key for the service.  Now to splice that into my bot.\n\n\nSo, I borrowed much of my code from Samuele Resca's \nblog post\n (excellent blog, btw).  I placed these helper methods in with the server.js code above:\n\n\n//=========================================================\n\n\n// URL Helpers\n\n\n//=========================================================\n\n\n\n\nvar\n \nextractUrl\n \n=\n \nfunction\n \n_extractUrl\n(\nmessage\n)\n \n{\n\n\n    \nif\n \n(\nmessage\n.\ntype\n \n!==\n \n\"message\"\n)\n \nreturn\n;\n\n\n    \nif\n \n(\ntypeof\n \nmessage\n.\nattachments\n \n!==\n \n\"undefined\"\n\n        \n&&\n \nmessage\n.\nattachments\n.\nlength\n \n>\n \n0\n)\n \n{\n\n        \nreturn\n \nmessage\n.\nattachments\n[\n0\n].\ncontentUrl\n;\n\n    \n}\n\n\n    \nif\n \n(\ntypeof\n \nmessage\n.\ntext\n \n!==\n \n\"\"\n)\n \n{\n\n        \nreturn\n \n_findUrl\n(\nmessage\n.\ntext\n);\n\n    \n}\n\n\n    \nreturn\n \n\"\"\n;\n\n\n};\n\n\n\n\nfunction\n \n_findUrl\n(\ntext\n)\n \n{\n\n    \nvar\n \nsource\n \n=\n \n(\ntext\n \n||\n \n''\n).\ntoString\n();\n\n    \nvar\n \nmatchArray\n;\n\n\n    \n// Regular expression to find FTP, HTTP(S) and email URLs.\n\n    \nvar\n \nregexToken\n \n=\n \n/(((http|https?):\\/\\/)[\\-\\w@:%_\\+.~#?,&\\/\\/=]+)/g\n;\n\n\n    \n// Iterate through any URLs in the text.\n\n    \nif\n \n((\nmatchArray\n \n=\n \nregexToken\n.\nexec\n(\nsource\n))\n \n!==\n \nnull\n)\n \n{\n\n        \nvar\n \ntoken\n \n=\n \nmatchArray\n[\n0\n];\n\n        \nreturn\n \ntoken\n;\n\n    \n}\n\n\n    \nreturn\n \n\"\"\n;\n\n\n}\n\n\n\n\n\n\nThen I set up a config file as shown in the \nrepository\n containing my Computer Vision API key (as well as placeholders for the future app id and app password I get after the deployment step - in a followup article TBD).  So, don't worry about the app id and app password for now.\n\n\nIf you are doing this at home, replace the \nprocess.env.VISION_API_KEY\n with a string holding your key (this is a separate file I called \nconfiguration.js\n), see instructions in this \nlab file\n.\n\n\nvar\n \n_config\n \n=\n \n{\n\n    \n// Ignore this part for now...\n\n    \nCHAT_CONNECTOR\n:\n \n{\n\n        \nAPP_ID\n:\n \nprocess\n.\nenv\n.\nMICROSOFT_APP_ID\n,\n \n//You can obtain an APP ID and PASSWORD here: https://dev.botframework.com/bots/new\n\n        \nAPP_PASSWORD\n:\n \nprocess\n.\nenv\n.\nMICROSOFT_APP_PASSWORD\n\n    \n},\n\n\n    \n//  Replace the API_KEY below with yours from the free trial\n\n    \nCOMPUTER_VISION_SERVICE\n:\n \n{\n\n        \nAPI_URL\n:\n \n\"https://api.projectoxford.ai/vision/v1.0/\"\n,\n\n        \nAPI_KEY\n:\n \nprocess\n.\nenv\n.\nVISION_API_KEY\n  \n//You can obtain an COGNITIVE SERVICE API KEY: https://www.microsoft.com/cognitive-services/en-us/pricing\n\n    \n}\n\n\n};\n\n\nexports\n.\nCONFIGURATIONS\n \n=\n \n_config\n;\n\n\n\n\n\n\nThen back in my \nserver.js\n file with the main code I set up my vision OCR functions to read the image text (basically call out the the Computer Vision service with an image url) and then process the text:\n\n\n//=========================================================\n\n\n// Vision Service\n\n\n//=========================================================\n\n\n\nvar\n \nrequest\n \n=\n \nrequire\n(\n\"request\"\n);\n\n\n\nvar\n \nreadImageText\n \n=\n \nfunction\n \n_readImageText\n(\nurl\n,\n \ncallback\n)\n \n{\n\n\n    \nvar\n \noptions\n \n=\n \n{\n\n        \nmethod\n:\n \n'POST'\n,\n\n        \nurl\n:\n \nconfig\n.\nCONFIGURATIONS\n.\nCOMPUTER_VISION_SERVICE\n.\nAPI_URL\n \n+\n \n\"ocr/\"\n,\n\n        \nheaders\n:\n \n{\n\n            \n'ocp-apim-subscription-key'\n:\n \nconfig\n.\nCONFIGURATIONS\n.\nCOMPUTER_VISION_SERVICE\n.\nAPI_KEY\n,\n\n            \n'content-type'\n:\n \n'application/json'\n\n        \n},\n\n        \nbody\n:\n \n{\nurl\n:\n \nurl\n,\n \nlanguage\n:\n \n\"en\"\n},\n\n        \njson\n:\n \ntrue\n\n    \n};\n\n\n    \nrequest\n(\noptions\n,\n \ncallback\n);\n\n\n\n};\n\n\n\nvar\n \nextractText\n \n=\n \nfunction\n \n_extractText\n(\nbodyMessage\n)\n \n{\n\n\n    \nif\n \n(\ntypeof\n \nbodyMessage\n.\nregions\n \n===\n \n\"undefined\"\n)\n \nreturn\n \n\"\"\n;\n\n\n    \nvar\n \nregs\n \n=\n \nbodyMessage\n.\nregions\n;\n\n\n    \nif\n \n(\ntypeof\n \nregs\n[\n0\n]\n \n!==\n \n\"undefined\"\n \n&&\n\n        \nregs\n[\n0\n].\nlines\n.\nlength\n \n>\n \n0\n)\n \n{\n\n\n        \ntext\n \n=\n \n\"\"\n;\n\n\n        \nvar\n \nlines\n \n=\n \nregs\n[\n0\n].\nlines\n;\n\n\n        \n// For all lines in image ocr result\n\n        \n//   grab the text in the words array\n\n        \nfor\n \n(\ni\n \n=\n \n0\n;\n \ni\n \n<\n \nlines\n.\nlength\n;\n \ni\n++\n)\n \n{\n\n            \nvar\n \nwords\n \n=\n \nlines\n[\ni\n].\nwords\n;\n\n            \nfor\n \n(\nj\n \n=\n \n0\n;\n \nj\n \n<\n \nwords\n.\nlength\n;\n \nj\n++\n)\n \n{\n\n                \ntext\n \n+=\n \n\" \"\n \n+\n \nwords\n[\nj\n].\ntext\n \n+\n \n\" \"\n;\n\n            \n}\n\n        \n}\n\n\n        \nreturn\n \ntext\n;\n\n    \n}\n\n\n    \nreturn\n \n\"Sorry, I can't find text in it :( !\"\n;\n\n\n};\n\n\n\n\n\n\nFinally, we tie it all together in a user dialog:\n\n\n//=========================================================\n\n\n// Bots Dialogs\n\n\n//=========================================================\n\n\n\nbot\n.\ndialog\n(\n'/'\n,\n \nfunction\n \n(\nsession\n)\n \n{\n\n\n    \n// Use our url helper method\n\n    \nvar\n \nextractedUrl\n \n=\n \nextractUrl\n(\nsession\n.\nmessage\n);\n\n\n    \n// Nothing returned?  Ask for an image url again...\n\n    \nif\n \n(\nextractedUrl\n \n===\n \n\"\"\n)\n \n{\n\n        \nsession\n.\nsend\n(\n\"Hello.  I'm an OCRBot.  Please give me an image link and I'll look for words.\"\n);\n\n    \n}\n\n\n    \n// Get a valid url?  Send it off to the Vision API for OCR...\n\n    \nreadImageText\n(\nextractedUrl\n,\n \nfunction\n \n(\nerror\n,\n \nresponse\n,\n \nbody\n)\n \n{\n\n        \nsession\n.\nsend\n(\nextractText\n(\nbody\n));\n\n    \n})\n\n\n});\n\n\n\n\n\n\nWe could have added a waterfall here to say confirm they want to process this image or perhaps added a built-in prompt that allows uploading an image attachment.  Perhaps in future releases.\n\n\nNow we test.\n\n\nOn Windows one would double click on the BFEmulator.exe directly and on Mac we use the Mono framework software to run BFEmulator.exe from the command line.  Pretty easy, peasy.\n\n\nLet's take this image:\n\n\n\n\nThe emulator experience will look a little something like:\n\n\n\n\nCommand Line Emulator for the Bot Framework - interacting with ocrbot locally on stats\n\n\nIn conclusion, to get started now, check out the github repo and start playing with the code and emulator.  For more instructions check out \nthis lab\n.  Chat later!",
            "title": "Building an OCR Chat Bot with the Microsoft Bot Framework on my Mac"
        },
        {
            "location": "/teaching-notes-post/",
            "text": "Posted:\n  2016-10-24\n\n\nTeaching notes\n\n\nHave the attendees introduce themselves to their neighbor(s)\n\n\nThe isolation and separateness that ones feels in a room of mostly strangers, about to embark on a few hours or a few days of training can melt away in the matter of 5 minutes.  In this technique, for about 5 minutes at the very beginning before any presentation begins, I ask everyone to take a moment and introduce themselves to their neighbors(s).  I encourage them to state their name, role and an interesting fact about themselves.\n\n\nThis technique forms very useful connections that will serve in the future when one person may need help or a discussion needs to take place to formulate a question to the instructor.  When the hands-on-labs role around, the students now know some of their neighbors and can work together on a problem.  When it comes time for a build-a-thon or hackathon, the students will likely feel more comfortable forming groups.\n\n\nLead with a motivating example or at least \"what you will be able to do by the end\"\n\n\nIn the best scenario I have a demo that wows the audience or at least immediately gets their attention.  The demo could be a Power BI dashboard driven by streaming data flowing in from a weather station.  It could be a service that allows me to upload a picture and returns the contents of that image in natural language (like \nCaptionBot\n).  It could be a fast process, such as running a large machine learning computation out-of-core on a single laptop, lightning fast, perhaps predicting tens of thousands of numeric labels on a hand-written data set of digits using a trained classificaion model.  \n\n\nSeeing something run and produce an amazingly quick and/or fascinating result or a dashboard powered by fast moving data streaming in, real-time, can definitely inspire and motivate those who like that sort of thing.\n\n\nAlways have advanced exercises for students who want to go ahead\n\n\nI feel one of the worse outcomes is not being challenged.  We mostly all like a good challenge and in the end, for me at least, it gives a feeling of accomplishment.  I learn best when I am challenged to think and/or be creative.  In the hands-on-labs this is especially important.  And it's really easy to incorporate \nAdvanced Exercises\n into the lab instructions.  Then I simply let them work and encourage them to keep going if they finish something earlier than others.  Just remember, it's got to be motivating and fun.  Basically, I try to always peak their curiosity and keep it rolling.\n\n\nMake mistakes and be a real person\n\n\nThis helps normalize the instructor to the students, removing barriers that might be there or percieved.  It humanizes the instructor and puts the students at ease knowing they too can explore their studies and make mistakes.  After all, the instructor did this in front of everyone.\n\n\nWhen I make a mistake, I admit it and I ask for help from the audience.  Also, when I forget something, can't remember a detail or am stumped, I pose the question to the audience to get them both thinking and my answer back (hopefully).  In this way, it serves many purposes:  engagement by having a group exercise and audience participation, normalizing everyone (especially the instructor to the students) and getting the answer.\n\n\nSticky notes (red = I need help or I'm not ready to move on, another color = no problem here)\n\n\nI borrowed this idea from Michael Levy (see Credits at the bottom) who teaches R at scale.  The basic premise is to allow students to indicate a problem, be it I'm moving too fast, they don't fully understand something, they need more time or have a quesion.  The red sticky note is then placed on the front of the laptop so that I can see it from the front of the room.  It's mainly useful in the hands-on-labs portions or the build-a-thon/hackathon.  This way students don't have to keep a hand raised or feel they are intruding or disturbing a class.  It equalizes things a little bit I believe, adding to the ease and freedom to ask a quesion or ask for help.  It's been popular and successful in the past especially in situations where the room is very quiet (like in a self-paced, self-guided tutorial).\n\n\nAt the end you may have the students leave the sticky notes (a red is dissatisfied and the other color is satisfied) in an anonymous spot.\n\n\nSome attendees expressing their appreciation:\n\n\n\n\nCredits\n\n\nI have to give Michael Levy credit for many of these ideas from his fantastic blog post on \nTeaching R to 200 students in a week\n \nhere\n.\n\n\n\n\n\n\n\n\n\n\n/**\n*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/\n/*\nvar disqus_config = function () {\nthis.page.url = http://127.0.0.1:8000/teaching-notes-post/;  // Replace PAGE_URL with your page's canonical URL variable\nthis.page.identifier = teaching-notes-post; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n};\n*/\n(function() { // DON'T EDIT BELOW THIS LINE\nvar d = document, s = d.createElement('script');\ns.src = '//michhar2.disqus.com/embed.js';\ns.setAttribute('data-timestamp', +new Date());\n(d.head || d.body).appendChild(s);\n})();\n\n\n\n\nPlease enable JavaScript to view the \ncomments powered by Disqus.",
            "title": "Tips I have Learned by Being a Trainer for a Year"
        },
        {
            "location": "/teaching-notes-post/#teaching-notes",
            "text": "",
            "title": "Teaching notes"
        },
        {
            "location": "/teaching-notes-post/#have-the-attendees-introduce-themselves-to-their-neighbors",
            "text": "The isolation and separateness that ones feels in a room of mostly strangers, about to embark on a few hours or a few days of training can melt away in the matter of 5 minutes.  In this technique, for about 5 minutes at the very beginning before any presentation begins, I ask everyone to take a moment and introduce themselves to their neighbors(s).  I encourage them to state their name, role and an interesting fact about themselves.  This technique forms very useful connections that will serve in the future when one person may need help or a discussion needs to take place to formulate a question to the instructor.  When the hands-on-labs role around, the students now know some of their neighbors and can work together on a problem.  When it comes time for a build-a-thon or hackathon, the students will likely feel more comfortable forming groups.",
            "title": "Have the attendees introduce themselves to their neighbor(s)"
        },
        {
            "location": "/teaching-notes-post/#lead-with-a-motivating-example-or-at-least-what-you-will-be-able-to-do-by-the-end",
            "text": "In the best scenario I have a demo that wows the audience or at least immediately gets their attention.  The demo could be a Power BI dashboard driven by streaming data flowing in from a weather station.  It could be a service that allows me to upload a picture and returns the contents of that image in natural language (like  CaptionBot ).  It could be a fast process, such as running a large machine learning computation out-of-core on a single laptop, lightning fast, perhaps predicting tens of thousands of numeric labels on a hand-written data set of digits using a trained classificaion model.    Seeing something run and produce an amazingly quick and/or fascinating result or a dashboard powered by fast moving data streaming in, real-time, can definitely inspire and motivate those who like that sort of thing.",
            "title": "Lead with a motivating example or at least \"what you will be able to do by the end\""
        },
        {
            "location": "/teaching-notes-post/#always-have-advanced-exercises-for-students-who-want-to-go-ahead",
            "text": "I feel one of the worse outcomes is not being challenged.  We mostly all like a good challenge and in the end, for me at least, it gives a feeling of accomplishment.  I learn best when I am challenged to think and/or be creative.  In the hands-on-labs this is especially important.  And it's really easy to incorporate  Advanced Exercises  into the lab instructions.  Then I simply let them work and encourage them to keep going if they finish something earlier than others.  Just remember, it's got to be motivating and fun.  Basically, I try to always peak their curiosity and keep it rolling.",
            "title": "Always have advanced exercises for students who want to go ahead"
        },
        {
            "location": "/teaching-notes-post/#make-mistakes-and-be-a-real-person",
            "text": "This helps normalize the instructor to the students, removing barriers that might be there or percieved.  It humanizes the instructor and puts the students at ease knowing they too can explore their studies and make mistakes.  After all, the instructor did this in front of everyone.  When I make a mistake, I admit it and I ask for help from the audience.  Also, when I forget something, can't remember a detail or am stumped, I pose the question to the audience to get them both thinking and my answer back (hopefully).  In this way, it serves many purposes:  engagement by having a group exercise and audience participation, normalizing everyone (especially the instructor to the students) and getting the answer.",
            "title": "Make mistakes and be a real person"
        },
        {
            "location": "/teaching-notes-post/#sticky-notes-red-i-need-help-or-im-not-ready-to-move-on-another-color-no-problem-here",
            "text": "I borrowed this idea from Michael Levy (see Credits at the bottom) who teaches R at scale.  The basic premise is to allow students to indicate a problem, be it I'm moving too fast, they don't fully understand something, they need more time or have a quesion.  The red sticky note is then placed on the front of the laptop so that I can see it from the front of the room.  It's mainly useful in the hands-on-labs portions or the build-a-thon/hackathon.  This way students don't have to keep a hand raised or feel they are intruding or disturbing a class.  It equalizes things a little bit I believe, adding to the ease and freedom to ask a quesion or ask for help.  It's been popular and successful in the past especially in situations where the room is very quiet (like in a self-paced, self-guided tutorial).  At the end you may have the students leave the sticky notes (a red is dissatisfied and the other color is satisfied) in an anonymous spot.  Some attendees expressing their appreciation:",
            "title": "Sticky notes (red = I need help or I'm not ready to move on, another color = no problem here)"
        },
        {
            "location": "/teaching-notes-post/#credits",
            "text": "I have to give Michael Levy credit for many of these ideas from his fantastic blog post on  Teaching R to 200 students in a week   here .    \n\n/**\n*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/\n/*\nvar disqus_config = function () {\nthis.page.url = http://127.0.0.1:8000/teaching-notes-post/;  // Replace PAGE_URL with your page's canonical URL variable\nthis.page.identifier = teaching-notes-post; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n};\n*/\n(function() { // DON'T EDIT BELOW THIS LINE\nvar d = document, s = d.createElement('script');\ns.src = '//michhar2.disqus.com/embed.js';\ns.setAttribute('data-timestamp', +new Date());\n(d.head || d.body).appendChild(s);\n})();  Please enable JavaScript to view the  comments powered by Disqus.",
            "title": "Credits"
        },
        {
            "location": "/notebooks1-post/",
            "text": "Posted:\n 2016-10-07\n\n\nI had a great time interacting with some of the most motivated people I've ever met at the \nData Science Summit\n in Atlanta, GA, on September 26 and 27\nth\n.  \n\n\nThe attendees showed up to learn new skills in the analytics domain.  A colleague and I delivered two courses for the price of one (well, at the very least they were in the same room - choose your own adventure) and we saw people so motivated, they worked for a full 3 hour session, heads down, and with an intense drive.  The courses were self-paced and self-guided with us there as proctors.  The other course in the room was on Microsoft R Server and SQL Server R Services, however my expertise lay in \nPython for Data Science\n, a course I created and that was recently released on the landing page of \nhttps://notebooks.azure.com\n.  This course's audience includes those:\n\n\n\n\nNew to Python\n\n\nExperienced with Python, but not data science\n\n\nIn need of a refresher\n\n\nHoping to learn a little about the Python 2 to 3 change\n\n\nIn need of a handy modular reference\n\n\n...\n\n\n\n\nAzure Notebooks was released the first day of the courses and used for the 13-module \nPython for Data Science\n course covering,\n\n\nfor those new to Python,\n\n\n\n\nBasics\n\n\nData Structures\n\n\nFunctional Programming\n\n\nSorting and Pattern Matching\n\n\nObject Oriented Programming\n\n\nBasic Difference from 2 to 3\nand for the DS,\n\n\nNumerical Computing\n\n\nData Analysis with pandas I\n\n\nData Analysis with pandas II\n\n\nMachine Learning I - ML Basics and Data Exploration\n\n\nMachine Learning II - Supervised and Unsupervised Learning\n\n\nMachine Learning III - Parameter Tuning and Model Evaluation\n\n\nVisualization\n\n\n\n\nThis course includes the gambit.  From list comprehension:\n\n\n# Solution to list comprehension exercise\n\n\n# Solution to list comprehension exercise\n\n\nletters\n \n=\n \nlist\n(\n'thepurposeoflife'\n)\n\n\n\n# Place your list comprehension below\n\n\noutput\n \n=\n \n[\nx\n.\nupper\n()\n \nif\n \nx\n \nis\n \n'e'\n \nelse\n \nx\n \nfor\n \nx\n \nin\n \nletters\n]\n\n\n\nprint\n(\n''\n.\njoin\n(\noutput\n))\n\n\n\n\n\nWhich gets you: \nthEpurposEoflifE\n\n\nTo anomaly detection with a one-class SVM in \nscikit-learn\n:\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\nimport\n \nmatplotlib.font_manager\n\n\nfrom\n \nsklearn\n \nimport\n \nsvm\n\n\n\nfrom\n \nsklearn.datasets\n \nimport\n \nload_iris\n\n\nfrom\n \nsklearn.cross_validation\n \nimport\n \ntrain_test_split\n\n\n\n\n# Iris data\n\n\niris\n \n=\n \nload_iris\n()\n\n\nX\n,\n \ny\n \n=\n \niris\n.\ndata\n,\n \niris\n.\ntarget\n\n\n\n# Taking only two columns...\n\n\nlabels\n \n=\n \niris\n.\nfeature_names\n[\n1\n:\n3\n]\n\n\nX\n \n=\n \nX\n[:,\n \n1\n:\n3\n]\n\n\n\n# split\n\n\nX_train\n,\n \nX_test\n,\n \ny_train\n,\n \ny_test\n \n=\n \ntrain_test_split\n(\nX\n,\n \ny\n,\n \ntest_size\n \n=\n \n0.3\n,\n \n    \nrandom_state\n \n=\n \n0\n)\n\n\n\n# make some outliers\n\n\nX_weird\n \n=\n \nnp\n.\nrandom\n.\nuniform\n(\nlow\n=-\n2\n,\n \nhigh\n=\n9\n,\n \nsize\n=\n(\n20\n,\n \n2\n))\n\n\n\n# fit the model\n\n\nclf\n \n=\n \nsvm\n.\nOneClassSVM\n(\nnu\n=\n0.1\n,\n \nkernel\n=\n\"rbf\"\n,\n \ngamma\n=\n1\n,\n \nrandom_state\n \n=\n \n0\n)\n\n\nclf\n.\nfit\n(\nX_train\n)\n\n\n\n# predict labels\n\n\ny_pred_train\n \n=\n \nclf\n.\npredict\n(\nX_train\n)\n\n\ny_pred_test\n \n=\n \nclf\n.\npredict\n(\nX_test\n)\n\n\ny_pred_outliers\n \n=\n \nclf\n.\npredict\n(\nX_weird\n)\n\n\n\n\n\n\nWhich when plotted in \nmatplotlib\n results in revealing some decision boundaries around if a value is weird or not from the iris dataset (and some made-up values for testing):\n\n\n![anomalies in scikit-learn image]({{ site.baseurl }}/img/novelty.png)\n\n\nYou can find the Notebooks offering at \nhttps://notebooks.azure.com\n and the Python course on the main page with the tutorials near the bottom and titled  \nFundamentals of Data Science with Python\n.  It's free, just like the product, Azure Notebooks.  The course is actually a \nLibrary\n.  \n\n\nIt was a really fun and challenging to create and I'm so very glad to have it released into the wild.  It contains exercises as well, some simple and some for those who enjoy a challenge themselves.  \n\n\nFor example, in this exercise a student is asked to create a Wallet for your friend Amy, let Amy earn some money and then let Amy spend some of that money.\n\n\nclass\n \nWallet\n:\n\n    \n'''The Wallet class holds cash for a owner.'''\n\n\n    \ndef\n \n__init__\n(\nself\n,\n \nowner\n,\n \ncash\n):\n\n        \nself\n.\nowner\n \n=\n \nowner\n\n        \nself\n.\ncash\n \n=\n \ncash\n\n\n    \n# Spend method\n\n    \ndef\n \nspend\n(\nself\n,\n \namount\n):\n\n        \n'''The spend method removes a given amount of money from the wallet.'''\n\n        \nself\n.\ncash\n \n-=\n \namount\n\n\n    \n# Earn method\n\n    \ndef\n \nearn\n(\nself\n,\n \namount\n):\n\n        \n'''The earn method adds a given amount of money to the wallet.'''\n\n        \nself\n.\ncash\n \n+=\n \namount\n\n\n\n\n\n\nI can't wait to add more modules.  Python will attack big data in the next one with \ndask\n.  Stay tuned.\n\n\nMy colleague and I also made use of the \"stickies\" teacher's aid.  We handed out blue and red sticky notes to all of the attendees and so when they had a question or got stuck, they could simply place a red sticky on the front of their laptop and we could see the \"flag\" and come to help.  This way the attendees didn't need to raise a hand at all, creating a situation where everyone had a chance to get questions answered or help when they needed without a tired arm or feeling as if they were disrupting the self-paced classroom.\n\n\nThe Jupyter notebooks were new to about 90% of the attendees so an \nin promptu\n tutorial was given at the beginning to familiarize the attendees with the learning framework.  An introduction to Python-flavored Jupyter notebooks is provided on my GitHub on  \nthis notebook\n.  If you are looking for an R-flavored one, I have one \nhere\n.  Please enjoy and know there are tons of resources on the net regarding Jupyter notebooks and learning Python or R (Jupyter actually is a portmanteau of Julia, Python and R).  I plan to write a short blog on using them and multi-tenant Jupyter notebook frameworks for teaching in another blog post.  Many posts have been promised so I better end this one.",
            "title": "Python for Data Science Goes Into the Wild"
        }
    ]
}