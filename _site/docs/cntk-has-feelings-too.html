<p><img src="/assets/img/cntk.jpg" alt="header pic" /></p>

<p><strong>Posted:</strong>  2017-12-12</p>

<h2 id="some-background">Some Background</h2>

<h3 id="cntk">CNTK</h3>

<p>The original name for Microsoft’s CNTK was the <em>Computational Network Toolkit</em>, now known today simply as the <em>Cognitive Toolkit</em>, still abbreviated CNTK for short.  It was orignally written and offered up as a C++ package and now has Python bindings, making it much more widely adoptable.</p>

<blockquote>
  <p>In its original words: [CNTK is] a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph</p>
</blockquote>

<p>It was first open-sourced in April of 2015 with intended use for researchers and protoypers using GPUs for accelerated matrix calculations, much of what deep learning is built upon these days.  Interestingly, TensorFlow has its initial public release in November of 2015.  Of note, 2015 was also a good year for Microsoft Research in the computer vision space as they won the <a href="https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge">ImageNet challenge</a> that December using this toolkit and a 152-layer deep neural network.</p>

<p>Since the beginning CNTK has been available for Linux and Windows.  We will be using a Linux Docker image in a minute.</p>

<h2 id="fer-faces-data">FER Faces Data</h2>

<p>This data comes via a Kaggle competition on facial expressions found <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">here</a>.</p>

<p>The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of two categories, Happy or Sad.  That being said this dataset has images (all but two of interest were dropped in this analysis) for a total of 6 emotions:  Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.</p>

<h2 id="how-i-set-things-up">How I set things up</h2>

<p>Since I’m on a Mac, I chose to use the Docker image of CNTK (instructions found <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Docker-Containers">here</a>).  This pulls down an image of a pre-created system and I run it in my own Docker container basically recreating an Ubuntu setup with CNTK locally.  It’s pretty neat!  And then I can run Jupyter notebooks on my system and pull in local files and write out data as needed.  Let me show you how and then after we’ll talk about the CNNs.</p>

<p>By following this <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Docker-Containers">Doc</a> I got a Jupyter notebook up and running with CNTK with all of the Tutorial notebooks at the ready and the ability to upload or create new ones as needed.</p>

<p>I ran these commands to get a Jupyter notebook set up with CNTK (v2.1 used here).</p>

<p>An important note:  in the <code class="highlighter-rouge">run</code> command for Docker I mounted a volume with <code class="highlighter-rouge">"$PWD/data:/data"</code>.  This “/data” folder can be accessed from the Jupyter notebook, as you can see if you check them out (link below), but also used to add data or pull data from the docker container just like any file folder on your system.  A very handy trick!</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull microsoft/cntk:2.1-cpu-python3.5

docker run -d --volume "$PWD/data:/data" -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk:2.1-cpu-python3.5

docker exec -it cntk-jupyter-notebooks bash -c "source /cntk/activate-cntk &amp;&amp; jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root"
</code></pre></div></div>

<h2 id="what-i-did-in-a-nutshell">What I Did In a Nutshell</h2>

<p>So, I tried many different CNN network architectures (simple three-layer CNN, ones with pooling and dropout layers, etc.) and several hyperparameter combinations (minibatch sizes for training, learning rate, etc.).  Here are just a few basic improvement tips for image classification based on what I found and a bit on how to do this with CNTK, plus my results - my hope is that you can do this on your own with some tips to start or maybe add to your workflow.</p>

<p><strong>The Jupyter notebooks associated with this post can be found on GitHub <a href="https://github.com/michhar/python-jupyter-notebooks/tree/master/cntk">here</a></strong></p>

<h2 id="positive-results">Positive Results</h2>

<h3 id="improvement-1--use-sufficient-data">Improvement #1:  Use sufficient data</h3>

<p>I began this journey with about 1000 images that I sorted and curated into a “sad” or “happy” bucket - using the <a href="http://pics.stir.ac.uk/2D_face_sets.htm">PICS 2D face sets</a>.  I even got up to 72% accuracy with this dataset and a CNN with pooling.  Then I discovered a Kaggle competition from about 5 years ago dealing with recognizing facial expressions - <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">Challenges in Representation Learning: Facial Expression Recognition Challenge</a> and this provided me with about 13500 training images and 2300 test images of the “happy” or “sad” kind (there were also images for categories: angry, disgust, fear, surprise and neutral in case you wish to try these out).</p>

<h3 id="improvement-2--intensity-normalization">Improvement #2:  Intensity normalization</h3>

<p>I read about the sensitivity of CNNs to certain normalizations.  So, I decided to try a recommended pixel intensity normalization (<a href="https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues">Ref to nice StackOverflow hint</a>).  This update resulted in about a 5% increase in accuracy on the held-out test set.</p>

<p>My code looked like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="s">"""
    Linear normalization
    http://en.wikipedia.org/wiki/Normalization_</span><span class="si">%28</span><span class="s">image_processing</span><span class="si">%29</span><span class="s">
    """</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>
    <span class="n">minval</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span>
    <span class="n">maxval</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">minval</span> <span class="o">!=</span> <span class="n">maxval</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">-=</span> <span class="n">minval</span>
        <span class="n">arr</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">255.0</span><span class="o">/</span><span class="p">(</span><span class="n">maxval</span><span class="o">-</span><span class="n">minval</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">arr</span>
</code></pre></div></div>

<h3 id="improvement-3--adjust-or-even-search-hyperparameter-space">Improvement #3:  Adjust or even search hyperparameter space</h3>

<p>Play with the hyperparameters.  I had a code cell full of my hyperparameters, e.g. learning rate and minibatch sizes.  I varied one or two at a time.  Just as an example, I took the learning rate down 10x (from 0.2 to 0.02) and my resulting accuracy increased approximately 5%.  Using the <a href="https://docs.microsoft.com/en-us/python/api/cntk.train.training_session?view=cntk-py-2.1">CNTK train package</a> one can design a “trainer” object encapsulating all training tasks and include parameter ranges inside it.  I also played with the strides and filter sizes or receptive fields in my CNN for the convolutional and pooling layers (see the Stanford CV course <a href="http://cs231n.github.io/convolutional-networks/#conv">notes</a> for great explanations of pretty much anything CNN and much more).  My big tip if you are starting out is simply to tune these adjustable parameters yourself and see what happens.</p>

<h3 id="improvement-4--add-more-layers-but-be-careful-of-overfitting">Improvement #4:  Add more layers (but be careful of overfitting)</h3>

<p>Interestingly, layering in three pooling layers in between the convolutional layers (last one before the dense output layer) resulted in about another 5% jump in accuracy on the test set.  (more on CNNs in the Stanford course <a href="http://cs231n.github.io/convolutional-networks">notes</a>)</p>

<p>Here’s what the relatively simple model looked like in the CNTK Python API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">default_options</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">(),</span> <span class="n">activation</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">relu</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">For</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Convolution2D</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> 
                                     <span class="n">num_filters</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> 
                                     <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                     <span class="n">strides</span><span class="o">=</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)][</span><span class="n">i</span><span class="p">]),</span>
                <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">AveragePooling</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="p">]),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_output_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></div>

<p>If you have seen some networks in CNTK, note I’m using the Sequential notation “shortcut” for repeating some layers, but maybe with different params (more in this <a href="https://cntk.ai/pythondocs/layerref.html#sequential">Doc</a>).</p>

<p>The above “simple” CNN resulted in an average test error of 28.53%.</p>

<p>I decided I’d like to try a more complex network and so began reading some papers on emotion recognition.  I came across one that appeared to be using the same size images (likely the same dataset).  Their proposed CNN architecture looked like this (<a href="https://arxiv.org/pdf/1706.01509.pdf">Ref</a>):</p>

<p><img src="/img/cntk_feels/proposed_cnn_Dachapally.png" alt="Propsed architecture from paper" /></p>

<p>I implemented the paper’s architecture in the CNTK Python API as follows (it had 5935330 parameters in 12 parameter tensors):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">default_options</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">(),</span> <span class="n">activation</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">relu</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">For</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Convolution2D</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span>
                    <span class="p">[(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)][</span><span class="n">i</span><span class="p">],</span> 
                                     <span class="n">num_filters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                                     <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                     <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
                    <span class="p">[</span><span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">AveragePooling</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                    <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                    <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">)][</span><span class="n">i</span><span class="p">]]),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_output_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></div>

<p>The above CNN architecture which matched the paper, resulted in a 25.61% average test error.  Let’s see if we can do even better.</p>

<p>A slightly modified, even more complex version of this architecture looks like this in the CNTK Python API (my update was to increase the number of filters for each conv layer):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">default_options</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">(),</span> <span class="n">activation</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">relu</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">For</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Convolution2D</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> 
                                     <span class="n">num_filters</span><span class="o">=</span><span class="p">[</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">64</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> 
                                     <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                     <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling</span><span class="p">(</span><span class="n">filter_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
                                    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">pad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="p">]),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">C</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_output_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></div>

<p>In the last architecture I had 37917906 parameters in 12 parameter tensors with an average test error of 22.35% (which, to be honest, could cause an issue of over-fitting due to the very large number of parameters - just something to consider).  A validation step would be highly recommended in this case given sufficient computational power!</p>

<h2 id="conclusion">Conclusion</h2>

<p>I picked a few random pictures from an online image search of happy and sad faces.  With my error rate of 22% it did pretty well, even on pictures of my own face (not shown).  These, for instance, were correctly identified as happy and sad.</p>

<p><img src="/img/cntk_feels/kids_happy_sad.png" alt="happy and sad kids" /></p>

<p>(Note, the images may not show correctly as their 48x48 square size in this browser).</p>

<p>The improvements are not an exhaustive list of everything one can try to gain better results of course, but do represent some common ways to deal with image data using CNNs.  If you have GPU-acceleration options, give some more complex network architectures a try - you could spin up an Azure Deep Learning VM which is what I usually do at my day job.  Happy deep learning!</p>

<p><strong>The Jupyter notebooks associated with this post can be found on GitHub <a href="https://github.com/michhar/python-jupyter-notebooks/tree/master/cntk">here</a></strong></p>

<h2 id="references">References</h2>

<ol>
  <li><a href="https://github.com/Microsoft/CNTK/tree/c977f3957d165ef2793ea9ee4d3f9165fc6c0b80">Historical (Jan. 2016) Readme on CNTK Repo</a></li>
  <li><a href="https://blogs.microsoft.com/ai/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge">Microsoft researchers win ImageNet computer vision challenge</a></li>
  <li><a href="https://arxiv.org/pdf/1706.01509.pdf">Facial Emotion Detection Using Convolutional Neural
Networks and Representational Autoencoder Units by Prudhvi Raj Dachapally</a></li>
</ol>

