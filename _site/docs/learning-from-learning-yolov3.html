<p><img src="http://kids.nationalgeographic.com/content/dam/kids/photos/animals/Bugs/H-P/monarch-butterfly-grass.adapt.945.1.jpg" alt="monarch butterfly" /></p>

<p><strong>tl:dr</strong>:  YOLO (for “you only look once”) v3 is a relatively recent (April 2018) architecture design for object detection.  PyTorch (recently merged with Caffe2 and production as of November 2018) is a very popular deep learning library with Python and C++ bindings for both training and inference that is known for having dynamic graphs.  This post is about what I learned expanding a PyTorch codebase that can train object detection models for any number of classes and on custom data (<em>We love you COCO, but we have our own interets, now.</em>).</p>

<p><strong>Posted:</strong>  2019-11-23</p>

<h2 id="quick-links">Quick Links</h2>

<ul>
  <li><a href="https://arxiv.org/pdf/1804.02767.pdf" target="_blank">Original YOLO v3 paper</a></li>
  <li><a href="https://github.com/ayooshkathuria/pytorch-yolo-v3" target="_blank">Original PyTorch codebase</a></li>
  <li><a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" target="_blank">Ayoosh Kathuria’s original blog post on implementing YOLO v3 in PyTorch</a></li>
  <li><a href="https://github.com/michhar/pytorch-yolo-v3-custom" target="_blank">My fork and rewrite for custom data and fine-tuning, etc.</a></li>
</ul>

<h2 id="lessons">Lessons</h2>

<h3 id="transfer-learning">Transfer learning</h3>

<p>In transfer learning we begin with a base model which gives us the weight values to start our training.  Objects from the training set of the base model, upon which the base model was trained, gets us closer to a new learned network for objects in the real world.  So, instead of starting with random weights to begin our training we begin from a “smarter” set of values.</p>

<ul>
  <li>One tidbit I learned was to skip making batch normalization layers trainable.</li>
</ul>

<p>How to allow layers in a PyTorch model to be trainable:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Freeze layers according to user specification
</span><span class="n">stop_layer</span> <span class="o">=</span> <span class="n">layers_length</span> <span class="o">-</span> <span class="n">args</span><span class="o">.</span><span class="n">unfreeze</span> <span class="c1"># Freeze up until this layer
</span><span class="n">cntr</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">cntr</span> <span class="o">&lt;</span> <span class="n">stop_layer</span><span class="p">:</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="s">'batch_norm'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Parameter has gradients tracked."</span><span class="p">)</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">cntr</span><span class="o">+=</span><span class="mi">1</span>
</code></pre></div></div>

<h3 id="finetuning">Finetuning</h3>

<ul>
  <li>How much of network to “open up” or set as trainable (the parameters that is)? - I would say open it more (likely all of the parameters in fine-tuning phase) if the object or objects are very different from any COCO classes (NB:  the <code class="highlighter-rouge">yolov3.weights</code> base model from darknet is trained on COCO dataset).  So, for instance, if the base model has never seen a catapiller before (not in COCO), you may want to let more layers be trainable.</li>
</ul>

<p>How to allow even more layers in the PyTorch model to be trainable (could set <code class="highlighter-rouge">stop_layer</code> to 0 to train whole network):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># "unfreeze" refers to the last number of layers to tune (allow gradients to be tracked - backprop)
</span><span class="n">stop_layer</span> <span class="o">=</span> <span class="n">layers_length</span> <span class="o">-</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">unfreeze</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># Freeze up to this layer (open up more than first phase)
</span>
<span class="s">"""...[same as above section]"""</span>
</code></pre></div></div>

<ul>
  <li>Another learning is that if the network is not converging, try opening up all of the layers during fine-tuning.</li>
</ul>

<h3 id="learning-rate-schedulers">Learning rate schedulers</h3>

<p>This is more of an implementation detail, but nonetheless, found it helpful to not make the mistake.</p>

<ul>
  <li>Place the learning rate scheduler at the level of the epoch update, <strong>not</strong> the inner loop over batches of data (where the optimizer is).</li>
</ul>

<h3 id="data-augmentation">Data augmentation</h3>

<p>Some of these I learned the hard way, others from the wonderful PyTorch forums and StackOverflow.</p>

<ul>
  <li>Be careful of conversions from a 0-255 to a 0-1 range as you don’t want to do that more than once in code.</li>
  <li>Keep this simple at first with only the resize and normalization.  Try with several types of augmentation next, increasing in complexity with each experiment.</li>
</ul>

<p>Start with just resize and standard pixel intensity normalize.  (NB:  the transforms operate on PIL images, then convert to <code class="highlighter-rouge">numpy</code> 3D array and finally to <code class="highlighter-rouge">torch.tensor()</code>)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>custom_transforms = Sequence([YoloResizeTransform(inp_dim), Normalize()])
</code></pre></div></div>
<p>then get more fancy with hue, saturation and brightness shifts, for example (look in <code class="highlighter-rouge">cfg</code> for the amounts if following along in <a href="https://github.com/michhar/pytorch-yolo-v3-custom" target="_blank">code</a>).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>custom_transforms = Sequence([RandomHSV(hue=hue, saturation=saturation, brightness=exposure), 
    YoloResizeTransform(inp_dim), Normalize()])
</code></pre></div></div>

<p>Where Normalize is a pixel intensity normalization (here, not to unit norm because we do that elsewhere):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Normalize</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">"""Pixel-intensity normalize the input numpy image"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channels</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">):</span>
        <span class="s">"""
        Args:
            img : numpy array
                Image to be scaled.
        Returns:
            img : numpy array
                normalize image.
        """</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>
        <span class="c1"># Do not touch the alpha channel
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channels</span><span class="p">):</span>
            <span class="n">minval</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span>
            <span class="n">maxval</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">minval</span> <span class="o">!=</span> <span class="n">maxval</span><span class="p">:</span>
                <span class="n">arr</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">minval</span>
                <span class="c1"># Don't divide by 255 because already doing elsewhere
</span>                <span class="n">arr</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="p">((</span><span class="n">maxval</span><span class="o">-</span><span class="n">minval</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">arr</span><span class="p">,</span> <span class="n">bboxes</span>
</code></pre></div></div>

<ul>
  <li>A great option for augmentation is to double or triple the size of a dataset with a library like <code class="highlighter-rouge">imgaug</code> which can handle bounding boxes and polygons now.</li>
</ul>

<h2 id="yolo-glossary">YOLO Glossary</h2>

<ul>
  <li>YOLOv3:  You Only Look Once v3.  Improvments over v1, v2 and YOLO9000 which include <a href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">Ref</a>:
    <ul>
      <li>Predicts more bounding boxes per image (hence a bit slower than previous YOLO architectures)</li>
      <li>Detections at 3 scales</li>
      <li>Addressed issue of detecting small objects</li>
      <li>New loss function (cross-entropy replaces squared error terms)</li>
      <li>Can perform multi-label classification (no more mutually exclusive labels)</li>
      <li>Performance on par with other architectures (a bit faster than SSD, even, in many cases)</li>
    </ul>
  </li>
  <li>Tiny-YOLOv3:  A reduced network architecture for smaller models designed for mobile, IoT and edge device scenarios</li>
  <li>Anchors:  There are 5 anchors per box.  The anchor boxes are designed for a specific dataset using K-means clustering, i.e., a custom dataset must use K-means clustering to generate anchor boxes.  It does not assume the aspect ratios or shapes of the boxes. <a href="https://medium.com/@vivek.yadav/part-1-generating-anchor-boxes-for-yolo-like-network-for-vehicle-detection-using-kitti-dataset-b2fe033e5807">Ref</a></li>
  <li>Loss:  using <code class="highlighter-rouge">nn.MSELoss</code> (for loss confidence) or mean squared error</li>
  <li>IOU:  intersection over union between predicted bounding boxes and ground truth boxes</li>
</ul>

<h2 id="references">References</h2>

<ol>
  <li><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607" target="_blank">37 Reasons why your Neural Network is not working</a></li>
  <li><a href="https://github.com/aleju/imgaug" target="_blank"><code class="highlighter-rouge">imgaug</code> augmentation Python library</a></li>
</ol>

