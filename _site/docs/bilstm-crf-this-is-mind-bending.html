<p><img src="img/bilstm/blstm_crf_details.png" alt="" /></p>

<p><strong>tl;dr</strong>:  The Bidirectional LSTM (Bi-LSTM) is trained on both past as well as future information from the given data. This will make more sense shortly.</p>

<p><strong>Posted:</strong>  2019-04-03</p>

<h2 id="outline">Outline</h2>
<ul>
  <li>Definitions
    <ul>
      <li>Bi-LSTM</li>
      <li>Named Entity Recognition Task</li>
      <li>CRF and potentials</li>
      <li>Viterbi</li>
    </ul>
  </li>
</ul>

<h2 id="definitions">Definitions</h2>

<h3 id="bi-lstm-bidirectional-long-short-term-memory">Bi-LSTM (Bidirectional-Long Short-Term Memory)</h3>

<p>As you may know an LSTM addresses the vanishing gradient problem of the generic RNN by adding cell state and more non-linear activation function layers to pass on or attenuate signals to varying degrees.  However, the main limitation of an LSTM is that it can <strong>only account for context from the past</strong>, that is, the hidden state, h_t, takes only past information as input.</p>

<h3 id="named-entity-recognition-task">Named Entity Recognition Task</h3>

<p>For the task of <strong>Named Entity Recognition (NER)</strong> it is helpful to have context from past as well as the future, or left and right contexts.  This can be addressed with a Bi-LSTM which is two LSTMs, one processing information in a forward fashion and another LSTM that processes the sequences in a reverse fashion giving the future context.  <em>That second LSTM is just reading the sentence in reverse.</em></p>

<p>The hidden states from both LSTMs are then concatenated into a final output layer or vector.</p>

<h3 id="conditional-random-field">Conditional Random Field</h3>

<p>We don’t have to stop at the output vector from the Bi-LSTM!  We’re not at our tag for the entity, yet.  We need to understand costs of moving from one tag to the next (or staying put on a tag, even).</p>

<p>In a CRF, we have the concept of a <em>transition matrix</em> which is the costs associated with transitioning from one tag to another - a transition matrix is calculated/trained for each time step.  It is used in the determination of the best path through all potential sequences.</p>

<p>Say <strong>B</strong> is the tag for the beginning of an entity, <strong>I</strong> signifies that we are inside an entity (will follow a <strong>B</strong>) and <strong>O</strong> means we are outside an entity.</p>

<p>Next, is an example of B-I-O scheme labeling for finding nouns in a sentence (by the way, there are a myriad of other schemes out there - see <a href="#references">Referenes</a> for some more).</p>

<table>
  <thead>
    <tr>
      <th>Word</th>
      <th>Scheme Tag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>She</td>
      <td>B</td>
    </tr>
    <tr>
      <td>was</td>
      <td>O</td>
    </tr>
    <tr>
      <td>born</td>
      <td>O</td>
    </tr>
    <tr>
      <td>in</td>
      <td>O</td>
    </tr>
    <tr>
      <td>North</td>
      <td>B</td>
    </tr>
    <tr>
      <td>Carolina</td>
      <td>I</td>
    </tr>
    <tr>
      <td>but</td>
      <td>O</td>
    </tr>
    <tr>
      <td>grew</td>
      <td>O</td>
    </tr>
    <tr>
      <td>up</td>
      <td>O</td>
    </tr>
    <tr>
      <td>in</td>
      <td>O</td>
    </tr>
    <tr>
      <td>Texas</td>
      <td>B</td>
    </tr>
  </tbody>
</table>

<p>Let’s look at the transition matrix for the costs of moving from one tag (using our B-I-O scheme) to the next (remember our Bi-LSTM is understanding both the forward and reverse ordering to get more accurate boundaries for the named entities).</p>

<p><img src="https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images//crf_transition_matrix.png" width="70%" /></p>

<p>The mathematical derivations for calculating this matrix and decoding it is beyond the scope of this post, however if you wish to learn more see <a href="http://www.cs.columbia.edu/~mcollins/crf.pdf">this</a> article.</p>

<h3 id="viterbi-algorithm">Viterbi Algorithm</h3>

<p>If each Bi-LSTM instance (time step) has an associated output feature map and CRF transition and emission values, then each of these time step outputs will need to be decoded into a path through potential tags and a final score determined.  This is the purpose of the Viterbi algorithm, here, which is commonly used in conjunction with CRFs.</p>

<p>Specifically, the Viterbi algorithm finds the optimal path through a sequence given a cost function by tracing backwards through a graph of all possible paths.  There are computational tricks to finding this path in the high dimensional space and you can find out more in the PyTorch tutorial code link <a href="#code">below</a> (<code class="highlighter-rouge">_forward_backwards_trick</code>).</p>

<p>Here, let’s see a simple example of just the Viterbi algorithm.  The simplicity of Viterbi is that at each time step, it “looks to the left” to find that best path and then moves to the right, repeating this “look to the left” until a “survivor path” or optimal path is found with the last column being the possible tags.  The score may also be found by tracing backwards along this path and using the metric decided upon.</p>

<p>In this example the optimal score (via a metric) is the lowest one, however, one could also look for the highest scoring path if another metric is used as is shown next.</p>

<p><img src="https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/viterbi.png" width="70%" /></p>

<p>Getting more realistic…</p>

<p>With regards to our NER work here, below is an example of a “survivor” path within the context of the linear-CRF where we are trying to find the highest scoring path through a sequence (giving us the tags and final score).  The transition matrix values are represented by the arrows and a sequence vector is shown as part of the overall cost function.</p>

<p><img src="https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/linear_crf_example.png" width="70%" /></p>

<h3 id="putting-it-all-together">Putting it All Together</h3>

<p>Here we have word embeddings as the data for the forward and reverse LSTMs.  The resulting forward vector (V_f) and backwards vector (V_b or Output layer, here) are concatenated into a final vector (V_o) that feeds into the CRF layer and is decoded via the Viterbi algorithm (part of CRF layer) into the optimal tag for that input.  Note, the initial values for the Hidden inputs for each LSTM (forward and reverse) are often a vector of random numbers.</p>

<p><br /><br /></p>

<p><img src="https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/blstm_crf_details.png" width="70%" /></p>

<div align="right"><a href="https://www.sciencedirect.com/science/article/pii/S1532046417300977" target="_blank">Reference</a></div>

<blockquote>
  <p>For a more in-depth discussion, see this excellent post describing the Bi-LSTM, CRF and usage of the Viterbi Algorithm (among other NER concepts and equations): <a href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html" target="_blank">Reference</a>.</p>
</blockquote>

<h3 id="code">Code</h3>

<p>See this PyTorch official <a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html" target="_blank">Tutorial Link</a> for the code and good explanations.</p>

<h3 id="references">References</h3>

<ol>
  <li><a href="https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66">Understanding Bidirectional RNN in PyTorch</a></li>
  <li><a href="https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463">Conditional Random Field Tutorial in PyTorch</a></li>
  <li><a href="https://www.sciencedirect.com/science/article/pii/S1532046417300977">Character-level neural network for biomedical named entity recognition</a></li>
  <li><a href="https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/">Other named entity tag schemes</a></li>
</ol>

<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    
    var disqus_config = function () {
        this.page.url = 'https://michhar.github.io/bilstm-crf-this-is-mind-bending/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'happycat1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = 'https://michhar.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
